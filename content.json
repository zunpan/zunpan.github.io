{"meta":{"title":"panzun blog","subtitle":"","description":"","author":"panzun","url":"https://zunpan.github.io","root":"/"},"pages":[{"title":"404 Not Found：该页无法显示","date":"2024-02-26T12:05:19.896Z","updated":"2023-09-24T04:27:40.270Z","comments":false,"path":"/404.html","permalink":"https://zunpan.github.io/404.html","excerpt":"","text":""},{"title":"关于","date":"2024-02-26T12:05:19.906Z","updated":"2023-09-24T04:27:40.286Z","comments":false,"path":"about/index.html","permalink":"https://zunpan.github.io/about/index.html","excerpt":"","text":"个人详细介绍"},{"title":"书单","date":"2024-02-26T12:05:19.906Z","updated":"2023-09-24T04:27:40.286Z","comments":false,"path":"books/index.html","permalink":"https://zunpan.github.io/books/index.html","excerpt":"","text":""},{"title":"分类","date":"2024-02-26T12:05:19.916Z","updated":"2023-09-24T04:27:40.286Z","comments":false,"path":"categories/index.html","permalink":"https://zunpan.github.io/categories/index.html","excerpt":"","text":""},{"title":"Excel比对与合并系统","date":"2023-03-03T07:00:03.000Z","updated":"2023-09-24T04:27:40.288Z","comments":true,"path":"hidden/Excel比对与合并系统_完整版.html","permalink":"https://zunpan.github.io/hidden/Excel%E6%AF%94%E5%AF%B9%E4%B8%8E%E5%90%88%E5%B9%B6%E7%B3%BB%E7%BB%9F_%E5%AE%8C%E6%95%B4%E7%89%88.html","excerpt":"","text":"背景 许多游戏策划使用Excel来配置数值。策划需要保存所有版本的数值表，有时需要查看两个数值表有何差异，有时想把差异或者叫修改应用到另一张数值表中，这非常类似于版本控制，但是市面上的版本控制系统svn和git都是针对文本文件，不能用于Excel的版本控制 Excel比对算法 一维数据比对算法 假设有两个序列A1...AmA_1...A_mA1​...Am​和B1...BnB_1...B_nB1​...Bn​，我们可以通过对A序列进行一些列操作，使之变为B序列。对每种操作都定义个Cost，如何找到总Cost最小的使A变为B的操作序列，可以通过动态规划解决。这是一个已经被广为研究的算法问题，本文就不再整述，读者可以在网上搜索Edit编辑距离获取更多信息。 操作集合的定义有多种方式，一种较为常见的操作集合定义如下(Cost均为1) : 在序列中插入一个元素: 在序列中删除一个元素; 比如，将字符串kiten变换为sitting，需要删除k，插入s，删除e，插入i，在尾部插入g。如果在原序列和目标序列中去掉删除和插入的元素，那么原序列和目标序列就是完全相同的了(比如上面的例子两者都变为itn了)，因此这种编辑距离被命名为LCS (Longest Common Subsequence) 编辑距离。LeetCode 1143. 最长公共子序列 再回到本文要讨论的差异比较问题，要比较两个序列的差异，实际上就是要找到二者之间尽量多的公共部分，剩下的就是差异部分，所以这和最短编辑距离问题是完全等价的。 此外，除了LCS编辑距离之外，还有一种常用的编辑距离，允许插入、删除和修改操作，叫做Levenshtein编组距离。另外，还可以定义一种广义的Levenshtein编辑距离，删除元素AiA_iAi​和插入元素BjB_jBj​;的Cost由一个单参数函数决定，记为Cost(AiA_iAi​)或Cost(BjB_jBj​); 将AiA_iAi​修改为BjB_jBj​;的操作的Cost由一个双参数函数决定，记为Cost2(Ai,BjA_i, B_jAi​,Bj​)。 /** * 比对的基本单位是单个字符 * @param text1 字符串1 * @param text2 字符串2 * @return levenshteinDP数组 */ static int[][] levenshteinDP(String text1, String text2) &#123; int len1 = text1.length(); int len2 = text2.length(); // dp[i][j]表示从text1[0...i-1]到text2[0...j-1]的最小编辑距离（cost） dp = new int[len1 + 1][len2 + 1]; // path记录此方格的来源是多个此类枚举值的布尔或值 path = new int[len1 + 1][len2 + 1]; for (int i = 0; i &lt; len1 + 1; i++) &#123; dp[i][0] = i; path[i][0] = FROM_INIT; &#125; for (int j = 0; j &lt; len2 + 1; j++) &#123; dp[0][j] = j; path[0][j] = FROM_INIT; &#125; for (int i = 1; i &lt; len1 + 1; i++) &#123; for (int j = 1; j &lt; len2 + 1; j++) &#123; path[i][j] = FROM_INIT; int left = dp[i][j - 1] + 1; int up = dp[i - 1][j] + 1; int leftUp; boolean replace; if (text1.charAt(i - 1) == text2.charAt(j - 1)) &#123; leftUp = dp[i - 1][j - 1]; replace = false; &#125; else &#123; leftUp = dp[i - 1][j - 1] + 1; replace = true; &#125; dp[i][j] = Math.min(Math.min(left, up), leftUp); if (dp[i][j] == left) &#123; path[i][j] |= FROM_LEFT; &#125; if (dp[i][j] == up) &#123; path[i][j] |= FROM_UP; &#125; // 对应：两字符完全一样或者可以修改成一样 if (dp[i][j] == leftUp) &#123; if (replace) &#123; path[i][j] |= FROM_LEFT_UP_REPLACE; &#125; else &#123; path[i][j] |= FROM_LEFT_UP_COPY; &#125; &#125; &#125; &#125; return dp; &#125; 同样的，对于广义Levenshtein编辑距离，如果去掉删除和插入的元素，那么两个序列中剩下的元素即为一一对应的关系，每组对应的两个元素，要么是完全相同的，要么前者是被后者修改掉的。从这部分论述中我们不难看出，比较算法的核心思路实际上就是找到元素与元素之间的一一对应关系 二维数据比对算法 Excel的一个分页可以认为是一个二维数据阵列，阵列中的每个元素是对应单元格内容的字符串值。根据前面的论述，比较两个二维阵列的核心就是找到他们公共的行/列，或者说原阵列和目标阵列的行/列对应关系。比如，对于下面两张表: 甲表的第1、2、3列对应乙表的1、2、4列，甲表的1、3行对应乙表的1、2行。那么这两张表的差异通过下列方式描述: 在第2列的位置插入新列 删除第2行 将(原表中) 第3行第3列的元素从9修改为0; 如何计算两张表对应的行/列，一个比较容易想到的方案是将其拆分为两个独立求解的问题，计算对应的行和计算对应的列。对于前者，我们可以把阵列的每一行当成一个元素，所有行组成一个序列，然后对这个序列进行比较:后者亦然。这样我们就把二维比较问题转化成了一维比较的问题。关于编辑距离的定义，可以采用广义Levenshtein编辑距离，定义删除、插入元素的Cost为该行(列)的元素数，定义修改元素的Cost为这两行(列)之间的LCS编辑距离.于是两个二维阵列的比较过程如下: 找到二者对应的 (或者叫公共的) 行/列，非公共的行/列记为删除、插入行/列操作；两张表只保留公共的行/列，此时他们尺寸完全相同，对应位置的单元格逐一比较，如果值不相同，则记为单元格修改操作; 算法优化 上一个部分介绍的二维阵列比较方案只是一个理论上可行的方案，在实际应用中，存在以下问题: 删除、插入行/列的操作都是对于整个行/列的，而计算两行/列之间的LCS编辑距离是独立计算的，因此算法本身有一定不合理性; 计算修改Cost里又包含了LCS编辑距离的计算，二层嵌套，性能开销比较大; 针对上述问题，从游戏开发的实际应用场景出发，做了如下优化: 首先计算列之间的对应关系，只取表头前h行(不同项目表头行数h可能不同，可以通过参数配置) ，这样就把对整列的LCS编辑距离计算优化为h个单元格逐已比较，大幅优化了效率，而且对准确度基本不会有什么影响; 根据上一步的计算结果，去掉非公共的列(即删除、添加的列)，这样，剩下的列都是两边一一对应的了，此时再计算行的对应关系，修改操作的Cost定义就可以从LCS编辑距离改为单元格的逐一比较了，这样又大幅优化了性能 在上面所述基础之上，还可以再做优化，因为在实际应用中，绝大多数情况下，绝大多数行都不会有修改，因此可以先用LCS编辑距离对所有行的对应关系进行计算，即只有当两行内容完全相同时才会被当做是对应的; 然后再把剩下的未匹配的行分组用广义Levenshtein编辑距离进行对应关系匹配。这样么做的原因是LCS编辑距离比广义Levenshtein编辑距离的求解速度要快很多。 功能扩展 在开发过程中，我们经常会将单行或者连续若干行单元格上移或下移到另一位置，按照目前的比较逻辑，该操作会被认为是删除这些行，然后在新的位置重新插入这些行。这样的结果和不合理的。为此，我们可以引入一种新的操作: 移动行到另一位置。加入了这个新的操作之后，我们依然可以建立之前所述的行对应关系，只不过两边行的顺序可以是乱序的。这种不保序对应关系可以通过多个轮次的编辑距离匹配计算，每次匹配之后去掉匹配上的行，剩下未匹配的行组成一个新的序列进行下一轮的匹配。每轮匹配是采用LCS编辑距离还是广义Levenshtein编指距离可以灵活决定，比如前若干轮或者行数较多时采用LCS编辑距离，后面的轮次再用广义Levenshtein编辑距离。 公式处理 在前面的论述中，为了简化模型，我们把单元格值当成了一个纯字符串处理。实际上单元格值由两部分组成:公式和文本。若公式不为空，那么文本就是公式的计算结果;若公式为空，文本则为填写的文字内容。因此对Excel的完整的比较过程应该也包含对公式的比较。比较公式的时候会有一个问题，因为公式里引用其他单元格用的是列坐标的形式，当有行/列增减的时候，会导致这些坐标发生变化，产生额外的比较差异。该问题可以通过如下的方式解决: 先不考虑公式，用文本内容计算出行/列的对应关系; 利用上一步计算出的对应关系，将甲表中所有公式的引用单元格进行坐标变换，变换到对应乙表中的坐标，再用变换后的公式去和乙表中的公式进行逐一比较 例如甲表中有一个公式为A1+B1，乙表在最上面插入了一行，因此该格子在乙表中的公式变成了A2+B2。借助文本内容进行匹配，得出甲表的行1对应着乙表的行2，因此甲表公式中的行1都改成行2，即A1+B1 -&gt; A2+B2，这样就和乙表中的实际公式是一致的了，没有产生不必要的差异 关于如何对Excel公式进行引用坐标变换，一个很容易想到的方案就是借助第三方的Excel公式语法分析库将公式解析成语法树，再进行变换。但是这样做效率是非常低的，对于项目中动辄上百列上万行的大规模Excel表格是不具有可行性的。 另一个方案是通过正则表达式匹配出所有的单元格引用。Excel的单元格用格式定义如下: 行坐标: \\S?\\d+ 列坐标: \\S?[A-Z]+ 单元格引用: 行坐标列坐标(行坐标:行坐标](列坐标:列坐标) 其中\\$表示该用是绝对位置引用还是相对位置引用，二者只有在编辑Excel过程中进行单元格复制时才有区别对本文中讨论的引用坐标变换过程无影响。 这样我们就可以通过正则表达式匹配找出所有的单元格引用，并对其进行变换。在这个过程中需要注意的是要排除掉一些干扰因素: 跨分页、跨文件引用时，分页名和文件名，可能包含单元格引用格式; 字符串常量中可能包含单元格引用格式 函数名可能包含单元格引用格式 (比如ATAN2函数，就符合单元格引用的正则表达式) 排除的办法是先对上述这些项进行匹配，然后再对单元格引用格式进行匹配。 通过正则表达式法进行公示变换，实测效率为~100个每毫秒。对于规模较大的表格仍然可能成为效率瓶颈。对此可以进行进一步优化。考虑到项目中通常会有大批形式重复的公式，只是引用的单元格不同，例如下图: 因此我们可以先对原始公式做一次变换，将连续的大写字母换成单个字母A，将连续的数字替换成单个数字0这样上图中的公式全部都变成了: 我们称之为“公式模板”，之后再对这个模板进行正则表达式匹配，匹配到的单元格引用，再通过模板与原公式之间的字符位置对应关系反推出原本位置。因为大量公式都对应同一个模板，所以可以通过哈希表把计算过的模板都须存起来，大幅优化运行效率。经过优化之后，实测效率为~1000个每毫秒，比优化前提高了10倍. Excel合并算法 合并(Merge) 是版本管理系统(Version Control System，例SVN、Git等) 中的一个基本概念。设文件初始状态记为Base，“我”对Base进行了修改，记为Mine，“其他人”同时也对Base进行了修改，记为Theirs,将“我”和“其他人”的修改合到一起，文件变为Merged状态，这一过程就叫做Merge。简单来说，Merge过程就是输入Base、Mine、Theirs文件，输出Merged文件的过程，可以用于本地冲突合并、分支合并、单次提交回退等场景。Merge的通常过程是，对Base和Mine进行比较，找到所有差异，再将所有差异应用到Theirs上，即可得到Merged文件。 对于Excel文件的Merge过程，也可以运用上述思路。首先比较Base和Mine，找到增减的行/列以及单元格值的修改，再将其应用到Theirs表上。在应用增删行/列这类改动时，可以调用相应API来实现，而不是将最终结果数据直接更新到目标分页中，后者会导致表格中格式的错乱。 此外还需要注意的问题是是，我们在描述这些改动的时候，涉及到的位置相关信息，用的是在哪张表中的行数或者列数。比如在Base表中的第10行后面插入了一个新的行，那么就需要计算出这一行在Theirs表中的位置，才能将这个改动应用到Theirs表，因此需要对Base和Theirs也做一次比较，目的是建立起Base和Theirs之间的行/列对应关系 在将Mine的改动应用到Theirs的过程中，会遇到一些特殊情况，需要人手动决定以何种方式应用改动项，或者改动项无法应用。例如某个格子在Base中的值时A，在Mine中被改成了B，在Theirs中被改成了C，那么在Merged中应该取值B还是C，就不是算法可以解决的了。这种情况就叫做“冲突” (Conflict)。总结下来一共有以下几种冲突情形: 双方都修改了同一个单元格的值，且修改结果不同，该单元格取值无法确定 一方新增了行，另一方新增了列，合并后新增行列交叉点处的单元格取值无法确定 一方修改了单元格值，另一方删除了这个单元格所在的行或者列; 一方新增了行(列) ，另一方或删除了列(行) ，因此新增行(列) 中有格子被另一方删除了: 其中1和2属于取值无法确定的情况，需要手动指定取值;3和4属于改动无法应用的情况，需要给出警告提示。在实际应用场景中，4的危害是比较小的，不提示也是可以的。"},{"title":"友情链接","date":"2024-02-26T12:05:19.936Z","updated":"2023-09-24T04:27:40.288Z","comments":true,"path":"links/index.html","permalink":"https://zunpan.github.io/links/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2024-02-26T12:05:19.936Z","updated":"2023-09-24T04:27:40.288Z","comments":false,"path":"repository/index.html","permalink":"https://zunpan.github.io/repository/index.html","excerpt":"","text":""},{"title":"标签","date":"2024-02-26T12:05:19.946Z","updated":"2023-09-24T04:27:40.289Z","comments":false,"path":"tags/index.html","permalink":"https://zunpan.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"查表法","slug":"查表法","date":"2025-06-15T02:37:14.000Z","updated":"2025-06-15T07:57:14.476Z","comments":true,"path":"2025/06/15/查表法/","link":"","permalink":"https://zunpan.github.io/2025/06/15/%E6%9F%A5%E8%A1%A8%E6%B3%95/","excerpt":"","text":"查表法 查表法两大优势： 空间换时间。预计算好输入对应的输出并存入映射表，然后运行时查表避免运行时计算，竞赛选手也把这叫打表法 简化条件分支。将每个输入对应的分支存入映射表，然后运行时查表选择执行的分支，开发者经常使用，可以降低圈复杂度 查表法经常和其它设计模式一起使用，包括策略模式、状态模式、工厂模式 查表法+策略模式 本质上是通过查表返回策略（方法） // HTTP状态码处理 const statusHandlers: Record&lt;number, (response: Response) =&gt; void&gt; = &#123; 200: (res) =&gt; processData(res.json()), 404: () =&gt; showNotFound(), 500: (res) =&gt; logError(res.text()), // ... &#125;; fetch(&#x27;/api/data&#x27;) .then(response =&gt; &#123; const handler = statusHandlers[response.status] || handleUnknownStatus; handler(response); &#125;); 查表法+状态模式 本质上是通过查表返回状态（枚举值） // 游戏物品属性表 const itemStatsTable: Record&lt;ItemType, ItemStats&gt; = &#123; [ItemType.SWORD]: &#123; damage: 15, speed: 1.2 &#125;, [ItemType.AXE]: &#123; damage: 25, speed: 0.8 &#125;, [ItemType.BOW]: &#123; damage: 10, speed: 2.0 &#125;, &#125;; class Player &#123; equipItem(type: ItemType) &#123; this.weaponStats = itemStatsTable[type]; &#125; calculateDamage(): number &#123; return this.weaponStats.damage * this.strength; &#125; &#125; 查表法+工厂模式 本质上是通过查表返回对象 // 文件格式处理器查表 const fileHandlers: Record&lt;string, FileHandler&gt; = &#123; &#x27;.jpg&#x27;: new ImageHandler(), &#x27;.png&#x27;: new ImageHandler(), &#x27;.mp4&#x27;: new VideoHandler(), &#x27;.pdf&#x27;: new DocumentHandler(), &#x27;.docx&#x27;: new DocumentHandler(), &#125;; function processFile(filename: string): void &#123; const ext = filename.slice(filename.lastIndexOf(&#x27;.&#x27;)); const handler = fileHandlers[ext.toLowerCase()]; if (handler) &#123; handler.process(filename); &#125; else &#123; throw new Error(`Unsupported file type: $&#123;ext&#125;`); &#125; &#125;","categories":[{"name":"开发经验","slug":"开发经验","permalink":"https://zunpan.github.io/categories/%E5%BC%80%E5%8F%91%E7%BB%8F%E9%AA%8C/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://zunpan.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"重构","slug":"重构","permalink":"https://zunpan.github.io/tags/%E9%87%8D%E6%9E%84/"}]},{"title":"ArkTS建造者模式","slug":"ArkTS建造者模式","date":"2025-06-14T02:37:14.000Z","updated":"2025-06-14T15:15:47.889Z","comments":true,"path":"2025/06/14/ArkTS建造者模式/","link":"","permalink":"https://zunpan.github.io/2025/06/14/ArkTS%E5%BB%BA%E9%80%A0%E8%80%85%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"ArkTS建造者模式 建造者模式的优点在于伸缩性强+线程安全，在复杂bean的初始化中经常使用，下面是一个Effective Java书中的例子 // Builder Pattern public class NutritionFacts &#123; private final int servingSize; private final int servings; private final int calories; private final int fat; private final int sodium; private final int carbohydrate; public static class Builder &#123; // Required parameters private final int servingSize; private final int servings; // Optional parameters - initialized to default values private int calories = 0; private int fat = 0; private int sodium = 0; private int carbohydrate = 0; public Builder(int servingSize, int servings) &#123; this.servingSize = servingSize; this.servings = servings; &#125; public Builder calories(int val) &#123; calories = val; return this; &#125; public Builder fat(int val) &#123; fat = val; return this; &#125; public Builder sodium(int val) &#123; sodium = val; return this; &#125; public Builder carbohydrate(int val) &#123; carbohydrate = val; return this; &#125; public NutritionFacts build() &#123; return new NutritionFacts(this); &#125; &#125; private NutritionFacts(Builder builder) &#123; servingSize = builder.servingSize; servings = builder.servings; calories = builder.calories; fat = builder.fat; sodium = builder.sodium; carbohydrate = builder.carbohydrate; &#125; &#125; 实现的关键是Bean只能由Builder初始化，所以Bean的构造函数私有化，保证外界不能new，但Builder作为静态内部类任然可以访问 ArkTS没有静态内部类，想实现上面的效果可以在Bean的构造函数中增加一个只能由Builder创建的token，下面是对应的例子 // 1. 定义私有令牌类（仅在模块内可见） class ConstructionToken &#123; // 私有构造函数确保只有本模块能创建实例 private constructor() &#123; &#125; // 工厂方法只允许 Builder 访问 static create(): ConstructionToken &#123; return new ConstructionToken(); &#125; &#125; // 2. NutritionFacts产品类 export class NutritionFacts &#123; private _servingSize: number; public get servingSize(): number &#123; return this._servingSize; &#125; private _servings: number; public get servings(): number &#123; return this._servings; &#125; private _calories?: number; public get calories(): number | undefined &#123; return this._calories; &#125; private _fat?: number; public get fat(): number | undefined &#123; return this._fat; &#125; private _sodium?: number; public get sodium(): number | undefined &#123; return this._sodium; &#125; private _carbohydrate?: number; public get carbohydrate(): number | undefined &#123; return this._carbohydrate; &#125; /** * 公开构造函数，但必须提供令牌 * @param token * @param servingSize * @param servings * @param calories * @param fat * @param sodium * @param carbohydrate */ constructor( token: ConstructionToken, // 必须提供令牌 servingSize: number, servings: number, calories?: number, fat?: number, sodium?: number, carbohydrate?: number, ) &#123; // 验证令牌有效性 if (!(token instanceof ConstructionToken)) &#123; throw new Error(&quot;Invalid construction token&quot;); &#125; this._servingSize = servingSize this._servings = servings this._calories = calories this._fat = fat this._sodium = sodium this._carbohydrate = carbohydrate &#125; &#125; // 3. NutritionFacts建造者 export class NutritionFactsBuilder &#123; private _servingSize: number; private _servings: number; private _calories?: number; private _fat?: number; private _sodium?: number; private _carbohydrate?: number; setServingSize(servingSize: number): NutritionFactsBuilder &#123; this._servingSize = servingSize; return this; &#125; setServings(servings: number): NutritionFactsBuilder &#123; this._servings = servings; return this; &#125; setCalories(calories: number): NutritionFactsBuilder &#123; this._calories = calories; return this; &#125; setFat(fat: number): NutritionFactsBuilder &#123; this._fat = fat; return this; &#125; setSodium(sodium: number): NutritionFactsBuilder &#123; this._sodium = sodium; return this; &#125; setCarbohydrate(carbohydrate: number): NutritionFactsBuilder &#123; this._carbohydrate = carbohydrate; return this; &#125; build(): NutritionFacts &#123; if (!this._servingSize || !this._servings) &#123; throw new Error(&quot;servingSize和servings是必须参数&quot;); &#125; // 获取有效构建令牌 const token = ConstructionToken.create(); return new NutritionFacts( token, this._servingSize, this._servings, this._calories, this._fat, this._sodium, this._carbohydrate ); &#125; &#125;","categories":[{"name":"ArkTS","slug":"ArkTS","permalink":"https://zunpan.github.io/categories/ArkTS/"}],"tags":[{"name":"ArkTS","slug":"ArkTS","permalink":"https://zunpan.github.io/tags/ArkTS/"},{"name":"设计模式","slug":"设计模式","permalink":"https://zunpan.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"大话设计模式学习笔记","slug":"大话设计模式学习笔记","date":"2024-08-12T02:37:14.000Z","updated":"2024-08-12T02:39:24.899Z","comments":true,"path":"2024/08/12/大话设计模式学习笔记/","link":"","permalink":"https://zunpan.github.io/2024/08/12/%E5%A4%A7%E8%AF%9D%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","excerpt":"","text":"设计模式 简单工厂模式 用单独的一个类完成创建所有实例，这就是工厂。 public class OperationFactory&#123; public static Operation createOperate(String op)&#123; Operation operation = null; switch(op)&#123; case &quot;+&quot;:&#123; operation = new OperationAdd(); &#125; case &quot;-&quot;:&#123; operation = new OperationSub(); &#125; // ... &#125; &#125; &#125; 优点：工厂对象类之间解耦 缺点：工厂类和工厂对象类之间存在耦合，新增工厂对象类需要修改工厂类 策略模式 策略模式定义了算法家族，分别封装起来，让它们之间可以互相替换，此模式让算法的变化，不会影响到使用算法的客户 单独使用策略模式，实例化对象的过程会落在客户端（switch-case-new过程）。因此可以将策略模式和简单工厂模式结合，将实例化对象的过程放在Context类中 策略模式和简单工厂模式结合使用的好处是，客户端只需要知道Context类 该模式不足的地方在于新增策略仍然要修改Context类中的switch代码块 单一职责原则 单一职责原则：就一个类而言，应该仅有一个引起它变化的原因 例如，游戏逻辑和界面应该分离 开闭原则 开闭原则：软件实体（类、模块、函数等等）应该可以扩展，但是不可以修改 开发人员应该对程序中频繁变化的那些部分做抽象 依赖倒转原则 依赖倒转原则： 高层模块不应该依赖低层模块。两个都应该依赖抽象 抽象不应该依赖细节，细节应该依赖抽象。换句话说，要针对接口编程，不要对实现编程 针对原则一，子类型必须能够替换掉它们的父类型而不影响软件工程，这样才能真正复用父类，因此高层和低层模块都应该依赖抽象。 针对原则二，例如，电脑主板、CPU、内存、硬盘都是针对接口设计的，如果针对实现来设计，内存就要对应具体的某个品牌的主板，那就会出现换内存要把主板一起换了的尴尬 装饰模式 装饰模式：动态地给一个对象添加一些额外的职责，就增加功能来说，装饰模式比生成子类更加灵活 代理模式 代理模式：为其它对象提供一种代理以控制对这个对象的访问 使用场景： 远程代理。为一个对象在不同的地址空间提供局部代表，这样可以隐藏一个对象存在于不同地址空间的事实。例子：客户端调用代理实现远程访问 虚拟代理。使用代理来存放创建开销很大的对象。例子：HTML中的未下载完成的图片框就是虚拟代理，它替代了真实图片 安全代理。用来控制真实对象的访问权限 智能指引。指调用真实对象时，代理处理另外一些事 工厂方法模式 简单工厂模式违背了开闭原则，于是有了工厂方法模式 工厂方法模式：定义一个用于创建对象的接口，让子类决定实例化哪一个类。工厂方法使一个类的实例化延迟到其子类 原型模式 原型模式：用原型实例指定创建对象的种类，并且通过拷贝这些原型创建新的对象 模板方法模式 魔棒方法模式：定义一个操作中的算法的骨架，而将一些步骤延迟到子类中。模板方法使得子类可以不改变一个算法的结构即可重定义该算法的某些特定步骤。 迪米特法则 迪米特法则（最少知识原则）：如果两个类不必彼此直接通信，那么这两个类就不应当发生直接的相互作用。如果其中一个类需要调用另一个类的某一个方法的话，可以通过第三者转发这个调用。 迪米特法则的根本思想是类之间的松耦合。 外观模式 外观模式：为子系统中的一组接口提供一个一致的界面，此模式定义了一个高层接口，这个接口使得这一子系统更加容易使用。 使用场景： 在设计阶段，分层架构之间增加Facade，例如表示层和业务逻辑层之间、业务逻辑层和数据访问层之间 在开发阶段，随着类越来越多，可以增加Facade降低类调用复杂性 在系统重构阶段，可以在系统中增加Facade，Facade调用旧系统代码保证平滑过渡 建造者模式 建造者模式：将一个复杂对象的构建与它的表示分离，使得同样的构建过程可以创建不同的表示。 观察者模式 观察者模式（发布-订阅模式）：定义了一种一对多的依赖关系，让多个观察者对象同时监听某一个主题对象。这个主题对象在状态发生变化时，会通知所有观察者对象，使它们能够自动更新自己 使用场景：当一个对象的改变需要同时改变其它对象，且不知道有多少对象需要改变时使用观察者模式 抽象工厂模式 抽象工厂模式：提供一个创建一系列相关或相互依赖对象的接口，而无需指定它们具体的类 抽象工厂的优点是只需要更换工厂实例对象，就可以更换一系列产品。缺点是增加产品时需要需改大量类 反射+配置文件+简单工厂可以避免大量修改 状态模式 状态模式：当一个对象的内在状态改变时允许改变其行为，这个对象看起来像是改变了其类 使用场景：当一个对象的行为取决于它的状态，并且它必须在运行时刻根据状态改变它的行为时，可以解决if-else过长的问题 适配器模式 适配器模式：将一个类的接口转换成客户希望的另外一个接口，使得原本由于接口不兼容而不能一起工作的那些类可以一起工作 备忘录模式 备忘录模式：在不破坏封装性的前提下，捕获一个对象的内部状态，并在该对象之外保存这个状态。这样以后就可以将该对象恢复到原先保存的状态 使用场景：适用于功能复杂、需要维护或记录属性历史的类，或者需要保存的属性只是众多属性中的一小部分 组合模式 组合模式：将对象组合成树形结构以表示‘部分-整体’的层次结构。组合模式使得用户对单个对象和组合对象的使用具有一致性 使用场景：需求中体现部分与整体层次的结构时，以及希望用户可以忽略组合对象与单个对象的不同，统一地使用组合结构中的所有对象时 迭代器模式 迭代器模式：提供一种方法顺序访问一个聚合对象中各个元素，而又不暴露该对象的内部表示（foreach语法糖） 单例模式 单例模式：保证一个类仅有一个实例，并提供一个访问它的全局访问点 桥接模式 桥接模式：将抽象部分与它的实现部分分离，使它们都可以独立的变化 使用场景：实现系统可能有多角度分类，每一种分类都有可能变化，那么就把这种多角度分离出来让它们独立变化，减少它们之间的耦合 命令模式 命令模式：将一个请求封装为一个对象，从而使你可用不同的请求对客户进行参数化；对请求排队或记录请求日志，以及支持可撤销的操作。 职责链模式 职责链模式：使多个对象都有机会处理请求，从而避免请求的发送者和接收者之间的耦合关系。将这个对象连成一条链，并沿着这条链传递该请求，直到有一个对象处理它为止 中介者模式 中介者模式：用一个中介对象来封装一系列的对象交互。中介者使各对象不需要显式地相互引用，从而使其耦合松散，而且可以独立地改变它们之间地交互。 使用场景：一组对象以定义良好但是复杂地方式进行通信的场合；想定制一个分布在多个类中的行为，而又不想生成太多子类的场合 享元模式 享元模式：运用共享技术有效地支持大量细粒度的对象 使用场景：如果一个应用程序使用了大量的对象，而大量的这些对象造成了很大的存储开销时就应该考虑使用 解释器模式 解释器模式：给定一个语言，定义它的文法的一种表示，并定义一个解释器，这个解释器使用该表示来解释语言中的句子 使用场景：当有一个语言需要解释执行，并且可以将该语言中的句子表示为一个抽象语法树时，可使用解释器模式（例如正则表达式，浏览器） 优势：容易地改变和扩展文法 劣势：解释器模式为文法中的每一条规则至少定义了一个类，因此包含许多规则的文法可能难以管理和维护。建议当文法非常复杂时，使用其它技术如语法分析程序或编译器生成器来处理 访问者模式 访问者模式：表示一个作用于某对象结构中的各元素的操作。它使你可以在不改变各元素的类的前提下定义作用于这些元素的新操作 使用场景：适用于数据结构稳定，算法易于变化 模式总结 创建型模式：单例、工厂方法、抽象工厂、建造者、原型 结构型模式：适配器、装饰、桥接、组合、享元、代理、外观 行为型模式：观察者、模板方法、命令、状态、职责链、解释器、终结者、访问者、策略、备忘录、迭代器","categories":[{"name":"Java","slug":"Java","permalink":"https://zunpan.github.io/categories/Java/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://zunpan.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"深入理解Java虚拟机学习笔记","slug":"深入理解Java虚拟机学习笔记","date":"2023-06-10T15:30:14.000Z","updated":"2023-09-24T04:27:40.285Z","comments":true,"path":"2023/06/10/深入理解Java虚拟机学习笔记/","link":"","permalink":"https://zunpan.github.io/2023/06/10/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Java%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","excerpt":"","text":"第一部分 走近Java 1. 走近Java 1.1 概述 Java不仅仅是一门编程语言，还是一个由一系列计算机软件和规范组成的技术体系 Java优点：1. 一次编写，到处运行；2. 避免了绝大部分内存泄露和指针越界问题；3. 实现了热点代码检测和运行时编译及优化，越运行性能越好；4. 完善的类库 1.2 Java技术体系 广义上，Kotlin等运行在JVM上的编程语言都属于Java技术体系 传统上，JCP定义的Java技术体系包含：1. Java程序设计语言；2. 各种硬件平台上的Java虚拟机实现；3. Class文件格式；4 Java类库API；5. 来自商业机构和开源社区的第三方Java类库 JDK：Java程序设计语言+Java虚拟机+Java类库 JRE：Java类库API中的Java SE API子集和Java虚拟机 1.3 Java发展史 1.4 Java虚拟机家族 Sun Classic/Exact VM。Sun Classic是世界上第一款商用Java虚拟机，纯解释执行代码，如果外挂即时编译器会完全接管执行，两者不能混合工作。Exact VM解决了许多问题但是碰上了引进的HotSpot，生命周期很短 HotSpot VM：使用最广泛的Java虚拟机。HotSpot继承了前者的优点，也有许多新特性，例如热点代码探测技术。JDK 8中的HotSpot融合了BEA JRockit优秀特性 Mobile/Embedded VM：针对移动和嵌入式市场的虚拟机 BEA JRockit/IBM J9 VM：JRockit专注服务端应用，不关注程序启动速度，全靠编译器编译后执行。J9定位类似HotSpot BEA Liquid VM/Azul VM：和专用硬件绑定，更高性能的虚拟机 Apache Harmony/Google Android Dalvik VM：Harmony被吸收进IBM的JDK 7以及Android SDK，Dalvik被ART虚拟机取代 Microsoft JVM：Windows系统下性能最好的Java虚拟机，因侵权被抹去 1.5 展望Java技术的未来 无语言倾向：Graal VM可以作为“任何语言”的运行平台 新一代即时编译器：Graal编译器，取代C2（HotSpot中编译耗时长但代码优化质量高的即时编译器） 向Native迈进：Substrate VM提前编译代码，显著降低内存和启动时间 灵活的胖子：经过一系列重构与开放，提高开放性和扩展性 语言语法持续增强：增加语法糖和语言功能 第二部分 自动内存管理 2. Java内存区域与内存溢出异常 2.1 概述 C、C++程序员需要自己管理内存，Java程序员在虚拟机自动内存管理机制下不需要为每一个new操作写对应的delete/free代码，但是一旦出现内存泄露和溢出，不了解虚拟机就很难排错 2.2 运行时数据区域 程序计数器：当前线程执行的下一条指令的地址；线程私有；不会OOM 虚拟机栈：Java方法执行的线程内存模型，每个方法执行时，JVM都会创建一个栈帧用于存储局部变量表、操作数栈、动态连接、方法出口等信息；线程私有；会栈溢出和OOM 局部变量表：存放的变量的类型有8种基本数据类型、对象引用和returnAddress类型（指向字节码指令的地址），这些变量除了64位的long和double占两个变量槽，其它占1个。局部变量表的大小在编译器确定 本地方法栈：和虚拟机栈作用类似，区别在于只是为本地（Native）方法服务，HotSpot将两者合二为一 堆：“几乎”所有对象实例都在此分配内存；可分代，也不分代；逻辑上连续，物理上可以不连续；会OOM 方法区：也叫“非堆”，存储已加载的类型信息、常量、静态变量、即时编译器编译后的代码缓存等数据；实现上，之前HotSpot使用永久代实现方法区，目的是方便垃圾回收，但是这样有OOM问题，JDK8废弃永久代改为使用本地内存的元空间，主要存放类型信息，其它移到堆；内存回收目标主要是常量池和类型信息 运行时常量池：Class文件中的常量池表存放编译期生成的各种字面量和符号引用，这部分内容在类加载后放到方法区的运行时常量池；具有动态性，运行时产生的常量也可以放入运行时常量池，String类的intern()利用了这个特性；会OOM 直接内存：不是运行时数据区的一部分，也不是Java虚拟机规范里定义的内存区域；NIO使用Native函数库直接分配堆外内存；会OOM 2.3 HotSpot虚拟机对象探秘 2.3.1 对象的创建 当JVM碰到new指令，首先检查指令参数能否在常量池中定位到一个类的符号引用，并且检查该类是否已被加载、解析和初始化，如果没有就进行类加载过程 JVM为新生对象分配内存。 对象所需内存大小在类加载后可确定，分配方法有两种：当垃圾收集器（Serial、ParNew）能空间压缩整理时，堆是规整的，分配内存就是把指针向空闲空间方向移动对象大小的距离，这种叫“指针碰撞”，使用CMS收集器的堆是不规整的，需要维护空闲列表来分配内存。 内存分配可能线程不安全，例如线程在给A分配内存，指针来没来得及修改，另一线程创建对象B又同时使用了原来的指针来分配内存。解决方法有两个：1.对分配内存空间的动作进行同步处理，JVM采用CAS+失败重试的方式保证原子性；2.预先给每个线程分配一小块内存，称为本地线程分配缓冲（TLAB），线程分配内存先在TLAB上分配，TLAB用完了再同步分配新的缓存区 JVM将分配到的内存空间（不包括对象头）初始化为零值，这步保证对象的实例字段可以不赋初始值就直接使用 JVM对对象进行必要的设置，例如这个对象是哪个类的实例、如何才能找到类的元数据信息、对象的哈希码、对象的GC分代年龄等信息，这些信息存放在对象的对象头中 执行构造函数，即Class文件中的&lt;init&gt;() 2.3.2 对象的内存布局 对象头 用于存储对象自身的运行时数据，如HashCode、GC分代年龄、锁状态标志、线程持有的锁、偏向线程ID、偏向时间戳，称为“Mark Word”。这部分数据长度32比特或64比特，Bitmap存储，为了在极小空间存储更多数据，不同状态的对象用不同标志位表示不同存储内容 类型指针，即对象指向它的类型元数据的指针，JVM通过该指针来确定对象是哪个类的实例。若对象是数组，还必须记录数组长度 实例数据，包括父类继承下来的，和子类中定义的字段 对齐填充，HotSpot要求对象大小必须是8字节的整数倍，不够就对齐填充 2.3.3 对象的访问定位 通过栈上的reference数据来访问堆上的具体对象，访问方式由虚拟机实现决定，主流有两种 句柄访问。Java堆会划分出一块内存来作为句柄池，reference中存储的是对象的句柄地址，句柄中包含了对象实例数据与类型数据各自具体的地址信息。优点是对象被移动时只需要改变句柄中的实例数据指针 直接指针访问。reference存储的就是对象地址，类型数据指针在对象中。优点是节省一次指针定位的时间开销，HotSpot使用此方式来访问对象 2.4 实现：OutOfMemoryError异常 2.4.1 Java堆溢出 /** * VM options:-Xms20m -Xmx20m -XX:+HeapDumpOnOutOfMemoryError * @author panjiahao.cs@foxmail.com * @date 2023/6/18 19:54 */ public class HeapOOM &#123; static class OOMObject&#123; &#125; public static void main(String[] args) &#123; ArrayList&lt;OOMObject&gt; list = new ArrayList&lt;&gt;(); while(true)&#123; list.add(new OOMObject()); &#125; &#125; &#125; 排查思路：首先确认导致OOM的对象是否是必要的，也就是是分清楚是内存泄露了还是内存溢出了 如果是内存泄露了，可以通过工具查看泄露对象到GC Roots的引用链，定位到产生内存泄露的代码的具体位置 如果是内存溢出了，可以调大堆空间，优化生命周期过长的对象 2.4.2 虚拟机栈和本地方法栈溢出 HotSpot不区分虚拟机栈和本地方法栈，所以-Xoss（本地方法栈大小）参数没效果，栈容量由-Xss参数设定。 当线程请求的栈深度大于虚拟机所允许的最大深度，将抛出StackOverflowError异常；当扩展栈容量无法申请到足够内存，将抛出OutOfMemoryError异常。HotSpot不支持扩展栈容量 2.4.3 方法区和运行时常量池溢出 JDK8使用元空间取代了永久代，运行时常量池移动到了堆中，所以可能会产生堆内存的OOM 方法区的主要职责是存放类型信息，如类名、访问修饰符、常量池、字段描述、方法描述等。用CGLib不断生成增强类，可能产生元空间的OOM /** * VM Args：-XX:MaxMetaspaceSize=10M * @author panjiahao.cs@foxmail.com * @date 2023/6/18 22:13 */ public class JavaMethodAreaOOM &#123; public static void main(String[] args) &#123; while(true)&#123; Enhancer enhancer = new Enhancer(); enhancer.setSuperclass(HeapOOM.OOMObject.class); enhancer.setUseCache(false); enhancer.setCallback(new MethodInterceptor() &#123; @Override public Object intercept(Object obj, Method method, Object[] objects, MethodProxy proxy) throws Throwable &#123; return proxy.invokeSuper(obj,args); &#125; &#125;); enhancer.create(); &#125; &#125; static class OOMObject&#123; &#125; &#125; 2.4.4 本机直接内存溢出 直接内存通过-XX:MaxDirectMemorySize参数来指定，默认与Java堆最大值一致 虽然使用DirectByteBuffer分配内存会OOM，但它抛出异常时并没有真正向操作系统申请内存，而是通过计算得知无法分配就手动抛异常，真正申请内存的方法是Unsafe::allocateMemory() /** * VM Args:-Xmx20M -XX:MaxDirectMemorySize=10M * @author panjiahao.cs@foxmail.com * @date 2023/6/19 20:38 */ public class DirectMemoryOOM &#123; private static final int _1MB = 1024 * 1024; public static void main(String[] args) throws IllegalAccessException &#123; Field unsafeField = Unsafe.class.getDeclaredFields()[0]; unsafeField.setAccessible(true); Unsafe unsafe = (Unsafe) unsafeField.get(null); while (true) &#123; unsafe.allocateMemory(_1MB); &#125; &#125; &#125; 由直接内存导致的内存溢出，一个明显的特征是Heap Dump文件不会看到有什么明显的异常情况。如果内存溢出后产生的Dump文件很小，而程序中又直接或间接使用了DirectMemory（NIO），可以重点检查直接内存 3. 垃圾收集器与内存分配策略 3.1 概述 线程独占的程序计数器、虚拟机栈、本地方法栈3个区域的内存分配和回收都具备确定性。 而Java堆和方法区有显著的不确定性：一个接口的多个实现类需要的内存可能不一样，一个方法不同分支需要的内存也不同，只有处于运行期间，才知道程序会创建哪些对象，这部分内存的分配和回收是动态的 3.2 判断对象是否存活的算法 引用计数法。看似简单，但必须配合大量额外处理才能正确工作，譬如简单的引用计数法无法解决对象循环引用 可达性分析算法。从GC Roots对象开始，根据引用关系向下搜索，走过的路径称为“引用链”，引用链上对象仍然存活，不在引用链上的对象可回收。 GC Roots对象包括以下几种： 虚拟机栈中引用的对象。例如方法参数、局部变量等 方法区中类静态属性引用的对象。例如Java类的引用类型静态变量 方法区中常量引用的对象。例如字符串常量池里的引用 本地方法栈JNI引用的对象 Java虚拟机内部的引用，如基本数据类型对应的Class对象，一些常驻的异常对象（比如NullPointException、OutOfMemoryError）、还有系统类加载器 所有被同步锁（synchronized）持有的对象 反映Java虚拟机内部情况的JMXBean、JVMTI中注册的回调、本地代码缓存等 根据用户选用的垃圾收集器以及回收区域，临时加入其它对象。目的是当某个区域垃圾回收时，该区域的对象也可能被别的区域的对象引用 引用包含以下几种类型： 强引用：被强引用引用的对象不会被回收 软引用：被软引用引用的对象在OOM前会被回收 弱引用：被弱引用引用的对象在下一次垃圾回收时被回收 虚引用：虚引用不会影响对象的生存时间，唯一目的是能在对象被回收时收到一个系统通知 即便对象已经不可达，也不是立即标记为可回收，对象真正死亡要经历两次标记过程：可达性分析发现不可达就第一次标记；如果对象重写了finalize()方法且没过JVM调用过，那么该对象会被放到队列中，由Finalizer线程去执行finalize()方法，这是对象自救的最后一次机会，只要重新与引用链上任意对象建立关联就行，譬如把this赋给某个对象的成员变量，第二次标记时就会被移除“即将回收”集合 /** * 此代码演示了两点： * 1.对象可以在被GC时自我拯救。 * 2.这种自救的机会只有一次，因为一个对象的finalize()方法最多只会被系统自动调用一次 * @author panjiahao.cs@foxmail.com * @date 2023/6/20 21:52 */ public class FinalizeEscapeGC &#123; public static FinalizeEscapeGC SAVE_HOOK = null; public void isAlive()&#123; System.out.println(&quot;yes, i am still alive&quot;); &#125; @Override protected void finalize() throws Throwable &#123; super.finalize(); System.out.println(&quot;finalize method executed! :)&quot;); FinalizeEscapeGC.SAVE_HOOK = this; &#125; public static void main(String[] args) throws InterruptedException &#123; SAVE_HOOK = new FinalizeEscapeGC(); SAVE_HOOK = null; System.gc(); Thread.sleep(500); if (SAVE_HOOK != null) &#123; SAVE_HOOK.isAlive(); &#125; else &#123; System.out.println(&quot;no ,i am dead! :(&quot;); &#125; // 自救失败 SAVE_HOOK = null; System.gc(); Thread.sleep(500); if (SAVE_HOOK != null) &#123; SAVE_HOOK.isAlive(); &#125; else &#123; System.out.println(&quot;no ,i am dead! :(&quot;); &#125; &#125; &#125; 方法区没有强制要垃圾回收，例如JDK11的ZGC不支持类卸载。 方法区主要回收两部分：废弃的常量和不再使用的类型。 回收废弃常量和回收堆中的对象非常类似 回收“不再使用的类”需要满足三个条件： 该类所有实例已被回收，包括子类实例 加载该类的类加载器已被回收，这个条件很难 该类对象的java.lang.Class对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法 3.3 垃圾收集算法 从如何判定对象消亡的角度出发，GC算法可以划分为“引用计数式”和“追踪式”，这两类也被称作“直接垃圾收集”和“间接垃圾收集”。本节介绍追踪式垃圾收集 3.3.1 分代收集理论 分代收集理论建立在两个假说上： 弱分代假说：绝大多数对象都是朝生夕灭的 强分代假说：熬过越多次垃圾收集过程的对象就越难以消亡 分代收集在遇到对象之间存在跨代引用时需要遍历其它代的所有对象来确定是否还存在跨代引用，性能负担大，所以给分代收集理论添加第三个假说： 跨代引用假说：跨代引用相对于同代引用来说占极少数 分代收集的名词定义： 部分收集（Partial GC）：指目标不是完整收集整个Java堆的垃圾收集，其中又分为： 新生代收集（Minor GC/Young GC）：指目标只是新生代的垃圾收集 老年代收集（Major GC/Old GC）：指目标只是老年代的垃圾收集。目前只有CMS会有单独收集老年代的行为 混合收集（Mixed GC）：指目标是收集整个新生代以及部分老年代的垃圾收集。目前只有G1会有这种行为 整堆收集（Full GC）：收集整个Java堆和方法区的垃圾 3.3.2 标记-清除算法 标记所有需要回收的对象，标记完成后，统一回收 缺点：执行效率不稳定；内存空间碎片化 3.3.3 标记-复制算法 将内存划分为大小相等的两块，每次只使用一块。当这一块的内存用完了，就将还存活着的对象复制到另一外上面，然后再把已使用过的内存空间一次性清理掉 优点：对于多数对象都是可回收的情况，算法复制开销小；没有碎片 缺点：可用内存小了一半；需要空间担保 1:1划分新生代太浪费空间，HotSpot将新生代划分成Eden:Survivor0:Survivor0 = 8:1:1，每次可以用Eden和一块Survivor，垃圾回收时把存活对象写到另一块Survivor，然后清理掉Eden和已用过的Survivor，当Survivor空间不足以容纳一次Minor GC之后存活的对象时，就需要依赖老年代进行分配担保 3.3.4 标记-整理算法 标记所有存活的对象，然后移动到内存空间一端，清理掉边界以外的内存 优点：解决了标记-清除算法造成的空间碎片化问题 缺点：整理期间，用户应用程序暂停，这段时间被称为“Stop The World” 整理即移动对象，移动则内存回收时会更复杂，不移动则内存分配会更复杂。从GC停顿时间来看，不移动对象停顿时间短；从吞吐量来看，移动对象更划算。 HotSpot里关注吞吐量的Parallel Scavenge收集器采用标记-整理算法，关注延迟的CMS收集器采用标记-清除算法 3.4 HotSpot的算法细节实现 3.4.1 根节点枚举 固定作为GC Root的节点主要在全局性的引用（例如常量或类静态属性）与执行上下文（例如栈帧中的本地变量表），Java程序越来越庞大，逐个作为起点进行可达性分析会消耗大量时间 目前，所有收集器在根节点枚举时和整理内存碎片一样必须暂停用户线程，可达性分析算法可以和用户线程一起并发。为了降低STW时间，在不扫描全部的GC Root节点情况下，得知哪些地方存在对象引用，HotSpot提供了OopMap的数据结构保存引用的位置信息 3.4.2 安全点 OopMap可以帮助HotSpot快速完成GC Root枚举，但是如果为每条改变OomMap内容的指令都生成对应的OopMap，会需要大量额外存储空间 因此HotSpot没有为每条指令都生成OopMap，只在特定位置生成，这些位置称为安全点。用户程序只有在执行到安全点才能停顿下来垃圾回收。 安全点的选取考虑：不能太少以至于让收集器等待时间太长，也不能太频繁以至于增大内存负担 如何在垃圾回收时让所有线程（不包括执行JNI调用的线程）都跑到最近的安全点，然后停顿下来。有两种方案： 抢先式中断：先把用户线程全部中断，如果发现有用户线程不在安全点上就恢复这个线程，过一会再中断直至它跑到安全点。现在几乎不使用 主动式中断：设置一个标志位，各个线程执行时主动轮询这个标志，一旦为真主动中断挂起。HotSpot使用内存保护陷阱的方式将轮询操作精简至只有一条汇编指令 3.4.3 安全区域 当用户线程处于Sleep或者Blocked状态时不能执行到安全点。针对这种情况，引入安全区域。 安全区域是指在某一段代码片段中，引用关系不会发生变化，在这个区域中任意地方开始GC都是安全的。 当用户线程执行到安全区域时，首先标识自己进入了安全区域，这样在GC时，虚拟机就不会去管这些已标识自己进入安全区域的线程。当线程离开安全区域时，它检查虚拟机是否完成了根节点枚举（或者其它需要暂停用户线程的阶段），如果完成了就继续执行，否则等待直到收到可以离开安全区域的信号为止。 3.4.4 记忆集与卡表 记忆集是一种记录从非收集区域指向收集区域的指针集合的抽象数据结构，用于解决对象跨代引用带来的问题 记忆集最简单的实现是非收集区域中所有含跨代引用的对象数组，这种结构浪费太多空间，可以粗化记录粒度： 字长精度：每个记录精确到一个机器字长（就是处理器的寻址位数，如32或64），该字包含跨代指针 卡精度：每个记录精确到一块内存区域，该区域有对象含有跨代指针 卡精度使用卡表实现，卡表的实现是一个字节数组，字节数组每个元素都对应着一块特定大小的内存块，称为卡页。只要卡页内有一个或更多对象的字段存在跨代引用，卡表中对应的数组元素的值标识为1，称为变脏。垃圾收集时，只要筛选出卡表中变脏的元素就可以知道哪些卡页内存块有跨代引用，把它们放入GC Roots中一起扫描 3.4.5 写屏障 卡表元素变脏的时间点是引用类型字段赋值那一刻，HotSpot通过写屏障来维护卡表状态。写屏障可以看作JVM层面对“引用类型字段赋值”这个动作的AOP切面。 写屏障会导致伪共享问题，伪共享是指，现代CPU的缓存系统是以缓存行为单位存储的，当多线程修改相互独立的变量时，如果这些变量恰好共享同一个缓存行，就会彼此影响（写回、无效化或者同步）而导致性能降低 伪共享的一种简单解决方法是不采用无条件的写屏障，而是先检查卡表标记，只有卡表元素未被标记时才将其变脏。HotSpot参数-XX:+UseCondCardMark决定是否开启卡表更新，开启会多一次判断开销，但能够避免伪共享带来的性能损耗 3.4.6 并发的可达性分析 可达性分析在标记阶段会暂停用户线程以在一致性的快照上进行对象图的遍历，不一致情况下会出现“对象消失”问题。原因可以由三色标记方法推导 白色：表示对象尚未被垃圾收集器访问过。显然在可达性分析刚开始时，所有对象都是白的，若在分析结束阶段，仍然是白色的对象是不可达 黑色：表示对象已经被垃圾收集器访问过，且这个对象的所有引用都已经扫描过。黑色对象代表已经扫描过，是安全存活的，如果有其它对象引用指向了黑色对象，无须重新扫描一遍。黑色对象不可能直接（不经过灰色对象）指向某个白色对象 灰色：表示对象已经被垃圾收集器访问过，但这个对象上至少存在一个引用还没有被扫描过 当且仅当以下两个条件同时满足时，会产生“对象消失”问题，即原本应该黑色的对象被误标为白色： 赋值器插入了一条或多条从黑色对象到白色对象的新引用 赋值器删除了全部从灰色对象到该白色对象的直接或间接引用 解决并发扫描时的对象消失问题，只需破坏两个条件之一即可，因此有两种方案： 增量更新。增量更新破坏条件一，当黑色对象插入新的指向白色对象的引用关系时，就将这个新插入的引用记录下来，等并发扫描结束后，再以这个黑色对象为根，重新扫描一次。可以理解为，黑色对象一旦新插入指向白色对象的引用之后，它就变回灰色对象 原始快照。当灰色对象要删除指向白色对象的引用关系时，就将这个要删除的引用记录下来，在并发扫描结束之后，再将这些记录过的引用关系中的灰色对象为根，重新扫描一次。可以理解为，无论引用关系删除与否，都会按照刚开始扫描那一刻的对象图快照进行搜索 对引用关系记录的插入和删除都是通过写屏障实现。 3.5 经典垃圾收集器 图中连线表示可以搭配使用 3.5.1 Serial收集器 适合资源（cpu和内存）受限的场景，新生代一两百兆以内的垃圾收集停顿时间最多一百多毫秒以内 3.5.2 ParNew收集器 实质是多线程版的Serial 3.5.3 Parallel Scavenge收集器 与ParNew类似，不同点是其它收集器关注停顿时间，Parallel Scavenge收集器目标是可控制的吞吐量 -XX:MaxGCPauseMillis控制最大垃圾收集停顿时间，-XX:GCTimeRatio直接设置吞吐量，-XX:+UseAdaptiveSizePolicy会根据系统运行情况动态调整虚拟机参数以获得最合适的停顿时间或者最大的吞吐量 3.5.4 Serial Old收集器 Serial收集器的老年代版本 3.5.5 Parallel Old收集器 Parallel Scavenge的老年代版本，在注重吞吐量或者处理器资源稀缺场合，可以考虑使用Parallel Scavenge+Parallel Old 3.5.6 CMS收集器 四步骤： 1）初始标记 2）并发标记 3）重新标记 4）并发清理 初始标记和重新标记仍然要暂停用户线程。初始标记仅仅标记GC Roots能直接关联的对象，速度很快；并发标记从关联对象开始遍历整个对象图，不需要暂停用户线程；重新标记用来修正并发标记期间的变动（增量更新） 优点：并发收集、低停顿 缺点： 对处理器资源敏感。并发会占用处理器，降低吞吐量 无法处理“浮动垃圾”。“浮动垃圾”指并发标记和清理期间用户线程产生的垃圾在下一次GC时清理，预留空间不足会，预留空间不足会导致“Concurrent Mode Failure”进而导致Full GC或者Serial Old重新回收老年代。 内存空间碎片化 3.5.7 Garbage First（G1）收集器 开创面向局部收集的设计思路和基于Region的内存布局形式 局部收集只收集范围不是新生代或老年代，而是堆中任意部分。 基于Region的堆内存布局：G1不再坚持固定大小和数量的分代区域划分，而是把连续的堆划分为多个大小相等的独立区域Region，每个Region都可以根据需要扮演新生代或者老年代。超过Region容量一半的对象会被存到Humongous区域中，超过整个Region容量的对象会被存到N个连续的Humongous Region中，G1大多数行为把Humongous Region当老年代处理 四个步骤： 初始标记：标记GC Roots能直接关联的对象，修改TAMS指针的值，让下一阶段用户并发运行时能正确在可用的Region中分配新对象 并发标记：递归扫描对象图。扫描完成后重新处理SATB记录下的在并发时有引用变动的对象 最终标记：处理并发阶段遗留的少量SATB记录 筛选回收：负责更新Region统计数据，制定回收计划 3.6 低延迟垃圾收集器 垃圾收集器“不可能三角”：内存占用、吞吐量、延迟。延迟是最重要指标 下图，浅色表示挂起用户线程，深色表示gc线程和用户线程并发 Shenandoah和ZGC在可管理的堆容量下，停顿时间基本是固定的，两者被命名为低延迟垃圾收集器 3.6.1 Shenandoah收集器 目标：能在任何堆内存大小下都可以把GC停顿时间限制在10ms以内 Shenandoah和G1有相似的堆内存布局，在初始标记、并发标记等阶段的处理思路高度一致。 有三个明显的不同： 支持并发的整理算法 默认不分代，即不使用专门的新生代Region和老年代Region 放弃G1中耗费大量内存和计算资源去维护的记忆集，改为“连接矩阵”记录跨Region的引用关系 收集器的工作分为九个阶段： 初始标记：与G1一样，首先标记GC Roots直接关联的对象，停顿时间与堆大小无关，与GC Roots数量有关 并发标记：与G1一样，遍历对象图，与用户线程并发，时间取决于对象数量和对象图的结构复杂程度 最终标记：与G1一样，处理剩余的SATB扫描，统计价值最高的Region组成回收集，有一小段停顿时间 并发清理：清理一个存活对象都没有的Region 并发回收：将回收集中的存活对象复制到未被使用的Region中，通过读屏障和“Brooks Pointers”转发指针解决和用户线程并发产生的问题 初始引用更新：一个非常短暂的停顿，用于确保所有并发回收阶段中收集器线程都已完成分配对象移动任务 并发引用更新：真正开始进行引用更新，与用户线程并发 最终引用更新：修正存在于GC Roots中的引用，停顿时间与GC Roots的数量有关 并发清理：经过并发回收和引用更新后，整个回收集的Region已经没有存货对象，回收这些Region的内存空间 对象移动和用户程序并发，原来的解决方案是在被移动对象原有的内存上设置保护陷阱，一旦用户程序访问到原有的地址就产生自陷中断，进入预设的异常处理器，将访问转发到新对象。这个方案会导致用户态频繁切换到核心态 Brooks Pointers是在原有对象布局结构的最前面加一个新的引用字段，移动前指向自己，移动后指向新对象。这里需要用CAS解决对象访问的并发问题 3.6.2 ZGC收集器 ZGC收集器是一款基于Region内存布局的，不设分代的，使用了读屏障、染色指针和内存多重映射等技术来实现可并发的标记-整理算法的，以低延迟为首要目标的一款垃圾收集器。收集过程全程可并发，短暂停顿时间只与GC Roots大小相关而与堆内存大小无关 ZGC的Region是动态的：动态创建和销毁、动态大小 染色指针：把标记信息记在引用对象的指针上，标记信息有4个bit，虚拟机可以直接从指针中看到其引用对 象的三色标记状态、是否进入了重分配集（即被移动过）、是否只能通过finalize()方法才能被访问到 三大优势： 染色指针可以使得一旦某个Region的存活对象被移走之后，这个Region立即就能够被释放和重用 掉，而不必等待整个堆中所有指向该Region的引用都被修正后才能清理 染色指针可以大幅减少在垃圾收集过程中内存屏障的使用数量，设置内存屏障，尤其是写屏障的 目的通常是为了记录对象引用的变动情况，如果将这些信息直接维护在指针中，显然就可以省去一些专门的记录操作 染色指针可以作为一种可扩展的存储结构用来记录更多与对象标记、重定位过程相关的数据，以 便日后进一步提高性能 标志位影响了寻址地址，ZGC通过多重映射，将不同标志位的指针映射到同一内存区域 ZGC的收集过程分为四大阶段 并发标记：与G1、Shenandoah类似，区别是标记在指针上而不是对象上，标记会更新染色指针的Marked 0、Marked 1标志位 并发预备重分配：根据特定的查询条件统计得出本次收集过程要清理哪些Region，将这些Region组成重分配集 并发重分配：把重分配集中的存活对象复制到新的Region上，并为重分配集中的每个Region维护一个转发表，记录从旧对象到新对象的转向关系 并发重映射：修正整个堆中指向重分配集中旧对象的所 有引用 3.7 选择合适的垃圾收集器 3.7.1 Epsilon收集器 不收集垃圾，负责堆的管理与布局、对象的分配、与解释器的协作、与编译器的协作、与监控子系统协作等职责 响应微服务而生，在堆内存耗尽前就退出，不收集垃圾就非常合适 3.7.2 收集器的权衡 三个因素： 应用程序关注点是什么？吞吐量、延时还是内存占用 基础设施如何？处理器的数量、内存大小、操作系统 JDK的发行商是谁？版本号是多少 例如直接面向用户的B/S系统，延迟是主要关注点。 预算充足就用商业的 预算不足但能掌控基础设施，可以尝试ZGC 对ZGC的稳定性有疑虑就考虑Shenandoah 软硬件和JDK都老的考虑CMS","categories":[{"name":"Java","slug":"Java","permalink":"https://zunpan.github.io/categories/Java/"}],"tags":[{"name":"深入理解Java虚拟机","slug":"深入理解Java虚拟机","permalink":"https://zunpan.github.io/tags/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Java%E8%99%9A%E6%8B%9F%E6%9C%BA/"}]},{"title":"Excel比对与合并系统","slug":"Excel比对与合并系统","date":"2023-03-03T07:00:03.000Z","updated":"2023-09-24T04:27:40.287Z","comments":true,"path":"2023/03/03/Excel比对与合并系统/","link":"","permalink":"https://zunpan.github.io/2023/03/03/Excel%E6%AF%94%E5%AF%B9%E4%B8%8E%E5%90%88%E5%B9%B6%E7%B3%BB%E7%BB%9F/","excerpt":"","text":"背景 许多游戏策划使用Excel来配置数值。策划需要保存所有版本的数值表，有时需要查看两个数值表有何差异，有时想把差异或者叫修改应用到另一张数值表中，这非常类似于版本控制，但是市面上的版本控制系统svn和git都是针对文本文件，不能用于Excel的版本控制 Excel比对算法 一维数据比对算法 假设有两个序列A1...AmA_1...A_mA1​...Am​和B1...BnB_1...B_nB1​...Bn​，我们可以通过对A序列进行一些列操作，使之变为B序列。对每种操作都定义个Cost，如何找到总Cost最小的使A变为B的操作序列，可以通过动态规划解决。这是一个已经被广为研究的算法问题，本文就不再整述，读者可以在网上搜索Edit编辑距离获取更多信息。 操作集合的定义有多种方式，一种较为常见的操作集合定义如下(Cost均为1) : 在序列中插入一个元素: 在序列中删除一个元素; 比如，将字符串kiten变换为sitting，需要删除k，插入s，删除e，插入i，在尾部插入g。如果在原序列和目标序列中去掉删除和插入的元素，那么原序列和目标序列就是完全相同的了(比如上面的例子两者都变为itn了)，因此这种编辑距离被命名为LCS (Longest Common Subsequence) 编辑距离。LeetCode 1143. 最长公共子序列 再回到本文要讨论的差异比较问题，要比较两个序列的差异，实际上就是要找到二者之间尽量多的公共部分，剩下的就是差异部分，所以这和最短编辑距离问题是完全等价的。 此外，除了LCS编辑距离之外，还有一种常用的编辑距离，允许插入、删除和修改操作，叫做Levenshtein编组距离。另外，还可以定义一种广义的Levenshtein编辑距离，删除元素AiA_iAi​和插入元素BjB_jBj​;的Cost由一个单参数函数决定，记为Cost(AiA_iAi​)或Cost(BjB_jBj​); 将AiA_iAi​修改为BjB_jBj​;的操作的Cost由一个双参数函数决定，记为Cost2(Ai,BjA_i, B_jAi​,Bj​)。 /** * 比对的基本单位是单个字符 * @param text1 字符串1 * @param text2 字符串2 * @return levenshteinDP数组 */ static int[][] levenshteinDP(String text1, String text2) &#123; int len1 = text1.length(); int len2 = text2.length(); // dp[i][j]表示从text1[0...i-1]到text2[0...j-1]的最小编辑距离（cost） dp = new int[len1 + 1][len2 + 1]; // path记录此方格的来源是多个此类枚举值的布尔或值 path = new int[len1 + 1][len2 + 1]; for (int i = 0; i &lt; len1 + 1; i++) &#123; dp[i][0] = i; path[i][0] = FROM_INIT; &#125; for (int j = 0; j &lt; len2 + 1; j++) &#123; dp[0][j] = j; path[0][j] = FROM_INIT; &#125; for (int i = 1; i &lt; len1 + 1; i++) &#123; for (int j = 1; j &lt; len2 + 1; j++) &#123; path[i][j] = FROM_INIT; int left = dp[i][j - 1] + 1; int up = dp[i - 1][j] + 1; int leftUp; boolean replace; if (text1.charAt(i - 1) == text2.charAt(j - 1)) &#123; leftUp = dp[i - 1][j - 1]; replace = false; &#125; else &#123; leftUp = dp[i - 1][j - 1] + 1; replace = true; &#125; dp[i][j] = Math.min(Math.min(left, up), leftUp); if (dp[i][j] == left) &#123; path[i][j] |= FROM_LEFT; &#125; if (dp[i][j] == up) &#123; path[i][j] |= FROM_UP; &#125; // 对应：两字符完全一样或者可以修改成一样 if (dp[i][j] == leftUp) &#123; if (replace) &#123; path[i][j] |= FROM_LEFT_UP_REPLACE; &#125; else &#123; path[i][j] |= FROM_LEFT_UP_COPY; &#125; &#125; &#125; &#125; return dp; &#125; 同样的，对于广义Levenshtein编辑距离，如果去掉删除和插入的元素，那么两个序列中剩下的元素即为一一对应的关系，每组对应的两个元素，要么是完全相同的，要么前者是被后者修改掉的。从这部分论述中我们不难看出，比较算法的核心思路实际上就是找到元素与元素之间的一一对应关系 二维数据比对算法 Excel的一个分页可以认为是一个二维数据阵列，阵列中的每个元素是对应单元格内容的字符串值。根据前面的论述，比较两个二维阵列的核心就是找到他们公共的行/列，或者说原阵列和目标阵列的行/列对应关系。比如，对于下面两张表: 甲表的第1、2、3列对应乙表的1、2、4列，甲表的1、3行对应乙表的1、2行。那么这两张表的差异通过下列方式描述: 在第2列的位置插入新列 删除第2行 将(原表中) 第3行第3列的元素从9修改为0; 如何计算两张表对应的行/列，一个比较容易想到的方案是将其拆分为两个独立求解的问题，计算对应的行和计算对应的列。对于前者，我们可以把阵列的每一行当成一个元素，所有行组成一个序列，然后对这个序列进行比较:后者亦然。这样我们就把二维比较问题转化成了一维比较的问题。关于编辑距离的定义，可以采用广义Levenshtein编辑距离，定义删除、插入元素的Cost为该行(列)的元素数，定义修改元素的Cost为这两行(列)之间的LCS编辑距离.于是两个二维阵列的比较过程如下: 找到二者对应的 (或者叫公共的) 行/列，非公共的行/列记为删除、插入行/列操作；两张表只保留公共的行/列，此时他们尺寸完全相同，对应位置的单元格逐一比较，如果值不相同，则记为单元格修改操作; 算法优化 上一个部分介绍的二维阵列比较方案只是一个理论上可行的方案，在实际应用中，存在以下问题: 删除、插入行/列的操作都是对于整个行/列的，而计算两行/列之间的LCS编辑距离是独立计算的，因此算法本身有一定不合理性; 计算修改Cost里又包含了LCS编辑距离的计算，二层嵌套，性能开销比较大; 针对上述问题，从游戏开发的实际应用场景出发，做了如下优化: 首先计算列之间的对应关系，只取表头前h行(不同项目表头行数h可能不同，可以通过参数配置) ，这样就把对整列的LCS编辑距离计算优化为h个单元格逐已比较，大幅优化了效率，而且对准确度基本不会有什么影响; 根据上一步的计算结果，去掉非公共的列(即删除、添加的列)，这样，剩下的列都是两边一一对应的了，此时再计算行的对应关系，修改操作的Cost定义就可以从LCS编辑距离改为单元格的逐一比较了，这样又大幅优化了性能 在上面所述基础之上，还可以再做优化，因为在实际应用中，绝大多数情况下，绝大多数行都不会有修改，因此可以先用LCS编辑距离对所有行的对应关系进行计算，即只有当两行内容完全相同时才会被当做是对应的; 然后再把剩下的未匹配的行分组用广义Levenshtein编辑距离进行对应关系匹配。这样么做的原因是LCS编辑距离比广义Levenshtein编辑距离的求解速度要快很多。 功能扩展 在开发过程中，我们经常会将单行或者连续若干行单元格上移或下移到另一位置，按照目前的比较逻辑，该操作会被认为是删除这些行，然后在新的位置重新插入这些行。这样的结果和不合理的。为此，我们可以引入一种新的操作: 移动行到另一位置。加入了这个新的操作之后，我们依然可以建立之前所述的行对应关系，只不过两边行的顺序可以是乱序的。这种不保序对应关系可以通过多个轮次的编辑距离匹配计算，每次匹配之后去掉匹配上的行，剩下未匹配的行组成一个新的序列进行下一轮的匹配。每轮匹配是采用LCS编辑距离还是广义Levenshtein编指距离可以灵活决定，比如前若干轮或者行数较多时采用LCS编辑距离，后面的轮次再用广义Levenshtein编辑距离。","categories":[{"name":"Java","slug":"Java","permalink":"https://zunpan.github.io/categories/Java/"}],"tags":[{"name":"LCS","slug":"LCS","permalink":"https://zunpan.github.io/tags/LCS/"},{"name":"Levenshtein","slug":"Levenshtein","permalink":"https://zunpan.github.io/tags/Levenshtein/"},{"name":"编辑距离","slug":"编辑距离","permalink":"https://zunpan.github.io/tags/%E7%BC%96%E8%BE%91%E8%B7%9D%E7%A6%BB/"},{"name":"diff","slug":"diff","permalink":"https://zunpan.github.io/tags/diff/"},{"name":"merge","slug":"merge","permalink":"https://zunpan.github.io/tags/merge/"}]},{"title":"MySQL实战45讲学习笔记","slug":"MySQL实战45讲学习笔记","date":"2023-02-01T10:57:05.000Z","updated":"2023-09-24T04:27:40.280Z","comments":true,"path":"2023/02/01/MySQL实战45讲学习笔记/","link":"","permalink":"https://zunpan.github.io/2023/02/01/MySQL%E5%AE%9E%E6%88%9845%E8%AE%B2%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","excerpt":"","text":"基础架构：一条SQL查询语句是如何执行的 MySQL可以分为Server层和存储引擎层两部分 Server层包括连接器、查询缓存、分析器、优化器、执行器等，涵盖MySQL的大多数核心服务功能，以及所有的内置函数（如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。 而存储引擎层负责数据的存储和提取。其架构模式是插件式的，支持InnoDB、MyISAM、Memory等多个存储引擎 连接器 连接器负责跟客户端建立连接、获取权限、维持和管理连接。建立连接后，权限修改不会影响已经存在的连接的权限 长连接：连接成功后，如果客户度持续有请求，一直使用同一个连接 短连接：每次执行很少的几次查询就断开连接，下次查询再重新建立 建立连接比较耗时，尽量使用长连接，但是全部使用长连接会导致OOM，因为MySQL在执行过程中临时使用的内存是管理在连接对象里面，连接不断开内存不会释放 解决方案： 定期断开长连接 执行mysql_reset_connection重新初始化连接资源 查询缓存 执行过的语句及其结果可能会以key-value对的形式，直接缓存在内存中 查询缓存的失效非常频繁，只要有对一个表的更新，这个表上的所有查询缓存都会被清空，因此不要使用查询缓存，MySQL 8.0删掉了此功能 分析器 分析器先做“词法分析”，识别出SQL语句中的字符串分别是什么，例如，识别“select”是查询语句，“T”识别成表名T 然后做“语法分析”，判断输入的SQL语句是否满足MySQL语法，如果语句不对，会收到“You have an error in your SQL syntax”的错误提醒 优化器 优化器是在表里面有多个索引的时候，决定使用哪个索引；或者在一个语句有多表关联（join）的时候，决定各个表的连接顺序 例如下面的语句执行两个表的join： mysql&gt; select * from t1 join t2 using(ID) where t1.c=10 and t2.d=20; 既可以先从表t1里面取出c=10的记录的ID值，再根据ID值关联到表t2，再判断t2里面d的值是否等于20。 也可以先从表t2里面取出d=20的记录的ID值，再根据ID值关联到t1，再判断t1里面c的值是否等于10。 这两种执行方法的结果一样但效率不同，优化器决定使用哪个方法 执行器 判断有没有表的执行权限 根据表的引擎定义调用引擎提供的接口，例如“取满足条件的第一行”，“满足条件的下一行”，数据库的慢查询日志rows_examined字段表示语句在执行过程中扫描了多少行，引擎扫描行数跟rows_examined并不是完全相同的 日志系统：一条SQL更新语句是如何执行的 一条查询语句的执行过程一般是经过连接器、分析器、优化器、执行器等功能模块，最后到达存储引擎。更新语句还涉及 redo log（重做日志）和 binlog（归档日志） redo log WAL（Write-Ahead Logging）：更新记录时，InnoDB引擎会先把记录写到redo log里面并更新内存，然后在适当的时候将操作记录更新到磁盘里面 InnoDB的redo log是固定大小和循环写的，write pos是当前记录的位置，checkpoint是当前要擦除的位置，擦除记录前要把记录更新到数据文件 redo log保证即使数据库异常重启，之前提交的记录都不会丢失，这个能力称为crash-safe binlog redo log是InnoDB引擎特有的日志，Server层特有的引擎是binlog（归档日志） 两者有三点不同 redo log是InnoDB引擎特有的；binlog是MySQL的Server层实现的，所有引擎都可以使用 redo log是物理日志，记录的是“在某个数据页上做了什么修改”；binlog是逻辑日志，记录的是这个语句的原始逻辑，比如“给ID=2这一行的c字段加1” redo log是循环写的，空间固定会用完；binlog是可以追加写入的。“追加写”是指binlog文件写到一定大小后会切换到下一个，并不会覆盖以前的日志 执行器和InnoDB引擎在执行update语句时的内部流程 执行器先找引擎取ID=2这一行。ID是主键，引擎直接用树搜索找到这一行。如果ID=2这一行所在的数据页本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然后再返回。 执行器拿到引擎给的行数据，把这个值加上1，比如原来是N，现在就是N+1，得到新的一行数据，再调用引擎接口写入这行新数据。 引擎将这行新数据更新到内存中，同时将这个更新操作记录到redo log里面，此时redo log处于prepare状态。然后告知执行器执行完成了，随时可以提交事务。 执行器生成这个操作的binlog，并把binlog写入磁盘。 执行器调用引擎的提交事务接口，引擎把刚刚写入的redo log改成提交（commit）状态，更新完成。 下图的浅色框表示在InnoDB内部执行，深色框表示在执行器中执行 两阶段提交 redo log的写入分为两步骤：prepare和commit，也就是”两阶段提交“，目的是为了让两份的日志之间的逻辑一致 当数据库需要恢复到指定的某一秒时，可以先在临时库上这样做： 找到最近的一次全量备份 从备份的时间点开始，将备份的binlog依次取出来重放到指定时间 如果redo log不是两阶段提交 先写redo log后写binlog。假设在redo log写完，binlog还没写完，MySQL异常重启，数据可以恢复，但是binlog没有记录这个语句。之后用binlog恢复临时库时会缺少更新 先写binlog后写redo log。假设binlog写完，redo log还没写，MySQL异常重启之后，这个事务无效，数据没有恢复。但是binlog里面已经有这个语句，所以之后用binlog恢复临时库会多一个事务 innodb_flush_log_at_trx_commit这个参数设置成1的时候，表示每次事务的redo log都直接持久化到磁盘。这个参数建议设置成1，这样可以保证MySQL异常重启之后数据不丢失。 sync_binlog这个参数设置成1的时候，表示每次事务的binlog都持久化到磁盘。建议设置成1，这样可以保证MySQL异常重启之后binlog不丢失。 如果redo log处于prepare状态且binlog写入完成，MySQL异常重启会commit掉这个事务 事务隔离 事务保证一组数据库操作要么全部成功，要么全部失败 隔离性与隔离级别 ACID中的I指的是隔离性（Isolation） 当数据库上有多个事务同时执行的时候，就可能出现脏读（dirty read）、不可重复读（non-repeatable read）、幻读（phantom read）的问题，为了解决这些问题，就有了“隔离级别”的概念。 事务隔离级别包括： 读未提交：一个事务还没提交时，它做的变更就能被别的事务看到 读提交：一个事务提交之后，它做的变更才会被其他事务看到 可重复读：一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。当然在可重复读隔离级别下，未提交变更对其它事务也是不可见的 串行化：对于同一行记录，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成才能继续执行 数据库使用视图来实现隔离级别。在“可重复读”隔离级别下，视图是在事务开启时创建的。在“读提交”隔离级别下，视图是在每个SQL语句开始执行的时候创建的。“读未提交”直接返回记录的最新值，没有视图概念。“串行化”直接用加锁的方式 事务隔离的实现 这里展开说明“可重复读” 在MySQL中，每条记录在更新时都会同时记录一条回滚操作。假设一个值从1按顺序改成了2、3、4，在回滚日志里有类似下面的记录 当前值是4，但是在查询这条记录的时候，不同时刻启动的事务会有不同的read-view。视图A、B、C对应的值分别是1、2、4，同一条记录在系统中可以存在多个版本，这就是数据库的多版本并发控制（MVCC）。对于视图A，要得到1，就必须将当前值依次执行图中所有的回滚操作。即使现在有另一个事务正在将4改成5，这个事务跟视图A、B、C对应的事务是不会冲突的。 系统会将没有比回滚日志更早的read-view时删掉这个回滚日志。因此尽量不要使用长事务，系统里面会存在很老的事务视图 事务的启动方式 MySQL的事务启动方式有如下几种： 显式启动事务，begin 或 start transaction。配套的提交语句是commit，回滚语句是rollback 隐式启动事务，一条SQL语句会自动开启一个事务。需要设置autocommit = 1 才会自动提交 set autocommit=0，会将这个线程的自动提交关掉。事务持续存在直到你主动执行commit 或 rollback语句，或者断开连接 建议使用set autocommit=1，并显示启动事务 在autocommit为1的情况下，用begin显式启动的事务，如果执行commit则提交事务。如果执行 commit work and chain，则是提交事务并自动启动下一个事务，这样也省去了再次执行begin语句的开销 可以在information_schema库的innodb_trx这个表中查询长事务，比如下面这个语句，用于查找持续时间超过60s的事务。 select * from information_schema.innodb_trx where TIME_TO_SEC(timediff(now(),trx_started))&gt;60 如何避免长事务对业务的影响？ 从应用端来看 通过MySQL的general_log确保autocommit=1 包含多个select语句的只读事务，没有一致性要求就拆分 通过SET MAX_EXECUTION_TIME控制每个语句的最长执行时间 从数据库端来看 监控 information_schema.Innodb_trx表，设置长事务阈值，超过就kill 推荐使用Percona的pt-kill 业务测试阶段就输出所有general_log，分析日志提前发现问题 innodb_undo_tablespaces设置成2或更大的值 深入浅出索引（上） 索引的出现是为了提高数据查询的效率 索引的常见模型 哈希表，只适用于只有等值查询的场景，不适用于范围查询 有序数组在等值查询和范围查询场景中都非常优秀，但更新数据需要挪动数组元素，成本太高。只适用于静态存储引擎（数据不再变化） 平衡二叉查找树的时间复杂度是O(log(N))，但是算上每次访问节点的磁盘IO开销，查询非常慢。为了减少磁盘IO次数，出现了N叉树 InnoDB的索引模型 根据叶子节点内容，索引类型分为主键索引和非主键索引 主键索引（聚簇索引）：叶子节点存的是整行数据 普通索引（二级索引）：叶子结点存的是主键的值 基于主键索引和普通索引的查询的区别： 如果语句是select * from T where ID=500，即主键查询方式，则只需要搜索ID这棵B+树； 如果语句是select * from T where k=5，即普通索引查询方式，则需要先搜索k索引树，得到ID的值为500，再到ID索引树搜索一次。这个过程称为回表。 索引维护 在插入新记录时，B+树为了维护有序性会进行页分裂和页合并 自增主键 VS 业务字段主键 性能上：自增主键按序插入，不会触发叶子节点的分裂，而业务字段做主键往往不是有序插入，导致页分裂和页合并，性能差 存储空间上：主键长度越小，普通索引的叶子节点就越小，普通索引占用的空间也就越小。业务字段主键是身份证号（20字节）不如自增主键（4字节或8字节） 业务字段做主键的场景是： 只有一个索引 该索引必须是唯一索引 这就是典型的KV场景，直接将这个字段设置为主键 深入浅出索引（下） 覆盖索引 覆盖索引：索引的叶子节点可以直接提供查询结果，不需要回表 可以为高频请求建立联合索引起到覆盖索引的作用 最左前缀原则 索引项是按照索引定义里面出现的字段的顺序排序的。满足最左前缀，就可以利用索引来加速检索。这个最左前缀可以是联合索引的最左N个字段，也可以是字符串索引的最左M个字符 索引内的字段顺序评估标准： 复用能力，如果该顺序的联合索引能少维护一个索引，那么该顺序优先使用 空间，如果必须维护联合索引和单独索引，那么给小字段单独索引，联合索引的顺序是（大字段，小字段） 索引下推 在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足的条件的记录，减少回表次数（联合索引在按最左匹配时碰到范围查询停止，索引下推可以对后面的索引字段做条件判断后再返回结果集） 全局锁和表锁 根据加锁的范围，MySQL里面的锁大致可以分成全局锁、表级锁和行锁三类 全局锁 全局锁是对整个数据库实例加锁，让整个库处于只读状态 Flush tables with read lock 全局锁的典型使用场景是不支持MVCC的引擎（MyISAM）的全库逻辑备份，如果所有表的引擎支持MVCC，可以在备份时开启事务确保拿到一致性视图（mysqldump加上参数-single-transaction） 让全库只读，另外一种方式是set global readonly = true，但仍然建议使用FTWRL，因为： readonly的值可能会用来做其它逻辑，比如判断是主库还是备库 FTWRL在客户端发生异常断开时，MySQL会自动释放全局锁，而readonly会一直保持 表级锁 表级锁有两种：表锁，元数据锁（meta data lock，MDL） 表锁 语法：lock tables … read/write 表锁会限制其它线程的读写，也会限定本线程的操作对象 例如，线程A执行lock tables t1 read, t2 write;，其它线程写t1、读写t2都会被阻塞，线程A只能执行读t1、读写t2，不能访问其它表 如果支持行锁，一般不使用表锁 元数据锁 MDL不需要显示使用，在访问表时会被自动加上，事务提交才释放，作用是保证读写的正确性 当对表做增删改查操作时，加MDL读锁；当对表结构变更时，加MDL写锁 读锁之间不互斥，因此可以有多个线程同时对一张表增删查改 读写锁之间、写锁之间是互斥的。因此如果有两个线程要同时给一个表加字段，其中一个要等另一执行完再执行 给表加字段的方式： kill掉长事务，事务不提交会一直占着MDL 在alter table语句设置等待时间，如果在等待时间内能拿到MDL写锁最好，拿不到也不要阻塞后面的业务语句。后面重试这个过程 行锁 行锁是针对表中行记录的锁 两阶段锁 两阶段锁协议：在InnoDB事务中，行锁是在需要的时候加上的，但并不是不需要了就立刻释放，而是要等到事务结束才释放 如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放 死锁和死锁检测 死锁：并发系统中不同线程出现循环资源依赖，涉及的线程都在等待别的线程释放资源，导致这几个线程无限等待 死锁出现后有两种策略： 设置等待时间，修改innodb_lock_wait_timeout 发起死锁检测，主动回滚死锁链条中的某一个事务，让其他事务可以继续执行。innodb_deadlock_detect设置为on表示开启 第一种策略，等待时间太长，业务的用户接受不了，等待时间太短会出现误伤。所以一般用死锁检测 死锁检测有性能问题，解决思路有几种： 如果能确保业务一定不会出现死锁，可以临时把死锁检测关掉。这种方法存在业务有损的风险，业务逻辑碰到死锁会回滚重试，但是没有死锁检测会超时导致业务有损 控制并发程度。数据库Server层实现，对相同行的更新，在进入引擎之前排队 将一行改成逻辑上的多行。例如账户余额等于10行之和，扣钱时随机扣一行，这种方案需要根据业务逻辑做详细设计 详解事务隔离 假如有如下表和事务A、B、C mysql&gt; CREATE TABLE `t` ( `id` int(11) NOT NULL, `k` int(11) DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB; insert into t(id, k) values(1,1),(2,2); &quot;快照&quot;在MVCC里是怎么工作的 快照不是整个数据库的拷贝。 InnoDB里每个事务都有一个唯一的transaction id，是事务开始时申请的，严格递增的。每行数据有多个版本，每次事务更新数据时，都会生成一个新的数据版本，并把transaction id赋给这个数据版本的事务id，记为row trx_id。某个版本的数据可以通过当前版本和undo log计算出来 在实现上，InnoDB为每个事务构造了一个数组，用来保存这个事务启动瞬间，当前正在“活跃”的所有事务ID。“活跃”指的是启动了但还没提交 数组里面事务ID的最小值记为低水位，当前系统里面已经创建过的事务ID的最大值加1记为高水位 对于当前事务的启动瞬间来说，一个数据版本的row trx_id，有以下几种可能： 如果落在绿色部分，表示这个版本是已提交的事务或者是当前事务自己生成的，这个数据是可见的； 如果落在红色部分，表示这个版本是由将来启动的事务生成的，是肯定不可见的； 如果落在黄色部分，那就包括两种情况 a. 若 row trx_id在数组中，表示这个版本是由还没提交的事务生成的，不可见； b. 若 row trx_id不在数组中，表示这个版本是已经提交了的事务生成的，可见。 InnoDB利用了“所有数据都有多个版本”的特性，实现了“秒级创建快照”的能力 可以用时间顺序来理解版本的可见性。 一个数据版本，对于一个事务视图来说，除了自己的更新总是可见以外，有三种情况： 版本未提交，不可见； 版本已提交，但是是在视图创建后提交的，不可见 版本已提交，而且是在视图创建前提交的，可见 更新逻辑 更新数据都是先读后写，读是当前值，称为“当前读”（current read）。所以事务B是在(1,2)上进行修改 select如果加锁，也是当前读，不加就是一致读，下面两个select语句分别加了读锁（S锁，共享锁）和写锁（X锁，排它锁）。行锁包括共享锁和排它锁 mysql&gt; select k from t where id=1 lock in share mode; mysql&gt; select k from t where id=1 for update; 假设事务C变成了事务C’ 事务C’还没提交，但是生成了最新版本(1,2)，根据“两阶段协议”，(1,2)这个版本上的写锁还没释放，事务B的更新是当前读，需要加锁，所以被阻塞 可重复读的核心就是一致性读（consistent read）；而事务更新数据时只能用当前读，如果当前的记录的行锁被其他事务占用的话，就进入锁等待。 读提交的逻辑和可重复读的逻辑类似，主要区别是： 在可重复读隔离级别下，只需要在事务开始的时候创建一致性视图，之后事务里的其他查询都共用这个一致性视图； 在读提交隔离级别下，每一个语句执行前都会重新算出一个新的视图。 普通索引和唯一索引之间的选择 普通索引VS唯一索引：两者类似，区别是唯一索引的列值不能重复，允许一个为空 下面从这两种索引对查询语句和更新语句的性能来分析，前提是业务保证记录的唯一性，如果业务不能保证唯一性又有唯一需求，就必须用唯一索引 查询过程 普通索引：查找到满足条件的第一个记录后，需要查找下一个记录，直到碰到第一个不满足条件的记录 唯一索引：由于唯一性，查找到满足条件的第一个记录后就停止 由于InnoDB的数据是按数据页为单位来读写，所以两者性能差距微乎其微 更新过程 change buffer：当需要更新一个数据页时，如果数据页在内存中就直接更新，如果不在，在不影响数据一致性的前提下，InnoDB会将这些更新操作缓存在change buffer中。在下次查询需要访问这个数据页时，读入内存，然后执行change buffer中与这个页有关的操作，这个过程称为merge 唯一索引的更新不能用change buffer，因为需要先将数据页读入内存判断操作是否违反唯一性约束 假如现在有个插入新记录的操作，如果要更新的目标页在内存中，普通索引和唯一索引性能差距不大。如果目标页不在内存中，对于唯一索引来说，需要将数据页读入内存，判断到没有冲突，插入这个值；对于普通索引来说，将更新记录在change buffer，此时普通索引的性能好（主键索引的数据页是一定要加载进内存做更新操作，普通索引的数据页不用进内存） change buffer的使用场景 因为merge的时候是真正做数据更新的时候，在merge之前，change buffer记录的变更越多，收益越大 对于写多读少的业务，change buffer的效果最好，比如账单类、日志类的系统 索引的选择和实践 尽量使用普通索引 如果更新完马上查询，就关闭change buffer。否则开着能提升更新性能 change buffer 和 redo log redo log 主要节省的是随机写磁盘的IO消耗（转成顺序写），而change buffer主要节省的则是随机读磁盘的IO消耗。 MySQL为什么有时候会选错索引 session B 先删除了所有数据然后调用idata存储过程插入了10万行数据。 然后session B 执行三条SQL： // 将慢查询日志的阈值设置为0，表示这个线程接下来的语句都会被记录慢查询日志中 set long_query_time=0; select * from t where a between 10000 and 20000; /*Q1*/ select * from t force index(a) where a between 10000 and 20000;/*Q2*/ Q1走了全表扫描，Q2使用了正确的索引 优化器的逻辑 选择索引是优化器的工作，目的是寻找最优方案执行语句，判断标准包括扫描行数、是否使用临时表、是否排序等因素 上面查询语句没有涉及临时表和排序，说明扫描行数判断错误了 MySQL是怎么判断扫描行数的 MySQL在真正开始执行语句之前，并不能精确知道有多少行，而只能用统计信息估算。这个统计信息就是索引的“区分度”，索引上不同的值称为“基数”，基数越大，区分度越好。基数由采样统计得出。 如果统计信息不对导致行数和实际情况差距较大，可以使用analyze table 表名 来重新统计索引信息 索引选择异常和处理 由于索引统计信息不准确导致的问题，可以用analyze table来解决，其它优化器误判的解决方法如下： 使用force index强行选择索引。缺点是变更不及时，开发通常不写force index，当生产环境出现问题，再修改需要重新测试和发布 修改语句，引导MySQL使用我们期望的索引。缺点是需要根据数据特征进行修改，不具备通用性 新建更合适的索引或删掉误用的索引。缺点是找到更合适的索引比较困难 怎么给字符串字段加索引 可以给字符串字段建立一个普通索引，也可以给字符串前缀建立普通索引。使用前缀索引，定义好长度，就可以既节省空间，又不用额外增加太多查询成本 可以通过统计索引上有多少个不同的值来判断使用多长的前缀，不同值越多，区分度越高，查询性能越好 首先算出这列有多少不同值 mysql&gt; select count(distinct email) as L from SUser; 然后选取不同长度的前缀来看这个值 mysql&gt; select count(distinct left(email,4)）as L4, count(distinct left(email,5)）as L5, count(distinct left(email,6)）as L6, count(distinct left(email,7)）as L7, from SUser; 前缀索引对覆盖索引的影响 前缀索引可能会增加扫描行数，导致影响性能，还可能导致用不上覆盖索引对查询的优化。 前缀索引的叶子节点只包含主键，如果查询字段不仅仅有主键，那必须回表。而用完整字符串做索引，如果查询字段只有主键和索引字段，那就不用回表 其它方式 对于邮箱来说，前缀索引的效果不错。 但是对于身份证来说，可能需要长度12以上的前缀索引，才能满足区分度要求，但是这样又太占空间了 有一些占用空间更小但是查询效率相同的方法： 倒序存储身份证号，建立长度为6的前缀索引，身份证后6位可以提供足够的区分度 加个身份证的整型hash字段，给这个字段加索引 这两种方法的相同点是都不支持范围查询，区别在于： 倒序存储不占额外空间 倒序每次写和读都需要额外调用一次reverse函数，hash字段需要额外调用一次crc32函数，reverse稍优 hash字段的查询性能更稳定一些 为什么MySQL为“抖”一下 “抖”：SQL语句偶尔会执行特别慢，且随机出现，持续时间很短 当内存数据页跟磁盘数据页内容不一致的时候，我们称这个内存页为“脏页”。内存数据写入到磁盘后，内存和磁盘上的数据页的内容就一致了，称为“干净页”。 平时执行很快的更新操作，其实就是在写内存和日志，“抖”可能是在刷脏页（flush），情况有以下几种： redo log满了，系统会停止所有更新操作，把checkpoint往前推进，原位置和新位置之间的所有脏页都flush到磁盘上。尽量避免这种情况，会阻塞更新操作 系统内存不足，淘汰脏页。尽量避免一个查询要淘汰的脏页太多 系统空闲 正常关闭 InnoDB刷脏页的控制策略 使用fio测试磁盘的IOPS，并把innodb_io_capacity设置成这个值，告诉InnoDB全力刷盘可以刷多快 fio -filename=$filename -direct=1 -iodepth 1 -thread -rw=randrw -ioengine=psync -bs=16k -size=500M -numjobs=10 -runtime=10 -group_reporting -name=mytest 不能总是全力刷盘，InnoDB刷盘速度还要参考内存脏页比例和redo log写盘速度 脏页比例不要经常接近75%，查看命令如下： mysql&gt; select VARIABLE_VALUE into @a from global_status where VARIABLE_NAME = &#x27;Innodb_buffer_pool_pages_dirty&#x27;; select VARIABLE_VALUE into @b from global_status where VARIABLE_NAME = &#x27;Innodb_buffer_pool_pages_total&#x27;; select @a/@b; 还有个策略是刷盘的“连坐”机制：脏页的邻居如果是脏页会一起被刷盘。这种策略对机械硬盘有大幅度性能提升，但是SSD的IOPS已不是瓶颈，推荐innodb_flush_neighbors设置成0，只刷自己 为什么表数据删掉一半，表文件大小不变 InnoDB表包含两部分：表结构定义和数据。MySQL 8.0 之前，表结构是存在以.frm为后缀的文件，MySQL 8.0 允许表结构定义放在系统数据表中 参数innodb_file_per_table 设置成OFF表示，表的数据放在系统共享表空间，也就是跟数据字典放在一起； 设置成ON表示，每个InnoDB表数据存储在一个以.ibd为后缀的文件中 从MySQL 5.6.6开始默认就是ON 数据删除流程 InnoDB里的数据都是用B+数的结构组织的。 记录的复用：删除R4记录时，InnoDB会把记录标记为删除，插入ID在300到600之间的记录时可能会复用这个位置，磁盘文件大小不会缩小 数据页的复用：InnoDB的数据是按页存储的。如果将page A上所有记录删除以后，page A会被标记为可复用，这时候插入ID=50的记录需要使用新页时，page A会被复用。因此，delete整个表会把所有数据页都标记为可复用，但是磁盘文件不会变小 可以复用，而没被使用的空间，看起来就像是“空洞”，不只是删除数据会造成空洞，随机插入数据会引发索引的数据页分裂，导致空洞。更新索引上的值，可以理解为删除旧值和插入新值，也会造成空洞。解决空洞的方法是重建表 重建表 可以使用alter table A engine=InnoDB命令来重建表。MySQL 5.6是离线重建，重建期间更新会丢失。 MySQL 5.6 引入了Online DDL，重建表的流程： 建立一个临时文件，扫描表A主键的所有数据页 用数据页中表A的记录生成B+数，存储到临时文件中 生成临时文件的过程中，将所有对A的操作记录在一个日志文件（row log）中，对应图中state2状态 临时文件生成后，将日志文件中的操作应用到临时文件，得到一个逻辑数据上与表A相同的数据文件，对应图中state3状态 用临时文件替换表A的数据文件 alter语句在启动时需要获取MDL写锁，这个写锁在真正拷贝数据之前就退化成读锁了，目的是实现Online，MDL读锁不会阻塞记录的增删改操作（DML） 推荐使用gh-ost做大表的重建 Online 和 inplace inplace是指整个DDL过程在 InnoDB 内部完成，对于 Server层来说，没有把数据挪动到临时表，这是一个“原地”操作，这就是inplace名称由来 和inplace对应的是copy，也就是前面离线重建 DDL过程如果是 Online 的，就一定是inplace的；反过来未必，全文索引和空间索引是 inplace 的，但不是 Online 的 optimize table、analyze table和alter table三种方式重建表的区别： 从MySQL 5.6开始，alter table t engine=InnoDB（也就是recreate）默认就是上面引入Online DDL后的重建过程 analyze table t 不是重建表，只是对表的索引信息做重新统计，没有修改数据，这个过程中加了MDL读锁 optimize table t 等于recreate+analyze count(*)慢该怎么办 count(*)的实现方式 InnoDB count(*)会遍历全表，优化器会找到最小的索引数进行计数，结果准确但有性能问题。show table status命令显示的行数是采样估算的，不准确 用缓存系统保存计数 可以用Redis来保存记录数，但是会出现逻辑上不精确的问题。根本原因是这两个不同的存储构成的系统，不支持分布式事务，无法拿到精确一致的视图 这种情况是Redis的计数不精确 这种情况是查询结果不精确 在数据库保存计数 将计数放在数据库里单独的一张计数表中，可以利用事务解决计数不精确的问题 在会话B读操作期间，会话A还没提交事务，因此B没有看到计数值加1的操作，因此计数值和“最近100条记录”的结果在逻辑上是一致的 不同的count用法 count(*)、count(主键id)和count(1) 都表示返回满足条件的结果集的总行数。count(字段），则表示返回满足条件的数据行里面，参数“字段”不为NULL的总个数 对于count(主键id)来说，InnoDB引擎会遍历整张表，把每一行的id值都取出来，返回给server层。server层拿到id后，判断是不可能为空的，就按行累加。 对于count(1)来说，InnoDB引擎遍历整张表，但不取值。server层对于返回的每一行，放一个数字“1”进去，判断是不可能为空的，按行累加。 count(1)执行得要比count(主键id)快。因为从引擎返回id会涉及到解析数据行，以及拷贝字段值的操作。 对于count(字段)来说： 如果这个“字段”是定义为not null的话，一行行地从记录里面读出这个字段，判断不能为null，按行累加； 如果这个“字段”定义允许为null，那么执行的时候，判断到有可能是null，还要把值取出来再判断一下，不是null才累加。 count(*) 是例外，并不会把全部字段取出来，而是专门做了优化，不取值。count(*)肯定不是null，按行累加。 结论是：按照效率排序的话， count(字段) &lt; count(主键id) &lt; count(1) ≈ count(*)，所以尽量使用count(*)。 orderby是怎么工作的 假设有SQL语句 select city,name,age from t where city=&#x27;杭州&#x27; order by name limit 1000; 全字段排序 如果要排序的数据量小于sort_buffer_size，排序就在内存中完成，否则外部排序（归并） rowid 排序 max_length_for_sort_data 是MySQL中专门控制用于排序的行数据的长度的参数，超过这个值就不会全字段排序，用rowid排序 全字段排序 VS rowid排序 如果内存够就用全字段排序，rowid排序回表多造成磁盘读，性能较差 并不是所有的order by语句都要排序的，如果建索引时就是有序的就不排 创建一个city和name的联合索引，查询过程如下： 还可以创建city、name和age的联合索引，这样就不用回表了 如何正确地显示随机消息 10000行记录如何随机选择3个 内存临时表 用order by rand()来实现这个逻辑 mysql&gt; select word from words order by rand() limit 3; R：随机数，W：单词，pos：rowid，对于有主键的表，rowid就是主键ID，没有主键就由系统生成 原表-&gt;内存临时表：扫描10000行 内存临时表-&gt;sort_buffer：扫描10000行 内存临时表-&gt;结果集：访问3行 order by rand()使用了内存临时表，内存临时表排序的时候使用了rowid排序方法 磁盘临时表 当内存临时表大小超过了tmp_table_size时，如果使用归并排序，内存临时表会转为磁盘临时表，如果使用优先队列排序（排序+limit操作），且维护的堆大小不超过sort_buffer_size，则不会转为磁盘临时表 随机排序方法 取得整个表的行数，记为C 取得 Y = floor(C * rand()) 再用 limit Y,1 取得一行 取多个随机行就重复多次这个算法 mysql&gt; select count(*) into @C from t; set @Y1 = floor(@C * rand()); set @Y2 = floor(@C * rand()); set @Y3 = floor(@C * rand()); select * from t limit @Y1，1； //在应用代码里面取Y1、Y2、Y3值，拼出SQL后执行 select * from t limit @Y2，1； select * from t limit @Y3，1； 或者优化一下，Y1，Y2，Y3从小到大排序，这样扫描的行数就是Y3 id1 = select * from t limit @Y1,1; id2 = select * from t where id &gt; id1 limit @Y2-@Y1,1; id3 = select * from t where id &gt; id2 limit @Y3 为什么逻辑相同的SQL语句性能差异巨大 对索引字段做函数操作，可能会破坏索引值的有序性，因此优化器决定放弃走树搜索功能，但不是放弃索引，优化器可以选择遍历索引 隐式类型转换可能会触发上面的规则1 隐式字符编码转换也可能触发上面的规则1 为什么只查一行的语句也执行这么慢 查询长时间不返回 等MDL锁。通过查询sys.schema_table_lock_waits，可以找出造成阻塞的process id，把这个连接用kill杀掉 等flush。可能情况是有一个flush tables命令被别的语句堵住了，然后它又堵住了查询语句，可以用show processlist 查出并杀掉阻塞flush的连接 等行锁。通过查询sys.innodb_lock_waits 杀掉对应连接 查询慢 查询字段没有索引，走了全表扫描 事务隔离级别为可重复读，当前事务看不到别的事务的修改，但是别的事务执行了多次修改，当前事务在查询时要根据undo log查询到应该看到的值 幻读 幻读：一个事务在前后两次查询同一个范围时，后一次查询看到了前一次查询没有看到的行 在可重复读隔离级别下，普通的查询时快照读，是不会看到别的事务插入的数据的。因此，幻读在“当前读”下才会出现 幻读专指“新插入的行” 幻读的问题 语义被破坏 session A在T1时刻声明了，“我要把所有d=5的行锁住，不准别的事务进行读写操作”。session B和C破坏了这个语义 数据不一致。根据binlog克隆的库与主库不一致，原因是即使给所有记录都加上锁，新记录还是没上锁 解决幻读 间隙锁：锁住两行之间的间隙 在行扫描过程中，不仅给行加行锁，还给行间的间隙上锁 跟间隙锁存在冲突关系的，是“往这个间隙中插入一个记录”这个操作。间隙锁之间都不存在冲突关系。 间隙锁和行锁合称next-key lock，左开右闭 间隙锁的引入，可能会导致同样的语句锁住更大的范围，影响并发度。 间隙锁只在可重复读隔离级别下才会生效 为什么只改一行的语句，锁这么多 加锁规则（可重复读隔离级别）： 原则1：加锁的基本单位是next-key lock 原则2：查找过程中访问到的对象才会加锁 优化1：索引上的等值查询，给唯一索引加锁的时候，next-key lock退化为行锁 优化2：索引上的等值查询，向右遍历时且最后一个值不满足等值条件的时候，next-key lock退化为间隙锁 一个bug：唯一索引上的范围查询会访问到不满足条件的第一个值为止 假设有如下SQL语句 CREATE TABLE `t` ( `id` int(11) NOT NULL, `c` int(11) DEFAULT NULL, `d` int(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `c` (`c`) ) ENGINE=InnoDB; insert into t values(0,0,0),(5,5,5), (10,10,10),(15,15,15),(20,20,20),(25,25,25); 案例一：等值查询间隙锁 由于表t中没有id=7的记录 根据原则1，加锁单位是next-key lock，session A加锁范围就是(5,10]； 同时根据优化2，这是一个等值查询(id=7)，而id=10不满足查询条件，next-key lock退化成间隙锁，因此最终加锁的范围是(5,10)。 所以，session B要往这个间隙里面插入id=8的记录会被锁住，但是session C修改id=10这行是可以的。 案例二：非唯一索引等值锁 这里session A要给索引c上c=5的这一行加上读锁。 根据原则1，加锁单位是next-key lock，因此会给(0,5]加上next-key lock 要注意c是普通索引，因此仅访问c=5这一条记录是不能马上停下来的，需要向右遍历，查到c=10才放弃。根据原则2，访问到的都要加锁，因此要给(5,10]加next-key lock。 但是同时这个符合优化2：等值判断，向右遍历，最后一个值不满足c=5这个等值条件，因此退化成间隙锁(5,10) 根据原则2 ，只有访问到的对象才会加锁，这个查询使用覆盖索引，并不需要访问主键索引，所以主键索引上没有加任何锁，这就是为什么session B的update语句可以执行完成。 但session C要插入一个(7,7,7)的记录，就会被session A的间隙锁(5,10)锁住。 需要注意，在这个例子中，lock in share mode只锁覆盖索引，但是如果是for update就不一样了。 执行 for update时，系统会认为你接下来要更新数据，因此会顺便给主键索引上满足条件的行加上行锁。 这个例子说明，锁是加在索引上的；同时，它给我们的指导是，如果你要用lock in share mode来给行加读锁避免数据被更新的话，就必须得绕过覆盖索引的优化，在查询字段中加入索引中不存在的字段。比如，将session A的查询语句改成select d from t where c=5 lock in share mode 案例三：主键索引范围锁 mysql&gt; select * from t where id=10 for update; mysql&gt; select * from t where id&gt;=10 and id&lt;11 for update; 这两条语句在逻辑上是等价的，但是加锁规则不一样 开始执行的时候，要找到第一个id=10的行，因此本该是next-key lock(5,10]。 根据优化1， 主键id上的等值条件，退化成行锁，只加了id=10这一行的行锁 范围查找就往后继续找，找到id=15这一行停下来，因此需要加next-key lock(10,15] 所以，session A这时候锁的范围就是主键索引上，行锁id=10和next-key lock(10,15] 需要注意一点，首次session A定位查找id=10的行的时候，是当做等值查询来判断的，而向右扫描到id=15的时候，用的是范围查询判断 案例四：非唯一索引范围锁 这次session A用字段c来判断，加锁规则跟主键索引范围锁的唯一不同是：在第一次用c=10定位记录的时候，索引c上加了(5,10]这个next-key lock后，由于索引c是非唯一索引，没有优化规则，也就是说不会蜕变为行锁，因此最终sesion A加的锁是，索引c上的(5,10] 和(10,15] 这两个next-key lock 案例五：唯一索引范围锁bug session A是一个范围查询，按照原则1的话，应该是索引id上只加(10,15]这个next-key lock，并且因为id是唯一键，所以循环判断到id=15这一行就应该停止了。 但是实现上，InnoDB会往前扫描到第一个不满足条件的行为止，也就是id=20。而且由于这是个范围扫描，因此索引id上的(15,20]这个next-key lock也会被锁上。 案例六：非唯一索引上存在“等值”的例子 现在插入一条新记录 mysql&gt; insert into t values(30,10,30); delete语句加锁的逻辑和 select … for update是类似的，session A在遍历的时候，先访问第一个c=10的记录。同样地，根据原则1，这里加的是(c=5,id=5)到(c=10,id=10)这个next-key lock。 然后，session A向右查找，直到碰到(c=15,id=15)这一行，循环才结束。根据优化2，这是一个等值查询，向右查找到了不满足条件的行，所以会退化成(c=10,id=10) 到 (c=15,id=15)的间隙锁。 也就是说，这个delete语句在索引c上的加锁范围，就是下图中蓝色区域覆盖的部分。蓝色区域左右两边都是虚线，表示开区间，即(c=5,id=5)和(c=15,id=15)这两行上都没有锁。 案例七：limit 语句加锁 session A的delete语句加了 limit 2。你知道表t里c=10的记录其实只有两条，因此加不加limit 2，删除的效果都是一样的，但是加锁的效果却不同。可以看到，session B的insert语句执行通过了，跟案例六的结果不同。这是因为，案例七里的delete语句明确加了limit 2的限制，因此在遍历到(c=10, id=30)这一行之后，满足条件的语句已经有两条，循环就结束了。 因此，索引c上的加锁范围就变成了从（c=5,id=5)到（c=10,id=30)这个前开后闭区间，如下图所示： 这个例子对我们实践的指导意义就是，在删除数据的时候尽量加limit。这样不仅可以控制删除数据的条数，让操作更安全，还可以减小加锁的范围。 案例八：一个死锁的例子 本案例目的是说明：next-key lock 实际上是间隙锁和行锁加起来的结果 session A 启动事务后执行查询语句加lock in share mode，在索引c上加了next-key lock(5,10] 和间隙锁(10,15)； session B 的update语句也要在索引c上加next-key lock(5,10] ，进入锁等待； 然后session A要再插入(8,8,8)这一行，被session B的间隙锁锁住。由于出现了死锁，InnoDB让session B回滚 你可能会问，session B的next-key lock不是还没申请成功吗？ 其实是这样的，session B的“加next-key lock(5,10] ”操作，实际上分成了两步，先是加(5,10)的间隙锁，加锁成功；然后加c=10的行锁，这时候才被锁住的。 也就是说，我们在分析加锁规则的时候可以用next-key lock来分析。但是要知道，具体执行的时候，是要分成间隙锁和行锁两段来执行的。 “饮鸩止渴”提高性能的方法 短连接风暴 短连接模式就是连接到数据库后，执行很少的SQL语句就断开，下次需要的时候再重连，在业务高峰期，会出现连接数暴涨的情况 两种有损业务的解决方法： 处理掉占着连接但是不工作的线程。优先断开事务外空闲太久的连接 减少连接过程的损耗。关闭权限验证 慢查询性能问题 引发慢查询的情况有三种： 索引没有设计好。最高效的解决方法是直接alter table建索引 SQL语句没有写好，导致没用上索引。解决方法是使用query_rewrite重写SQL语句 MySQL选错了索引。应急方案是给语句加上force index或者使用query_rewrite重写语句加上force index 出现情况最多的是前两种，通过下面过程可以预先发现和避免 上线前，在测试环境，把慢查询日志（slow log）打开，并且把long_query_time设置成0，确保每个语句都会被记录入慢查询日志 在测试表里插入模拟线上的数据，做一遍回归测试 观察慢查询日志里每类语句的输出，特别留意Rows_examined字段是否与预期一致 QPS突增问题 业务bug导致。可以把这个功能的SQL从白名单去掉 如果新功能使用的是单独的数据库用户，可以用管理员账号把这个用户删掉，然后断开连接 用query_rewrite把压力最大的SQL语句直接重写成&quot;select 1&quot;返回 方法3存在两个副作用： 如果别的功能也用到了这个SQL语句就会误伤 该语句可能是业务逻辑的一部分，导致业务逻辑一起失败 方法3是优先级最低的方法。方法1和2依赖于规范的运维体系：虚拟化、白名单机制、业务账号分离 MySQL是怎么保证数据不丢的 只要redo log和binlog保证持久化到磁盘，就能确保MySQL异常重启后，数据可以恢复 binlog的写入机制 binlog的写入逻辑：事务执行过程中，先把日志写到binlog cache，事务提交的时候，再把binlog cache写到binlog文件中 一个事务的binlog是不能被拆开的，因此不论这个事务多大，也要确保一次性写入 系统给binlog cache分配了一片内存，每个线程一个，参数binlog_cache_size用于控制单个线程内binlog cache所占内存的大小。如果超过了这个参数规定的大小，就要暂存到磁盘 事务提交的时候，执行器把binlog cache里的完整事务写入到binlog中，并清空binlog cache 图中的write，指的是把日志写入到文件系统的page cache，并没有把数据持久化到磁盘，所以速度比较快 图中的fsync，才是将数据持久化到磁盘的操作。一般情况下，我们认为fsync才占磁盘的IOPS write和fsync的时机由参数sync_binlog控制： sync_binlog=0，表示每次提交事务都只write，不fsync sync_binlog=1，表示每次提交事务都会fsync sync_binlog=N(N&gt;1)，表示每次提交事务都write，但累积N个事务后才fsync sync_binlog设置成N可以改善IO瓶颈场景的性能，但对应的风险是：如果主机发生异常重启，会丢失最近N个事务的binlog redo log的写入机制 事务执行过程中，生成的redo log要先写到redo log buffer，但不是每次生成后都要直接持久化到磁盘，因为事务没提交，日志丢了也不会有损失。 但是也有可能事务没有提交，redo log buffer 中的部分日志持久化到了磁盘。下图是redo log的三种状态 日志写到redo log buffer是很快的，write到page cache也快，但是持久化到磁盘就很慢。 InnoDB提供了innodb_flush_log_at_trx_commit参数来控制redo log的写入策略： 设置为0表示每次事务提交时都只是把redo log 留在redo log buffer中 设置为1表示每次事务提交时都将redo log直接持久化到磁盘 设置为2表示每次事务提交时都只是把redo log写到page cache InnoDB有个后台线程，每隔1秒，就会把redo log buffer中的日志，调用write写到文件系统的page cache，然后调用fsync持久化到磁盘 注意，事务执行过程中的redo log也是直接写在redo log buffer中的，这些redo log也会被后台线程一起持久化到磁盘。也就是说，一个没有提交的事务的redo log，也可能已经持久化到磁盘 除了后台线程每秒一次的轮询操作外，还有两个场景会让没提交的事务的redo log写入到磁盘 redo log buffer占用的空间即将达到 innodb_log_buffer_size一半的时候，后台线程会主动write到page cache 并行的事务提交的时候，顺带将这个事务的redo log buffer持久化到磁盘 两阶段提交的过程，时序上redo log先prepare，再写binlog，最后再把redo log commit 如果innodb_flush_log_at_trx_commit设置为1，那么redo log在prepare阶段就要持久化一次，因为有一个崩溃恢复逻辑是prepare的redo log + 完整的binlog 每秒一次后台轮询刷盘，再加上崩溃恢复这个逻辑，InnoDB就认为redo log在commit的时候就不需要fsync了，只会write到文件系统的page cache中就够了 通常我们说MySQL的“双1”配置，指的就是sync_binlog和innodb_flush_log_at_trx_commit都设置成 1。也就是说，一个事务完整提交前，需要等待两次刷盘，一次是redo log（prepare 阶段），一次是binlog redo log组提交 日志逻辑序列号LSN：LSN单调递增，用来对应redo log的一个个写入点。每次写入长度为length的redo log，LSN的值就会加上length 如下图所示，是三个并发事务在prepare阶段，都写完redo log buffer，持久化到磁盘的过程中 从图中可以看到， trx1是第一个到达的，会被选为这组的leader 等trx1要开始写盘的时候，这个组里面已经有3个事务了，这时候LSN也变成了160 trx1去写盘的时候，带的就是LSN=160，因此等trx1返回时，所有LSN小于等于160的redo log，都已经被持久化到磁盘 这时候trx2和trx3就可以直接返回了 所以，一次组提交里面，组员越多，节约磁盘IOPS的效果越好。但如果只有单线程压测，那就只能老老实实地一个事务对应一次持久化操作了。 在并发更新场景下，第一个事务写完redo log buffer以后，接下来这个fsync越晚调用，组员可能越多，节约IOPS的效果就越好。 MySQL为了让组提交的效果更好，细化了两阶段提及的顺序，让redo log的fsync往后拖 上图的顺序说明binlog也可以组提交，但是通常情况下步骤3会执行得很快，所以能集合到一起持久化的binlog比较少。可以通过设置binlog_group_commit_sync_delay 和 binlog_group_commit_sync_no_delay_count来提升binlog组提交的效果 性能瓶颈在IO的提升方法 设置 binlog_group_commit_sync_delay 和 binlog_group_commit_sync_no_delay_count参数，减少binlog的写盘次数。这个方法是基于“额外的故意等待”来实现的，因此可能会增加语句的响应时间，但没有丢失数据的风险。 将sync_binlog 设置为大于1的值（比较常见是100~1000）。这样做的风险是，主机掉电时会丢binlog日志。 将innodb_flush_log_at_trx_commit设置为2。这样做的风险是，主机掉电的时候会丢数据。 MySQL是怎么保证主备一致的 MySQL的主备一致依赖于binlog MySQL主备的基本原理 主备切换流程 客户端的读写是直接访问主库，备库同步主库的更新，与主库保持一致。虽然备库不会被客户端访问，但仍推荐设置成只读模式，因为： 有时候一些运营类的查询语句会放到备库上去查，设置为只读可以防止误操作 防止切换逻辑有bug，比如切换过程中出现双写，造成主备不一致 可以用readonly状态来判断节点的角色 备库的只读对超级权限用户是无效的，用于同步更新的线程拥有超级权限 同步流程 主库的更新语句同步到备库的完成流程图如下 备库B跟主库A之间维持了一个长连接。主库A内部有一个线程，专门用于服务备库B的这个长连接。一个事务日志同步的完整过程如下： 在备库B上通过change master命令，设置主库A的IP、端口、用户名、密码，以及要从哪个位置开始请求binlog，这个位置包含文件名和日志偏移量 在备库B上执行start slave命令，这时候备库会启动两个线程，就是图中的io_thread和sql_thread。其中io_thread负责与主库建立连接。 主库A校验完用户名、密码后，开始按照备库B传过来的位置，从本地读取binlog，发给B 备库B拿到binlog后，写到本地文件，称为中转日志（relay log） sql_thread读取中转日志，解析出日志里的命令，并执行 binlog的三种格式对比 binlog有三种格式，statement、row以及前两种格式的混合mixed 假设有如下表： mysql&gt; CREATE TABLE `t` ( `id` int(11) NOT NULL, `a` int(11) DEFAULT NULL, `t_modified` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP, PRIMARY KEY (`id`), KEY `a` (`a`), KEY `t_modified`(`t_modified`) ) ENGINE=InnoDB; insert into t values(1,1,&#x27;2018-11-13&#x27;); insert into t values(2,2,&#x27;2018-11-12&#x27;); insert into t values(3,3,&#x27;2018-11-11&#x27;); insert into t values(4,4,&#x27;2018-11-10&#x27;); insert into t values(5,5,&#x27;2018-11-09&#x27;); statement格式就是SQL语句原文 下图是该语句执行效果 statement格式下，delete 带 limit，很可能出现主备数据不一致的情况，比如上面的例子： 如果delete语句使用的是索引a，那么会根据索引a找到第一个满足条件的行，也就是说删除的是a=4这一行 但如果使用的是索引t_modified，那么删除的就是 t_modified='2018-11-09’ 也就是a=5这一行。 row格式binlog如下 row格式的binlog把SQL语句替换成了两个event：Table_map和Delete_rows Table_map event，用于说明接下来要操作的表是test库的表t; Delete_rows event，用于定义删除的行为。 借助mysqlbinlog工具查看详细的binlog 当binlog_format使用row格式的时候，binlog里面记录了真实删除行的主键id，这样binlog传到备库去的时候，就肯定会删除id=4的行，不会有主备删除不同行的问题。 mixed格式吸收了statement和row格式的优点，占用空间小，避免了数据不一致 但是现在binlog设置成row的场景更多，理由有很多，其中之一是恢复数据。 如果执行的是delete语句，row格式的binlog也会把被删掉的行的整行信息保存起来。所以，如果你在执行完一条delete语句以后，发现删错数据了，可以直接把binlog中记录的delete语句转成insert，把被错删的数据插入回去就可以恢复了 如果你是执行错了insert语句呢？那就更直接了。row格式下，insert语句的binlog里会记录所有的字段信息，这些信息可以用来精确定位刚刚被插入的那一行。这时，你直接把insert语句转成delete语句，删除掉这被误插入的一行数据就可以了。 如果执行的是update语句的话，binlog里面会记录修改前整行的数据和修改后的整行数据。所以，如果你误执行了update语句的话，只需要把这个event前后的两行信息对调一下，再去数据库里面执行，就能恢复这个更新操作了 循环复制问题 binlog的特性确保了主备一致性。实际生产上使用比较多的是双M结构 双M结构中，节点A和B之间总是互为主备关系，在切换的时候就不用再修改主备关系 循环复制指的是A节点更新完，把binlog发给B，B更新完又生成binlog发给了A，解决循环复制的方法如下： 规定两个库的server id必须不同，如果相同，则它们之间不能设定为主备关系 一个备库接到binlog并在重放的过程中，生成与原binlog的server id相同的新的binlog 每个库在收到从自己的主库发过来的日志后，先判断server id，如果跟自己的相同，表示这个日志是自己生成的，直接丢弃 因此，双M结构的日志执行流会变成这样： 从节点A更新的事务，binlog里面记得都是A的server id 传到节点B执行一次以后，节点B生成的binlog的server id也是A的server id 再传回给节点A，A判断到这个server id与自己的相同，就不会再处理这个日志。所以，死循环这里就断掉了 MySQL是怎么保证高可用的 正常情况下，只要主库执行更新生成的所有binlog，都可以传到备库并被正确地执行，备库就能达到跟主库一致的状态，这就是最终一致性。但是MySQL要提供高可用，只有最终一致性是不够的 主备延迟 与数据同步有关的时间点主要包括以下三个： T1：主库A执行完成一个事务，写入binlog T2：备库B接收完这个binlog T3：备库B执行完成这个事务 主备延迟：同一个事务，在备库执行完成的时间和主库执行完成的时间之间的差值，也就是T3-T1 seconds_behind_master表示备库延迟了多少秒 网络正常情况下，主备延迟的主要因素是T3-T2，直接表现是备库消费中转日志（relay log）的速度比主库生产binlog的速度慢 主备延迟的来源 备库的机器性能差。解决方法是对称部署 备库的压力大。有些统计查询语句只在备库上跑，导致备库压力大，解决方法是一主多从分担读的压力或者把binlog输送到Hadoop来提供统计查询能力 大事务。比如一次性用delete删除太多数据或者大表DDL 备库的并行复制能力 由于主备延迟的存在，所以在主备切换的时候，有不同的策略 可靠性优先策略 在双M结构下，主备切换流程如下： 判断备库B现在的seconds_behind_master，如果小于某个值（比如5秒）继续下一步，否则持续重试这一步； 把主库A改成只读状态，即把readonly设置为true； 判断备库B的seconds_behind_master的值，直到这个值变成0为止； 把备库B改成可读写状态，也就是把readonly 设置为false； 把业务请求切到备库B。 步骤2直到步骤5，主库A和备库B都处于readonly状态，系统不可用（不可写） 可用性优先策略 把上面策略里的步骤4和5放到最开始执行，代价是可能出现数据不一致的情况 一般情况下，可靠性优于可用性。在满足数据可靠性的前提下，MySQL高可用系统的可用性，是依赖于主备延迟的。延迟的时间越小，在主库故障的时候，服务恢复需要的时间就越短，可用性就越高。 备库为什么会延迟好几个小时 当备库执行日志的速度持续低于主库生成日志的速度，那这个延迟就有可能成了小时级别 MySQL 5.6之前，备库应用日志更新数据只能使用单线程，在主库并发高、TPS高时会出现严重的主备延迟问题 按表分发策略 按表分发事务的基本思路是，如果两个事务更新不同的表，它们就可以并行 worker线程维护一张执行队列里的事务涉及的表，key是“库名.表名”,value表示队列中有多少个事务修改这个表 事务在分发的时候，和所有worker的冲突关系有3种： 如果跟所有worker都不冲突，coordinator线程就会把这个事务分配给最空闲的woker; 如果跟多于一个worker冲突，coordinator线程就进入等待状态，直到和这个事务存在冲突关系的worker只剩下1个； 如果只跟一个worker冲突，coordinator线程就会把这个事务分配给这个存在冲突关系的worker。 按表分发方案在多个表负载均衡的场景效果很好。但是碰到热点表会退化成单线程复制 按行分发策略 要解决热点表的并行复制问题，就需要一个按行并行复制方案。核心思路是：如果两个事务没有更新相同的行，它们在备库上可以并行执行。这个模式要求binlog格式必须是row 判断事务和worker是否冲突，用的规则不是“修改同一个表”，而是“修改同一行”。worker维护的hash表的key是“库名+表名+唯一索引的名字+唯一索引的值” 按行分发策略比按表分发策略需要消耗更多的计算资源，这两种方案都有一样的约束条件： 要能够从binlog里面解析出表名、主键值和唯一索引的值。也就是说，主库的binlog格式必须是row； 表必须有主键； 不能有外键。表上如果有外键，级联更新的行不会记录在binlog中，这样冲突检测就不准确。 MySQL 5.6版本的并行复制策略 官方MySQL 5.6版本支持的并行复制的力度是按库并行。hash表的key是数据库名 相比于按表和按行分发，有两个优势： 构造hash值的时候很快，只需要库名；而且一个实例上DB数也不会很多，不会出现需要构造100万个项这种情况 不要求binlog的格式。因为statement格式的binlog也可以很容易拿到库名 MariaDB的并行复制策略 MariaDB的并行复制策略利用了redo log组提交优化的特性： 能够在同一组里提交的事务，一定不会修改同一行 主库上可以并行执行的事务，备库上也一定可以并行执行 这个策略的目标是“模拟主库的并行模式”，但它没有实现“真正的模拟主库并发度”这个目标。在主库上，一组事务在commit的时候，下一组事务是同时处于“执行中”状态的 MySQL 5.7的并行复制策略 由参数slave-parallel-type来控制并行复制策略： 配置为DATABASE，表示使用MySQL 5.6版本的按库并行策略； 配置为 LOGICAL_CLOCK，表示的就是优化过的类似MariaDB的策略 该优化的策略的思想是： 同时处于prepare状态的事务，在备库执行时是可以并行的； 处于prepare状态的事务，与处于commit状态的事务之间，在备库执行时也是可以并行的。 MySQL 5.7.22的并行复制策略 新增了一个并行复制策略，基于WRITESET的并行复制。 新增参数binlog-transaction-dependency-tracking，用来控制是否启用这个新策略： COMMIT_ORDER，表示的就是前面介绍的，根据同时进入prepare和commit来判断是否可以并行的策略。 WRITESET，表示的是对于事务涉及更新的每一行，计算出这一行的hash值，组成集合writeset。如果两个事务没有操作相同的行，也就是说它们的writeset没有交集，就可以并行 WRITESET_SESSION，是在WRITESET的基础上多了一个约束，即在主库上同一个线程先后执行的两个事务，在备库执行的时候，要保证相同的先后顺序。 该策略类似按行分发，但是有很大优势： writeset是在主库生成后直接写入到binlog里面的，这样在备库执行的时候，不需要解析binlog内容（event里的行数据），节省了很多计算量； 不需要把整个事务的binlog都扫一遍才能决定分发到哪个worker，更省内存； 由于备库的分发策略不依赖于binlog内容，所以binlog是statement格式也是可以的。 该方案对于“表上没主键”和“外键约束”的场景，也会暂时退化为单线程模型。 主库出问题了，从库怎么办 大多数互联网应用场景都是读多写少，要解决读性能问题，就要涉及：一主多从 图中，虚线箭头表示的是主备关系，也就是A和A’互为主备， 从库B、C、D指向的是主库A。一主多从的设置，一般用于读写分离，主库负责所有的写入和一部分读，其他的读请求则由从库分担。 下面讨论，在一主多从架构下，主库故障后的主备切换问题 相比于一主一备的切换流程，一主多从结构在切换完成后，A’会成为新的主库，从库B、C、D也要改接到A’。正是由于多了从库B、C、D重新指向的这个过程，所以主备切换的复杂性也相应增加了 基于位点的主备切换 节点B设置成节点A’ 的从库的时候，需要执行change master命令，必须设置主库的日志文件名和偏移量。A和A’的位点是不同的，从库B切换时需要先经过“找同步位点”这个逻辑 同步位点很难精确取到 取同步位点的方法如下： 等待新主库A’ 把中转日志（relay log）全部同步完成 在A’ 上执行show master status命令，得到当前A’ 上最新的 File 和 Position 取原主库A故障的时刻T 用mysqlbinlog工具解析A’的File，得到T时刻的位点 mysqlbinlog File --stop-datetime=T --start-datetime=T 图中，end_log_pos后面的值“123”，表示的就是A’这个实例，在T时刻写入新的binlog的位置，可以把这个值作为$master_log_pos ，用在节点B的change master命令里 这个值并不精确，从库B的同步线程可能会出错，解决方法如下： 通过sql_slave_skip_counter跳过出错事务 设置slave_skip_errors，跳过指定错误，通常设置成1032，1062，对应的错误是删除数据找不到行，插入数据唯一键冲突 GTID 前两种方式操作复杂，容易出错，MySQL 5.6 引入了GITD。 GTID全称是Global Transaction Identifier，也就是全局事务ID，是一个事务在提交的时候生成的，是这个事务的唯一标识，格式是： GTID=server_uuid:gno server_uuid是一个实例第一次启动时自动生成的，是一个全局唯一的值 gno是一个整数，初始值是1，每次提交事务的时候分配给这个事务，并加1 GTID有两种生成方式： 如果gtid_next=automatic，代表使用默认值。这时，MySQL就会把server_uuid:gno分配给这个事务。 a. 记录binlog的时候，先记录一行 SET @@SESSION.GTID_NEXT=‘server_uuid:gno’; b. 把这个GTID加入本实例的GTID集合 如果gtid_next是一个指定的GTID的值，比如通过set gtid_next='current_gtid’指定为current_gtid，那么就有两种可能： a. 如果current_gtid已经存在于实例的GTID集合中，接下来执行的这个事务会直接被系统忽略； b. 如果current_gtid没有存在于实例的GTID集合中，就将这个current_gtid分配给接下来要执行的事务，也就是说系统不需要给这个事务生成新的GTID，因此gno也不用加1 每个MySQL实例都维护了一个GTID集合，用来对应“这个实例执行过的所有事务” 当从库需要跳过某个事务时，在主库上查出GTID，在从库上提交空事务，把这个GTID加入到从库的GTID集合中 set gtid_next=&#x27;aaaaaaaa-cccc-dddd-eeee-ffffffffffff:10&#x27;; begin; commit; set gtid_next=automatic; start slave; 基于GTID的主备切换 切换命令指定master_auto_position=1表示这个主备关系使用的是GTID协议，不需要指定主库日志文件和偏移量 我们把A’ 的GTID集合记为set_a，实例B的GTID集合记为set_b，切换流程如下： 实例B指定主库A’，基于主备协议建立连接。 实例B把set_b发给主库A’ 实例A’算出set_a与set_b的差集，也就是所有存在于set_a，但是不存在于set_b的GITD的集合，判断A’本地是否包含了这个差集需要的所有binlog事务。 a. 如果不包含，表示A’已经把实例B需要的binlog给删掉了，直接返回错误； b. 如果确认全部包含，A’从自己的binlog文件里面，找出第一个不在set_b的事务，发给B； 之后就从这个事务开始，往后读文件，按顺序取binlog发给B去执行 GTID和在线DDL 假设，这两个互为主备关系的库还是实例X和实例Y，且当前主库是X，并且都打开了GTID模式。这时的主备切换流程可以变成下面这样： 在实例X上执行stop slave。 在实例Y上执行DDL语句。注意，这里并不需要关闭binlog。 执行完成后，查出这个DDL语句对应的GTID，并记为 server_uuid_of_Y:gno。 到实例X上执行以下语句序列： set GTID_NEXT=&quot;server_uuid_of_Y:gno&quot;; begin; commit; set gtid_next=automatic; start slave; 这样做的目的在于，既可以让实例Y的更新有binlog记录，同时也可以确保不会在实例X上执行这条更新。 接下来，执行完主备切换，然后照着上述流程再执行一遍即可 读写分离有哪些坑 由于主从可能存在延迟，客户端执行完一个更新事务后马上发起查询，如果查询选择的是从库的话，就有可能读到刚刚的事务更新之前的状态，这种现象称为“过期读” 过期读处理方案包括： 强制走主库方案 sleep方案 判断主备无延迟方案 配合semi-sync方案 等主库位点方案 等GTID方案 强制走主库方案 该方案将查询请求分为两类： 对于必须要拿到最新结果的请求，强制将其发到主库上。比如，在一个交易平台上，卖家发布商品以后，马上要返回主页面，看商品是否发布成功。那么，这个请求需要拿到最新的结果，就必须走主库 对于可以读到旧数据的请求，才将其发到从库上。在这个交易平台上，买家来逛商铺页面，就算晚几秒看到最新发布的商品，也是可以接受的。那么，这类请求就可以走从库 这个方案用的最多，但是问题在于存在“所有查询都不能是过期读”的需求，比如金融类业务，那就必须放弃读写分离，所有读写压力都在主库 下面讨论的是：可以支持读写分离的场景下，有哪些解决过期读的方案 Sleep方案 主库更新后，读从库之前先sleep一下。这个方案假设，大多数情况下主备延迟在1s之内 该方案可以解决类似Ajax场景下的过期读问题。例如卖家发布商品，直接将卖家输入的内容作为新商品显示出来，并不查从库。等待卖家刷新页面，相当于sleep了一段时间，解决了过期读问题 该方案存在的问题是不精确： 如果查询请求本来0.5s就可以在从库上拿到正确结果，也会等到1s 如果延迟超过1s，还是会出现过期读 判断主备无延迟方案 有三种方法： 每次从库执行查询请求前，先判断seconds_behind_master是否已经等于0。如果还不等于0 ，那就必须等到这个参数变为0才能执行查询请求。 对比位点。如果Master_Log_File和Relay_Master_Log_File、Read_Master_Log_Pos和Exec_Master_Log_Pos这两组值完全相同表示主备无延迟 对比GITD。Retrieved_Gtid_Set和Executed_Gtid_Set相同表示是主备无延迟 该方案比Sleep更准确，方法2和3比1准确，但是不能说精确。因为存在客户端已经收到提交确认，而备库还没收到日志的状态，因此备库认为主备无延迟，从而发生过期读 配合semi-sync 为解决上面的问题，引入semi-sync replication: semi-sync做了这样的设计： 事务提交的时候，主库把binlog发给从库 从库收到binlog以后，发回给主库一个ack 主库收到ack以后，才能给客户端返回“事务完成”的确认 开启semi-sync，就表示所有给客户端发送过确认的事务，都确保备库已经收到了这个日志 semi-sync+判断主备无延迟方案存在两个问题： 一主多从情况下，因为主库只要收到一个从库的ack就给客户端返回确认，其它未响应ack的从库可能会发生过期读问题 在业务高峰期，主库的位点或者GITD集合更新很快，这种情况下，可能出现从库一直存在主备延迟导致客户端查询一直等待 等主库位点方案 该方案解决了前面两个问题 命令： select master_pos_wait(file, pos[, timeout]); 这条命令的逻辑如下： 它是在从库执行的 参数file和pos指的是主库上的文件名和位置 timeout可选，设置为正整数N表示这个函数最多等待N秒 为了解决前面两个问题，流程如下： trx1事务更新完成后，马上执行show master status得到当前主库执行到的File和Position； 选定一个从库执行查询语句； 在从库上执行select master_pos_wait(File, Position, 1)； 如果返回值是&gt;=0的整数，则在这个从库执行查询语句； 否则，到主库执行查询语句。 GTID方案 等GTID也可以解决前面两个问题 流程如下： trx1事务更新完成后，从返回包直接获取这个事务的GTID，记为gtid1； 选定一个从库执行查询语句； 在从库上执行 select wait_for_executed_gtid_set(gtid1, 1)； 如果返回值是0，则在这个从库执行查询语句； 否则，到主库执行查询语句。 如何判断一个数据库是不是出问题了 select 1 判断 select 1 成功返回只能说明数据库进程还在，不能说明没问题 并发连接：通过show precesslist查询连接数，连接数可以远大于并发查询数量 并发查询：“当前正在执行”的语句的数量 线程进入锁等待后，并发线程的计数会减一，即进入锁等待的线程不吃CPU 假如设置并发线程数是3，下面的情况是A、B、C在并发查询，D先select 1不占并发线程数所以能正常返回，但实际上已经不能正常查询了 查表判断 为了能够检测InnoDB并发线程数过多导致的系统不可用情况，我们需要找一个访问InnoDB的场景。一般的做法是，在系统库（mysql库）里创建一个表，比如命名为health_check，里面只放一行数据，然后定期执行： mysql&gt; select * from mysql.health_check; 这种方法在磁盘空间满了就无效。因为更新事务要写binlog，而一旦binlog所在磁盘满了，那么所有更新语句都会堵住，但是系统仍然可以读数据 更新判断 我们把查询换成更新来作为监控语句。常见做法是放一个timestamp字段表示最后一次检测时间，这条更新语句类似于： mysql&gt; update mysql.health_check set t_modified=now(); 主库和备库用同样的更新语句可能会出现行冲突，导致主备同步停止，所以mysql.health_check表不能只有一行数据 mysql&gt; CREATE TABLE `health_check` ( `id` int(11) NOT NULL, `t_modified` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP, PRIMARY KEY (`id`) ) ENGINE=InnoDB; /* 检测命令 */ insert into mysql.health_check(id, t_modified) values (@@server_id, now()) on duplicate key update t_modified=now(); MySQL规定主备的server_id必须不同，这样主备各自的检测命令就不会冲突 更新判断存在的问题是“判定慢”。因为更新语句在IO负载已经100%时仍然可能在超时前完成。检测系统看到update命令没有超时，就认为“系统正常”，但实际上正常SQL语句已经很慢了 内部统计 前面几种方法都是外部检测，外部检测都需要定时轮询，所以系统可能已经出问题了，但是却需要等到下一个检测发起执行语句的时候，才有可能发现问题，导致主备切换慢 针对磁盘利用率问题，MySQL 5.6 在file_summary_by_event_name表里统计了每次IO请求的时间，可以设置阈值作为检测逻辑 误删数据怎么办？ 误删分为以下几类： 使用delete误删数据行 使用drop table或者truncate table误删数据表 使用drop database误删数据库 使用rm误删整个MySQL实例 误删行 可以使用Flashback恢复，原理是修改binlog的内容，拿回原库重放。使用这个方案的前提是确保binlog_format=row 和 binlog_row_image=FULL 建议在备库上执行，再恢复回主库 误删库/表 这种情况要求线上有定期的全量备份，并且实时备份binlog 假如有人中午12点误删了一个库，恢复数据的流程如下： 取最近一次全量备份，假设这个库是一天一备，上次备份是当天0点； 用备份恢复出一个临时库； 从日志备份里面，取出凌晨0点之后的日志； 把这些日志，除了误删除数据的语句外，全部应用到临时库 如果临时库有多个数据库，在使用mysqlbinlog时可以加上-database指定误删表所在库，加速数据恢复 在应用日志的时候，需要跳过12点误操作的那个语句的binlog： 如果原实例没有使用GTID模式，只能在应用到包含12点的binlog文件的时候，先用–stop-position参数执行到误操作之前的日志，然后再用–start-position从误操作之后的日志继续执行； 如果实例使用了GTID模式，就方便多了。假设误操作命令的GTID是gtid1，那么只需要执行set gtid_next=gtid1;begin;commit; 先把这个GTID加到临时实例的GTID集合，之后按顺序执行binlog的时候，就会自动跳过误操作的语句 即使这样，使用mysqlbinlog方法恢复数据仍然不快，因为： mysqlbinlog并不能指定只解析一个表的日志 用mysqlbinlog解析出日志应用，应用日志的过程就只能是单线程 一种加速方法是，在用备份恢复出临时实例之后，将这个临时实例设置成线上备库的从库，这样： 在start slave之前，先通过执行 change replication filter replicate_do_table = (tbl_name) 命令，就可以让临时库只同步误操作的表； 这样做也可以用上并行复制技术，来加速整个数据恢复过程。 延迟复制备库 上面的方案存在“恢复时间不可控问题”，比如一周一备份，第6天误操作，那就需要恢复6天的日志，这个恢复时间可能按天计算 一般的主备复制结构存在的问题是，如果主库上有个表被误删了，这个命令很快也会被发给所有从库，进而导致所有从库的数据表也都一起被误删了。 延迟复制的备库是一种特殊的备库，通过 CHANGE MASTER TO MASTER_DELAY = N命令，可以指定这个备库持续保持跟主库有N秒的延迟。 比如你把N设置为3600，这就代表了如果主库上有数据被误删了，并且在1小时内发现了这个误操作命令，这个命令就还没有在这个延迟复制的备库执行。这时候到这个备库上执行stop slave，再通过之前介绍的方法，跳过误操作命令，就可以恢复出需要的数据。 这样的话，你就随时可以得到一个，只需要最多再追1小时，就可以恢复出数据的临时实例，也就缩短了整个数据恢复需要的时间 预防误删库/表的方法 账号分离，避免写错命令 只给业务开发同学DML权限，而不给truncate/drop权限。而如果业务开发人员有DDL需求的话，也可以通过开发管理系统得到支持 即使是DBA团队成员，日常也都规定只使用只读账号，必要的时候才使用有更新权限的账号 指定操作规范，避免写错要删除的表名 删除数据表之前，必须先对表做改名操作。然后，观察一段时间，确保对业务无影响以后再删除这张表。 改表名的时候，要求给表名加固定的后缀（比如加_to_be_deleted)，然后删除表的动作必须通过管理系统执行。并且，管理系删除表的时候，只能删除固定后缀的表。 rm删除数据 对于有高可用机制的MySQL集群，最不怕rm。只要整个集群没被删掉，HA系统会选出新主库，保证整个集群正常工作。因此备库尽量跨机房、跨城市 为什么还有kill不掉的语句 MySQL有两个kill命令： kill query+线程id，表示终止这个线程正在执行的语句 kill connection+线程id，connection可缺省，表示断开这个线程的连接，如果有语句正在执行，先停止语句 收到kill后，线程做什么 kill并不是马上停止，而是告诉线程，这条语句已经不需要继续执行了，可以开始“执行停止的逻辑了” 处理kill query命令的线程做了两件事： 把目标线程的运行状态改成THD::KILL_QUERY(将变量killed赋值为THD::KILL_QUERY)； 给目标线程发一个信号，通知目标线程处理THD::KILL_QUERY状态。如果目标线程处于等待状态，必须是一个可以被唤醒的等待，否则不会执行到判断线程状态的“埋点” 处理kill connection命令的线程做了两件事： 把目标线程状态设置为KILL_CONNECTION 关闭目标线程的网络连接 kill无效的两类情况： 线程没有执行到判断线程状态的逻辑。这种情况有innodb_thread_concurrency 不够用，IO压力过大 终止逻辑耗时较长。这种情况有kill超大事务、回滚大查询、kill最后阶段的DDL命令 处于Killed状态的线程，你可以通过影响系统环境来让状态尽早结束。比如并发度不够导致线程没有执行到判断线程状态的逻辑，就增大innodb_thread_concurrency。除此之外，做不了什么，只能等流程自己结束 大查询会不会打爆内存 主机内存小于表的大小，全表扫描不会用光主机内存，否则逻辑备份早就挂了 全表扫描对server层的影响 假设对200G的表 db1.t 全表扫描，需要保留结果到客户端，会使用类似命令： mysql -h$host -P$port -u$user -p$pwd -e &quot;select * from db1.t&quot; &gt; $target_file 服务端不保存完整的查询结果集，取数据和发数据的流程是这样的： 获取一行，写到net_buffer中 重复获取行，直到net_buffer写满，调用网络接口发出去 如果发送成功，就清空net_buffer，然后继续取下一行，并写入net_buffer 如果发送函数返回EAGAIN或WSAEWOULDBLOCK，就表示本地网络栈（socket send buffer）写满了，进入等待。直到网络栈重新可写，再继续发送 从这个流程可以看出： 一个查询在发送过程中，占用的MySQL内部的内存最大就是net_buffer_length这么大，并不会达到200G； socket send buffer 也不可能达到200G（默认定义/proc/sys/net/core/wmem_default），如果socket send buffer被写满，就会暂停读数据的流程。 全表扫描对InnoDB层的影响 数据页在Buffer Pool（BP）中管理，BP可以起到加速查询的作用，作用效果依赖于一个重要指标：内存命中率 BP的大小由参数 innodb_buffer_pool_size 确定，一般设置成可用物理内存的60%~80% 如果BP满了，要从磁盘读入一个数据页，就要淘汰一个旧数据页，InnoDB内存管理用的是改进后的最近最少使用（LRU）算法 上图head指向刚刚被访问过的数据页 基本的LRU算法在遇到全表扫描历史数据表时，会出现内存命中率急剧下降，磁盘压力增加，SQL响应变慢的情况 InnoDB按照 5:3 将LRU链表分成young区和old区，LRU_old指向old区域第一个位置，即整个链表的5/8处 改进后的LRU算法如下： 访问young区域的数据页，和之前的算法一样，移动到链表头 访问不在链表中的数据页，淘汰tail指向的最后一页，在LRU_old处插入新数据页 访问old区域的数据页，若这个数据页在LRU链表中存在时间超过1s，就移动到链表头部，否则不动，1s由参数innodb_old_blocks_time控制 这个策略在扫描大表时不会对young区域造成影响，保证BP响应正常业务的查询命中率 可不可以使用join 先创建两个DDL一样的表 CREATE TABLE `t2` ( `id` int(11) NOT NULL, `a` int(11) DEFAULT NULL, `b` int(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `a` (`a`) ) ENGINE=InnoDB; /*省略给t2插入1000行数据*/ create table t1 like t2; insert into t1 (select * from t2 where id&lt;=100) Index Nested-Loop Join 有如下语句： select * from t1 straight_join t2 on (t1.a=t2.a); straight_join让MySQL使用固定的连接方式执行查询，这里t1是驱动表，t2是被驱动表 这个语句的执行流程如下： 从表t1中读入一行数据R 从数据行R中，取出a字段到表t2里去查 取出表t2中满足条件的行，跟R组成一行，作为结果集的一部分 重复执行步骤1到3，直到表t1的末尾循环结束 在形式上，这个过程和我们写程序时的嵌套查询类似，并且可以用上被驱动表的索引，所以我们称之为“Index Nested-Loop Join”，简称NLJ 在流程里： 对驱动表t1做了全表扫描，这个过程需要扫描100行 对于每一行R，根据a字段去表t2查找，走的是树搜索过程。由于我们构造的数据是一一对应的，因此每次搜索都只扫描一行，也就是总共扫描100行 所以，整个执行流程，总扫描行数是200 如果不用join，上面的连接需求，用单表查询实现的话，扫描行数一样，但是交互次数多，而且客户端要自己拼接SQL语句和结果，因此不如直接join 假设驱动表行数是N。被驱动表行数是M，被驱动表查一行数据要先走索引a，再走主键索引，因此时间复杂度是2∗log2M2*log_2 M2∗log2​M。驱动表要扫描N行，然后每行都要去被驱动表上匹配，所以整个执行过程复杂度是 N+N∗2∗log2MN+N*2*log_2 MN+N∗2∗log2​M。显然N影响更大，因此让小表做驱动表 Simple Nested-Loop Join 现在语句改成如下： select * from t1 straight_join t2 on (t1.a=t2.b); 由于t2的字段b没有索引，每次到t2去匹配都要做全表扫描，因此这个查询要扫描100*1000=10万行。 Block Nested-Loop Join 当被驱动表上没有可用索引，MySQL使用的算法流程如下： 把表t1的数据读入线程内存join_buffer中，由于我们这个语句中写的是select *，因此是把整个表t1放入了内存； 扫描表t2，把表t2中的每一行取出来，跟join_buffer中的数据做对比，满足join条件的，作为结果集的一部分返回。 该算法和Simple Nested-Loop Join算法扫描的行数一样多，但该算法是内存操作，速度更快。碰到大表不能放入join_buffer的情况就分多次放 总结一下： 第一个问题：能不能使用join语句？ 如果可以使用Index Nested-Loop Join算法，也就是说可以用上被驱动表上的索引，其实是没问题的； 如果使用Block Nested-Loop Join算法，扫描行数就会过多。尤其是在大表上的join操作，这样可能要扫描被驱动表很多次，会占用大量的系统资源。所以这种join尽量不要用 所以在判断要不要使用join语句时，就是看explain结果里面，Extra字段里面有没有出现“Block Nested Loop”字样 第二个问题：如果要使用join，应该选择大表做驱动表还是选择小表做驱动表？ 总是使用小表做驱动表。更准确地说，在决定哪个表做驱动表的时候，应该是两个表按照各自的条件过滤，过滤完成之后，计算参与join的各个字段的总数据量，数据量小的那个表，就是“小表”，应该作为驱动表 join语句怎么优化 创建两个表t1、t2(id int primary key, a int, b int, index(a))。给表t1插入1000行数据，每一行a=1001-id，即字段a是逆序的。给表t2插入100万行数据 Multi-Range Read优化 现在有SQL语句： select * from t1 where a&gt;=1 and a&lt;=100; MRR优化的设计思路是：大多数的数据都是按照主键递增顺序插入的，所以按照主键的递增顺序查询的话，对磁盘的读比较接近顺序读，能够提升读性能。使用MRR的语句的执行流程如下： 根据索引a，定位到满足条件的记录，将id值放入read_rnd_buffer中; 将read_rnd_buffer中的id进行递增排序 排序后的id数组，依次到主键id索引中查记录，并作为结果返回。 MRR能够提升性能的核心在于，这条查询语句在索引a上做的是一个范围查询（也就是说，这是一个多值查询），可以得到足够多的主键id。这样通过排序以后，再去主键索引查数据，才能体现出“顺序性”的优势 Batched Key Access MySQL 5.6 引入Batched Key Acess(BKA)算法，这个算法是对NLJ算法的优化 NLJ算法执行的逻辑是：从驱动表t1，一行行地取出a的值，再到被驱动表t2去做join。也就是说，对于表t2来说，每次都是匹配一个值。这时，MRR的优势就用不上了 优化思路就是，从表t1里一次性多拿出些行，一起传给表t2。取出的数据先放到join_buffer BNL算法的性能问题 可能会多次扫描被驱动表，占用磁盘IO资源； 判断join条件需要执行M*N次对比（M、N分别是两张表的行数），如果是大表就会占用非常多的CPU资源； 可能会导致Buffer Pool的热数据被淘汰，影响内存命中率。 如果explain命令发现优化器使用BNL算法。我们就需要优化，常见做法是，给被驱动表的join字段加上索引，把BNL算法转成BKA算法 BNL转BKA select * from t1 join t2 on (t1.b=t2.b) where t2.b&gt;=1 and t2.b&lt;=2000; 在索引创建资源开销大情况下，可以考虑使用临时表： 把表t2中满足条件的数据放在临时表tmp_t中； 为了让join使用BKA算法，给临时表tmp_t的字段b加上索引； 让表t1和tmp_t做join操作 对应的SQL语句： create temporary table temp_t(id int primary key, a int, b int, index(b))engine=innodb; insert into temp_t select * from t2 where b&gt;=1 and b&lt;=2000; select * from t1 join temp_t on (t1.b=temp_t.b); 扩展-hash join BNL的问题是join_buffer里面维护的是一个无序数组，如果是一个hash表，可以大幅减少判断次数。可以在业务端实现这个优化： select * from t1;取得表t1的全部1000行数据，在业务端存入一个hash结构 select * from t2 where b&gt;=1 and b&lt;=2000; 获取表t2中满足条件的2000行数据。 把这2000行数据，一行一行地取到业务端，到hash结构的数据表中寻找匹配的数据。满足匹配的条件的这行数据，就作为结果集的一行 为什么临时表可以重名 内存表和临时表的区别： 内存表，指的是使用Memory引擎的表，建表语法是create table … engine=memory。这种表的数据都保存在内存里，系统重启的时候会被清空，但是表结构还在。除了这两个特性看上去比较“奇怪”外，从其他的特征上看，它就是一个正常的表 临时表，可以使用各种引擎类型 。如果是使用InnoDB引擎或者MyISAM引擎的临时表，写数据的时候是写到磁盘上的。当然，临时表也可以使用Memory引擎 临时表的特性 临时表在使用上有以下几个特点： 建表语法是create temporary table …。 一个临时表只能被创建它的session访问，对其他线程不可见。所以，图中session A创建的临时表t，对于session B就是不可见的。 临时表可以与普通表同名 session A内有同名的临时表和普通表的时候，show create语句，以及增删改查语句访问的是临时表 show tables命令不显示临时表 临时表的应用 由于不用担心线程之间的重名冲突，临时表经常会被用在复杂查询的优化过程中。其中，分库分表系统的跨库查询就是一个典型的使用场景。 一般分库分表的场景，就是要把一个逻辑上的大表分散到不同的数据库实例上。比如。将一个大表ht，按照字段f，拆分成1024个分表，然后分布到32个数据库实例上。如下图所示： 分区key的选择是以“减少跨库和跨表查询”为依据的。如果大部分的语句都会包含f的等值条件，那么就要用f做分区键 比如： select v from ht where f=N; 可以通过分表规则（比如，N%1024)来确认需要的数据被放在了哪个分表上 但是，如果这个表上还有另外一个索引k，并且查询语句是这样的： select v from ht where k &gt;= M order by t_modified desc limit 100; 由于查询条件里面没有用到分区字段f，只能到所有的分区中去查找满足条件的所有行，然后统一做order by 的操作。这种情况有两种思路： 在proxy层的进程代码中实现排序。优势是快，缺点是工作量大，proxy端压力大 把分库数据汇总到一个表中，再在汇总上操作。如下图所示 为什么临时表可以重名？ create temporary table temp_t(id int primary key)engine=innodb; 执行该语句，MySQL会创建一个frm文件保存表结构定义。该文件放在临时文件目录下，文件名的后缀是.frm，前缀是“#sql{进程id}_{线程id}_序列号” 除了文件名不同，内存里面也有一套机制区别不同的表，每个表都对应一个table_def_key 一个普通表的table_def_key的值是由“库名+表名”得到的，所以如果你要在同一个库下创建两个同名的普通表，创建第二个表的过程中就会发现table_def_key已经存在了。 而对于临时表，table_def_key在“库名+表名”基础上，又加入了“server_id+thread_id” 临时表和主备复制 如果当前的binlog_format=row，那么跟临时表有关的语句，就不会记录到binlog里 如果binlog_format=statment/mixed，创建临时表的语句会传到备库，由备库的同步线程执行。因为主库的线程退出时会自动删除临时表，但是备库同步线程是持续运行的，所以还需要在主库上再写一个DROP TEMPORARY TABLE传给备库执行 主库上不同线程创建同名的临时表是没关系的，但是传到备库怎么处理？ MySQL在记录binlog的时候，会把主库执行这个语句的线程id写到binlog中。这样，在备库的应用线程就能够知道执行每个语句的主库线程id，并利用这个线程id来构造临时表的table_def_key： session A的临时表t1，在备库的table_def_key就是：库名+t1+“M的serverid”+“session A的thread_id”; session B的临时表t1，在备库的table_def_key就是 ：库名+t1+“M的serverid”+“session B的thread_id” 为什么会使用内部临时表 union 执行流程 假设有表t1： create table t1(id int primary key, a int, b int, index(a)); delimiter ;; create procedure idata() begin declare i int; set i=1; while(i&lt;=1000)do insert into t1 values(i, i, i); set i=i+1; end while; end;; delimiter ; call idata(); 然后执行： (select 1000 as f) union (select id from t1 order by id desc limit 2); 这个语句的执行流程是这样的： 创建一个内存临时表，这个临时表只有一个整型字段f，并且f是主键字段 执行第一个子查询，得到1000这个值，并存入临时表中 执行第二个子查询： 拿到第一行id=1000，试图插入临时表中。但由于1000这个值已经存在于临时表了，违反了唯一性约束，所以插入失败，然后继续执行； 取到第二行id=999，插入临时表成功 从临时表中按行取出数据，返回结果，并删除临时表，结果中包含两行数据分别是1000和999 如果使用union all，就没有去重，执行的时候是依次执行子查询，得到的结果直接作为结果集的一部分，不需要临时表 group by 执行流程 select id%10 as m, count(*) as c from t1 group by m; 这个语句的执行流程如下： 创建内存临时表，表里有两个字段m和c，主键是m 扫描表t1的索引a，依次取出叶子节点上的id值，计算id%10的结果，记为x； 如果临时表中没有主键为x的行，就插入一个记录(x,1); 如果表中有主键为x的行，就将x这一行的c值加1 遍历完成后，再根据字段m做排序，得到结果集返回给客户端 如果不需要排序，在语句末尾加上order by null 当内存临时表大小达到上限时，会转成磁盘临时表，磁盘临时表默认使用的引擎是InnoDB group by 优化方法 --索引 新增一列，给这列加索引 alter table t1 add column z int generated always as(id % 100), add index(z); 对这列group by： select z, count(*) as c from t1 group by z; group by 优化方法 --直接排序 碰到不能加索引的场景就得老老实实做排序 在group by语句中加入SQL_BIG_RESULT这个提示（hint），就可以告诉优化器：这个语句涉及的数据量很大，请直接用磁盘临时表 select SQL_BIG_RESULT id%100 as m, count(*) as c from t1 group by m; 这个语句的执行流程如下： 初始化sort_buffer，确定放入一个整型字段，记为m 扫描表t1的索引a，依次取出里面的id值, 将 id%100的值存入sort_buffer中 扫描完成后，对sort_buffer的字段m做排序（如果sort_buffer内存不够用，就会利用磁盘临时文件辅助排序） 排序完成后，就得到了一个有序数组。顺序扫描一遍就可以得到结果 基于上面的union、union all和group by语句的执行过程的分析，我们来回答文章开头的问题：MySQL什么时候会使用内部临时表？ 如果语句执行过程可以一边读数据，一边直接得到结果，是不需要额外内存的，否则就需要额外的内存，来保存中间结果 join_buffer是无序数组，sort_buffer是有序数组，临时表是二维表结构 如果执行逻辑需要用到二维表特性，就会优先考虑使用临时表。比如我们的例子中，union需要用到唯一索引约束， group by还需要用到另外一个字段来存累积计数。 都说InnoDB好，那还要不要使用Memory引擎 内存表的数据组织结构 假设有两张表t1，t2，t1使用Memory引擎，t2使用InnoDB引擎 create table t1(id int primary key, c int) engine=Memory; create table t2(id int primary key, c int) engine=innodb; insert into t1 values(1,1),(2,2),(3,3),(4,4),(5,5),(6,6),(7,7),(8,8),(9,9),(0,0); insert into t2 values(1,1),(2,2),(3,3),(4,4),(5,5),(6,6),(7,7),(8,8),(9,9),(0,0); 然后，分别执行select *from t1和select* from t2。t2表的(0,0)出现在第一行，t1表出现在最后一行 这是因为InnoDB引擎的数据就存在主键索引上，而主键索引是有序存储的，在执行select *的时候，就会按照叶子节点从左到右扫描，所以得到的结果里，0就出现在第一行 而Memory引擎的数据和索引是分开的。主键索引存的是每个数据的位置。执行select *走的是全表扫描数据数组 InnoDB和Memory引擎的数据组织方式是不同的： InnoDB引擎把数据放在主键索引上，其他索引上保存的是主键id。这种方式，我们称之为索引组织表 Memory引擎采用的是把数据单独存放，索引上保存数据位置的数据组织形式，我们称之为堆组织表 两个引擎的一些典型不同： InnoDB表的数据总是有序存放的，而内存表的数据就是按照写入顺序存放的 当数据文件有空洞的时候，InnoDB表在插入新数据的时候，为了保证数据有序性，只能在固定的位置写入新值，而内存表找到空位就可以插入新值； 数据位置发生变化的时候，InnoDB表只需要修改主键索引，而内存表需要修改所有索引 InnoDB表用主键索引查询时需要走一次索引查找，用普通索引查询的时候，需要走两次索引查找。而内存表没有这个区别，所有索引的“地位”都是相同的 InnoDB支持变长数据类型，不同记录的长度可能不同；内存表不支持Blob 和 Text字段，并且即使定义了varchar(N)，实际也当作char(N)，也就是固定长度字符串来存储，因此内存表的每行数据长度相同。 hash索引和B-Tree索引 内存表的范围查询不能走主键索引，但是可以加一个B-Tree索引,B-Tree索引类似于InnoDB的B+树索引 alter table t1 add index a_btree_index using btree (id); 不建议在生产环境使用内存表，原因有两方面： 锁粒度问题。内存表不支持行锁，只支持表锁 数据持久化问题 自增主键为什么不是连续的 自增主键可以让主键索引尽量保持递增顺序插入，避免页分裂，因此索引更紧凑，但自增主键不能保证连续递增 自增值保存在哪？ InnoDB的自增值保存在内存中。每次重启MySQL都会计算max(id)+1作为自增值。8.0版本，重启的时候依靠redo log恢复自增值 自增值修改机制 假设，某次插入的值是X，当前的自增值是Y 如果X &lt; Y，那么自增值不变 如果X &gt;= Y，将当前自增值修改为新的自增值 Z = auto_increment_offset+k*auto_increment_increment。Z &gt; X，auto_increment_offset是自增初始值，auto_increment_increment是自增步长，k是自然数 自增值的修改时机 自增值在真正执行插入数据的操作之前修改。如果因为唯一键冲突导致插入失败会出现id不连续，事务回滚也是类似现象 自增锁的优化 自增id锁并不是一个事务锁，而是每次申请完就马上释放，以便允许别的事务再申请。建议innodb_autoinc_lock_mode设置成2，即前面的策略，同时binlog_format=row，避免insert … select造成主备数据不一致 insert语句的锁为什么这么多 insert … select 语句 在可重复读隔离级别下，binlog_format=statement时，执行 insert … select 语句会对select表的需要访问的资源加锁。加锁是为了避免主备不一致 insert 循环写入 如果把select表的结果insert到select表中，会对select表全表扫描，创建一个临时表，再将select结果insert回表。这么做的原因是：这类一边遍历数据，一边更新数据的情况，如果读出来的数据直接写回原表，就可能在遍历过程中，读到刚刚插入的记录，新插入的记录如果参与计算逻辑，就跟语义不符 优化方法是：手动创建内存临时表，先 insert临时表select目标表，再 insert目标表select临时表，这样就不会对目标表全表扫描 insert 唯一键冲突 在session A执行rollback语句回滚的时候，session C几乎同时发现死锁并返回 这个死锁产生的逻辑是这样的： 在T1时刻，启动session A，并执行insert语句，此时在索引c的c=5上加了记录锁。注意，这个索引是唯一索引，因此退化为记录锁 在T2时刻，session B要执行相同的insert语句，发现了唯一键冲突，加上读锁；同样地，session C也在索引c上，c=5这一个记录上，加了读锁(共享next-key lock) T3时刻，session A回滚。这时候，session B和session C都试图继续执行插入操作，都要加上写锁（排它next-key lock）。两个session都要等待对方的行锁，所以就出现了死锁 insert into … on duplicate key update 这个语义的逻辑是，插入一行数据，如果碰到唯一键约束，就执行后面的更新语句。它给唯一索引加排它的next-key lock（写锁） 怎么最快地复制一张表 如果可以控制对原表的扫描行数和加锁范围很小的话，可以直接用insert … select。否则先将数据写到外部文件，再写回目标表，方法有三种： 物理拷贝的方式速度最快，尤其对于大表拷贝来说是最快的方法。如果出现误删表的情况，用备份恢复出误删之前的临时库，然后再把临时库中的表拷贝到生产库上，是恢复数据最快的方法。但是，这种方法的使用也有一定的局限性： 必须是全表拷贝，不能只拷贝部分数据； 需要到服务器上拷贝数据，在用户无法登录数据库主机的场景下无法使用； 由于是通过拷贝物理文件实现的，源表和目标表都是使用InnoDB引擎时才能使用。 用mysqldump生成包含INSERT语句文件的方法，可以在where参数增加过滤条件，来实现只导出部分数据。这个方式的不足之一是，不能使用join这种比较复杂的where条件写法 用select … into outfile的方法是最灵活的，支持所有的SQL写法。但，这个方法的缺点之一就是，每次只能导出一张表的数据，而且表结构也需要另外的语句单独备份 grant之后要跟着flushprivileges吗 grant语句会同时修改数据表和内存，判断权限的时候使用的是内存数据。因此，规范地使用grant和revoke语句，是不需要随后加上flush privileges语句的。 flush privileges语句本身会用数据表的数据重建一份内存权限数据，所以在权限数据可能存在不一致的情况下再使用。而这种不一致往往是由于直接用DML语句操作系统权限表导致的，所以我们尽量不要使用这类语句。 要不要使用分区表 相对于用户分表： 优势：对业务透明，使用分区表的业务代码更简洁，且可以很方便的清理历史数据 劣势：第一次访问的时候需要访问所有分区；共用MDL锁","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://zunpan.github.io/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://zunpan.github.io/tags/MySQL/"},{"name":"数据库","slug":"数据库","permalink":"https://zunpan.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"}]},{"title":"Java并发编程实战学习笔记","slug":"Java并发编程实战学习笔记","date":"2023-01-13T09:02:46.000Z","updated":"2023-09-24T04:27:40.278Z","comments":true,"path":"2023/01/13/Java并发编程实战学习笔记/","link":"","permalink":"https://zunpan.github.io/2023/01/13/Java%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AE%9E%E6%88%98%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","excerpt":"","text":"本书分成四部分， 第一部分是基础，主要内容是并发的基础概念和线程安全，以及如何用java类库提供的并发构建块组成线程安全的类（2-5章节） 第二部分是构建并发应用程序，主要内容是如何利用线程来提高并发应用的吞吐量和响应能力（6-9章节） 第三部分是活跃性、性能和测试，主要内容是如何确保并发程序能够按照你的要求执行，并且具有可接受的性能（10-12章节） 第四部分是高级主题，涵盖了可能只有有经验的开发人员才会感兴趣的主题:显式锁、原子变量、非阻塞算法和开发自定义同步器（13-16章节） Part 1 基础 Chapter 1 简介 1.1 并发简史 早期计算机没有操作系统，程序独占所有资源，一次只能有一个程序运行，效率低下 操作系统出现后，程序（进程）可以并发运行，由操作系统分配资源，进程互相隔离，必要时依靠粗粒度的通信机制：sockets、signal handlers、shared memory等机制通信 进程提高了系统吞吐量和资源利用率，线程的出现也是这个原因，线程有时被称为轻量级进程，大多数现代操作系统将线程而不是进程视为调度的基本单位，同一程序的多个线程可以同时在多个CPU上调度，如果没有同步机制协调对共享数据的访问，一个线程可能会修改另一个线程正在使用的变量，导致不可预测的结果 1.2 线程优势 减小开发和维护开销 提高复杂应用的性能 提高GUI的响应能力 简化JVM实现 1.3 线程风险 竞争(多个线程以未知顺序访问资源) 活跃性(死锁,饥饿,活锁) 性能(频繁切换导致开销过大) 1.4 无处不在的线程 框架通过在框架线程中调用应用程序代码将并发性引入到应用程序中，在代码中将不可避免的访问应用程序状态,因此所有访问这些状态的代码路径都必须是线程安全的 Timer类，TimerTask在Timer管理的线程中执行 Servlet(每个请求使用一个线程同时执行Servlet) RMI(由RMI负责打包拆包远程对象) Swing(具有异步性) Chapter 2 线程安全 多个线程访问同一个可变的状态变量时没有使用合适的同步机制,可以用以下方法修复: 不在线程间共享该变量 将变量变为常量 访问时候使用同步 2.1 什么是线程安全？ 如果一个类被多线程访问，不管线程调度或交叉执行顺序如何，类的表现都是正确的，那么类是线程安全的 线程安全类封装任何需要的同步，因此客户端不需要提供自己的同步。 无状态的对象永远是线程安全的 2.2 原子性 竞争情况（race condition）：由于不恰当的执行顺序导致出现不正确的结果，常发生在以下情况中： 读取-修改-写入，例子: 自增操作 先检查后执行，例子:延迟初始化,不安全的单例模式,懒汉模式 第一种情况解决方式：使用juc里面的类，比如count可以用AtomicLong类型操作保证原子性 第二种情况解决方式：加锁保证原子性 2.3 加锁 如果类有多个变量需要更新，即使它们的各自操作都是原子性的，也要把他们放在同一个原子操作中，方式是加锁。Java 提供了锁机制来增强原子性：synchronized 内置锁: synchronized 实例方法会将被调用方法的对象作为内置锁或监视锁，内置锁是互斥的，同一时刻最多只有一个线程拿到这个锁 可重入: 内置锁是可重入的，已经拿到锁的线程可以再次获取锁，实现方式是锁会(就是lock对象)关联一个持有者和计数值，持有者再次进入次数加一，退出减一，减到0会释放锁 2.4 用锁来保护状态 混合操作比如说读取-修改-写入和先检查后执行，需要保证原子性来避免竞争情况。 常见错误: 只有写入共享变量才需要同步 原因：读取也需要同步，不然可能会看到过期值 每个共享的可变变量应该由同一个锁保护，常见的加锁习惯是将可变变量封装到一个对象中 对于不变性条件（invariant）中涉及的多个变量，这多个变量都需要用同一个锁保护，例如Servlet缓存了请求次数和请求数据（数组），不变性条件是请求数据的长度等于次数，这通过加锁来保证 2.5 活跃性和性能 给Servlet的方法声明syncronized极大降低了并发性，我们可以通过缩小同步块的范围，在保证线程安全的情况下提高并发性。合理的做法是将不影响共享状态的操作从同步块中排除 Chapter 3 共享对象 synchronized 块和方法可以确保操作的原子性执行，它还有另一个重要作用：内存可见性。我们不仅想要防止一个线程在另一个线程使用一个对象时修改它的状态，还想要确保当一个线程修改一个对象的状态时，其他线程可以看到最新更改 3.1 可见性 过期数据(当一个线程修改数据,但其他线程不能立马看到)。 读取操作如果不同步，仍然能看到一个过期数据，这叫做最低安全性(过期数据至少是由之前的线程设置的，而不是随机值) 大多数变量都满足最低安全性,除了非volatile修饰的64位变量(double和long)，jvm允许将64位操作拆解为2个 32位操作，读取这样的变量可能会出现过期值的高32位+新值的低32位的结果 内置锁保证可见性 volatile: 保证可见性,禁止指令重排,不保证原子性(使用场合:保证自身可见性,引用对象状态可见性,标识重要的生命周期事件) 当且仅当满足以下所有条件时,才应该使用volatile变量: 对变量的写入不依赖于它的当前值，或者可以确保只有一个线程更新该值; 该变量不会与其他状态变量一起纳入不变性条件 在访问变量时，由于任何其他原因不需要加锁。 3.2 发布与逃逸 发布是指让对象在外部可见，常见方式是对象引用声明为 public static。发布对象的同时，任何通过非私有字段引用和方法调用链从发布对象中访问的对象也被发布了 逃逸是指对象的私有信息也对外可见了，比如发布一个对象包含一个私有数组，同时提供一个返回引用的get方法，外部可以通过引用修改内部私有数组 3.3 线程封闭 如果对象限制在一个线程中使用，即使对象不是线程安全的，也会自动线程安全 例子：Swing: 将组件和数据对象放到事件分发线程，其它线程访问不了这些对象；JDBC.Connection对象: 应用线程从数据库连接池中获取一个连接对象，连接对象由该线程独自使用 Java 提供 ThreadLocal 来实现线程封闭，程序员做的是阻止对象从线程中逃逸 线程封闭通常用来实现一个子系统，例如GUI，它是单线程的 Ad-hoc封闭: 核线程封闭性的职责完全由程序实现来承担(脆弱,少用) 栈封闭: 只能通过局部变量访问对象(Java基本类型或者局部变量) ThreadLocal类: 提供getter和setter，每个使用该变量的线程存有一份独立的副本 3.4 不可变 不可变对象永远是线程安全的 满足以下条件,对象才是不可变的: 构造函数之后状态不可修改 所有域都是final 对象正确创建（this引用没有在构造期间逃逸） 多个状态的对象需要保证线程安全，可以将状态封装到一个不可变类中，用volatile修饰不可变对象引用 3.5 安全发布 不正确的发布对象会出现两个问题：其它线程会看到null或旧值；最糟糕的是其它线程看到最新的引用但是被引用的对象还是旧的 由于不可变对象很重要，Java内存模型为不可变对象的共享提供一种特殊的初始化安全性保证，不用同步也能安全发布 一个正确构造的对象可以通过以下方式安全发布: 静态初始化函数中初始化一个对象引用 引用保存到volatile域或者AtomicReference对象中 引用保存到某个正确构造对象的final域 引用保存到锁保护的域(容器也可) 不可变对象，可以放宽到事实不可变对象(对象在发布之后不会改变状态) 可变对象必须通过安全方式发布,并且必须是线程安全的或者锁保护起来 并发程序共享对象实用策略 线程封闭 只读共享 线程安全共享:对象内部实现同步 保护对象:锁机制 Chapter 4 组合对象 本章讨论如何将线程安全的组件组合成更大的组件或程序 4.1 设计一个线程安全的类 在设计线程安全类的过程中，常会包含以下三个基本要素: 找出构成对象状态的所有变量。 找出约束状态变量的不变性条件和后验条件。 建立对象状态的并发访问管理策略。 如果不了解对象的不变性条件和后验条件,就无法确保线程安全 依赖状态的方法需要先满足某种状态才能运行，即先验条件。java提供了 wait and notify 机制来等待先验条件成立，它依赖内置锁。更简单的实现方法是用java类库的阻塞队列或者信号量 一般情况下，状态所属权是封装状态的类，除非类公开可变对象的引用，这时候类只有共享权 4.2 实例封闭 在对象中封装数据，通过使用对象方法访问数据，从而更容易确保始终在持有适当锁的情况下访问数据。 Java监视器模式：封装可变状态到对象中，使用对象的内置锁保护状态，使用私有锁对象更有优势 4.3 线程安全的委托 将线程安全的职责委托给线程安全的类，例如计数器类不做同步处理，依赖AtomicLong类型达到线程安全 可以将线程安全委托给多个基础状态变量，只要它们是独立的 委托失效：多个变量间有不变性条件,比如大小关系等,需要加锁,除非复合操作也可以委托给变量 如果一个状态变量是线程安全的，不参与任何限制其值的不变性条件，并且在任何操作中都没有禁止的状态转换，那么它就可以安全地发布。 4.4 给现有的线程安全类加功能 继承方式（可能会因为子父类加的锁不一样线程不安全） 客户端加锁，使用辅助类，若类的加锁依赖其它类，那么辅助类容易错误加锁 组合方式，加锁策略完全由组合类提供 4.5 文档化同步策略 为类的客户端记录线程安全保证;为其维护者记录其同步策略 Chapter 5 基础构建模块 在实际应用中，委托是创建线程安全类最有效的策略之一，本章介绍平台库的并发构建模块，例如线程安全的集合和各种可以协调线程控制流的同步器 5.1 同步集合 Vector、Hashtable，以及JDK1.2增加了 Collections.synchronizedXxx 创建同步包装类 复合线程安全的类的方法可能不是线程安全的，例如复合方法调用size和get方法，中间可能被删掉元素导致size结果不对 迭代器或者for-each不会锁定集合，在迭代过程中检测到集合变化时会抛出ConcurrentModificationException异常，检测是通过检测count值，但是没有同步，可能看到过期值 隐藏的迭代器(某些操作底层隐藏着调用迭代器，比如集合的toString) 5.2 并发集合 同步集合通过序列化对集合状态的所有访问来实现线程安全，性能低。Java 5增加了并发集合 ConcurrentHashMap，使用分段锁，具有弱一致性，同时size和isEmpty是估计并不精确，只有需要独占Map，才不建议使用该Map CopyOnWriteArrayList，每次修改都是返回副本,建议迭代多修改少的时候使用 5.3 阻塞队列和生产者-消费者模式 BlockingQueue，常用来实现生产者和消费者，有一个特殊的实现SynchronousQueue，它不是一个实际的队列，当生产者生产数据时直接交给消费者，适用于消费者多的场景 Deque，常用来实现工作窃取模式。生产者和消费者模式中，消费者共享一个队列，工作窃取模式中，消费者有独自的队列，当消费完后会偷其他人的工作。工作窃取模式可以减少对于共享队列的竞争 5.4 阻塞方法与中断方法 当某个方法抛出InterruptedException,说明该方法是阻塞方法,可以被中断 代码中调用一个阻塞方法（阻塞方法和线程状态没有必然关系，方法可能是个长时间方法所以声明抛出InterruptedException，也有可能是会导致线程状态改变的sleep方法）,必须处理中断响应. 捕获/抛出异常 恢复中断.调用当前线程的interrupt 5.5 同步器 阻塞队列 闭锁（Latch）: 延迟线程进度，直到条件满足，FutureTask也可以做闭锁 信号量：类似发布凭证,但是任意线程都可以发布和返还 栅栏: 阻塞一组线程,直到某个条件满足;如果有某个线程await期间中断或者超时,所有阻塞的调用都会终止并抛出BrokenBarrierException 5.6 构建高效且可伸缩的缓存 使用hashMap+synchronized，性能差 ConcurrentHashMap代替hashMap+synchronized，有重复计算问题 ConcurrentHashMap的值用FutureTask包起来，只要键已经存在，从FutureTask获取结果，因为check-then-act模式，仍然存在重复计算问题 使用putIfAbsent设置缓存 Part 2 构建并发应用程序 Chapter 6 任务执行 6.1 在线程中执行任务 串行执行任务(响应会慢，服务器资源利用率低) 显式为每个请求申请一个线程 任务处理线程从主线程分离,提高响应速度 任务可以并行处理,提高吞吐量 任务处理代码必须是线程安全的,多个线程会并发执行 无限制创建线程的不足 创建销毁浪费时间 浪费资源 稳定性差 6.2 Executor框架 Executor基于生产-消费模式,提交任务相当于生产者,执行任务的线程相当于消费者. 执行策略 What: 在什么线程中执行任务，按什么顺序执行，任务执行前后要执行什么操作 How Many: 多少任务并发，多少等待 Which: 系统过载时选择拒绝什么任务 How: 怎么通知任务成功/失败 线程池，管理一组同构工作线程的资源池,跟工作队列密切相关 Executor生命周期 运行 : 对象新建时就是运行状态 关闭 : 不接受新任务,同时等待已有任务完成,包括未执行的任务,关闭后任务再提交由 “被拒绝的执行处理器” 处理或者直接抛异常 终止 : 关闭后任务完成 延迟任务和周期任务 Timer类可以负责,但是存在缺陷,应该考虑ScheduledThreadPoolExecutor代替它 Timer: 只用一个线程执行定时任务，假如某个任务耗时过长，会影响其他任务的定时准确性。除此之外，不支持抛出异常，发生异常将终止线程(已调度（scheduled）未执行的任务，线程不会执行，新任务不会调度，称为线程泄露) DelayQueue: 阻塞队列的一种实现，为ScheduledThreadPoolExecutor提供调度策略 6.3 寻找可利用的并行性 将耗时的IO使用别的线程获取;而不是简单的串行执行 Future 表示一个任务的生命周期，并提供相应的方法判断完成/取消，get会阻塞或抛异常 使用Callable和Future并行化下载和渲染 异构任务并行化获取重大性能提升很困难. 任务大小不同 负载均衡问题 协调开销 CompletionService 将 Executor 和BlockingQueue结合在一起，Executor是生产者，CompletionService是消费者 使用 CompletionService 并行化下载和渲染 为任务设置时限 需要获取多个设置了时限的任务的结果可以用带上时间的 invokeAll 提交多个任务 Chapter 7 取消和关闭 本章讲解如何停止任务和线程，Java没有安全强制线程停止的方法，只有一种协作机制，中断 7.1 任务取消 有一种协作机制是在任务中设置取消位，任务定期查看该标识，假如置位就结束任务(假如线程阻塞了，就看不到取消位，那么就停不下来了) 中断: 在取消任务或线程之外的其他操作中使用中断是不合适的 每个线程都有一个中断标志，interrupt中断目标线程，isInterrupted返回目标线程的中断状态，interrupted（糟糕的命名）清除当前线程中断; Thread.sleep和Object.wait都会检查线程什么时候中断,发现时提前返回(不会立即响应,只是传递请求而已) 中断策略：尽快推迟执行流程，传递给上层代码；由于每个线程拥有各自的中断策略，除非知道中断对这个线程的含义，否则不应该中断该线程 中断响应 当调用会抛出InterruptedException的阻塞方法时，有两种处理策略 传播异常，让你的方法也变成会抛出异常的阻塞方法（中断标志一直为true） 恢复中断状态，以便调用堆栈上较高级的代码处理它（try-catch之后中断标志为false，可以调用当前线程的interrupt方法恢复成中断状态）。 在中断线程之前，要了解线程的中断策略 通过Future取消任务 处理不可中断的阻塞 java.io中的同步Socket I/O.通过关闭Socket可以使阻塞线程抛出异常 java.io中的同步 I/O.终端一个InterruptibleChannel会抛出异常并关闭链路 获取某个锁. Lock提供lockInterruptibly 通过 newTaskFor 方法进一步优化 7.2 停止基于线程的服务 基于线程的服务：拥有线程的服务，例如线程池 只要拥有线程的服务的生命周期比创建它的方法的生命周期长，就提供生命周期方法。例如线程池 ExecutorService 提供了shutdown 日志服务：多生产者写入消息到阻塞队列，单消费者从阻塞队列中取消息，停止日志服务需要正确关闭线程。需要对结束标志位和队列剩余消息数同步访问（书有错误，LoggerThread 应该 synchronized (LogService.this)） 毒丸，生产者将毒丸放在队列上，消费者拿到毒丸就结束 shutdownNow 取消正在执行的任务，返回已提交未开始的任务，可以用个集合保存执行中被取消的任务 7.3 处理非正常的线程终止 通常是因为抛出运行时异常导致线程终止 处理方法： try-catch 捕获任务异常，如果不能恢复，在finally块中通知线程拥有者 当线程因未捕获异常而退出时，JVM会将事件报告给线程拥有者提供的UncaughtExceptionHandler，如果没有处理程序就将堆栈打印到System.err 通过execute提交的任务的异常由UncaughtExceptionHandler处理,submit提交的任务，通过调用Future.get方法，包装在ExecutionException里面 7.4 JVM关闭 有序关闭：最后一个非守护线程终止（可能是调用了System.exit，或者发送SIGINT或按Ctrl-C）后终止 突然关闭：通过操作系统终止JVM进程，例如发送SIGKIll 有序关闭中，JVM首先启动所有已注册的关闭钩子(通过Runtime.addShutdownHook注册的未启动线程)。如果应用程序线程在关闭时仍在运行，将与关闭线程并行执行。当所有关闭钩子都完成时，如果runFinalizersOnExit为true,那么jvm可能运行终结器，然后停止 守护线程：执行辅助功能的线程，不会阻止JVM关闭。当JVM关闭时，守护线程直接关闭，不执行 finally 块，栈不会展开。守护线程适合做“内务”任务，例如清缓存 终结器：GC在回收对象后会执行 finalize 方法释放持久资源。终结器在JVM管理的线程中运行，需要同步访问。终结器难写且性能低，除非要关闭 native 方法获取的资源，否则在 finally中显示关闭就够了 Chapter 8 使用线程池 本章将介绍配置和调优线程池的高级选项，描述使用任务执行框架时需要注意的危险 8.1 任务和执行策略之间的隐式耦合 Executor 框架在任务提交和执行之间仍存在一些耦合： 依赖其它任务的任务，相互依赖可能导致活跃性问题 利用线程封闭的任务，这类任务不做同步，依赖单线程执行 响应时间敏感的任务，可能需要多线程执行 使用 ThreadLocal 的任务，ThreadLocal不应该用于线程池中任务之间的通信 8.1.1 线程饥饿死锁 把相互依赖的任务提交到一个单线程的Executor一定会发生死锁。增大线程池，如果被依赖的任务在等待队列中，也会发生死锁 8.1.2 运行耗时长的任务 即使不出现死锁,也会降低性能,通过限制执行时间可以缓解 8.2 设置线程池大小 cpu数可以调用 Runtime.availableProcessors得出 计算密集型场景，线程池大小等于cpu数+1 IO密集型场景，线程池大小等于cpu数 * cpu利用率 * （1+等待/计算时间比） 8.3 配置 ThreadPoolExecutor 8.3.1 线程创建和销毁 corePoolSize：线程池大小，只有工作队列满了才会创建超出这个数量的线程 maximumPoolSize：最大线程数量 keepAliveTime：空闲时间超过keepAliveTime的线程会成为回收的候选线程，如果线程池的大小超过了核心的大小，线程就会被终止 8.3.2 管理工作队列 可以分成三类：无界队列、有界队列和同步移交。队列的选择和线程池大小、内存大小的有关 无界队列可能会耗尽资源，有界队列会带来队列满时新任务的处理问题，同步移交只适合用在无界线程池或饱和策略可以接受 8.3.3 饱和策略 当任务提交给已经满的有界队列或已经关闭的Executor，饱和策略开始工作 Abort，默认策略，execute方法会抛RejectedExecutionException Discard：丢弃原本下个执行的任务，并重新提交新任务 Caller-Runs：将任务给调用execute 的线程执行 无界队列可以使用信号量进行饱和策略 8.3.4 线程工厂 通过ThreadFactory.newThread创建线程，自定义线程工厂可以在创建线程时设置线程名、自定义异常 8.3.5 调用构造函数后再定制ThreadPoolExecutor 线程池的各项配置可以通过set方法配置，如果不想被修改，可以调用Executors.unconfigurableExecutorService 将其包装成不可修改的线程池 8.4 扩展 ThreadPoolExecutor ThreadPoolExecutor给子类提供了钩子方法，beforeExecute、afterExecute和terminated beforeExecute和afterExecute钩子在执行任务的线程中调用，可用于添加日志记录、计时、监控或统计信息收集。无论任务从run正常返回还是抛出异常，afterExecute钩子都会被调用。如果beforeExecute抛出一个RuntimeException，任务就不会执行，afterExecute也不会被调用 terminated钩子在任务都完成且所有工作线程都关闭后调用，用来释放资源、执行通知或日志记录 8.5 递归算法并行化 如果迭代操作之间是独立的，适合并行化 递归不依赖于后续递归的返回值 Chapter 9 GUI应用 9.1 为什么GUI是单线程的 由于竞争情况和死锁，多线程GUI框架最终都变成了单线程 9.1.1 串行事件处理 优点：代码简单 缺点：耗时长的任务会发生无响应（委派给其它线程执行） 9.1.2 Swing的线程封闭 所有Swing组件和数据模型对象都封闭在事件线程中,任何访问它们的代码必须在事件线程里 9.2 短时间的GUI任务 事件在事件线程中产生,并冒泡到应用程序提供的监听器 Swing将大多数可视化组件分为两个对象(模型对象和视图对象),模型对象保存数据,可以通过引发事件表示模型发生变化,视图对象通过订阅接收事件 9.3 长时间的GUI任务 对于长时间的任务可以使用线程池 取消 使用Future 进度标识 9.4 共享数据模型 只要阻塞操作不会过度影响响应性,那么事件线程和后台线程就可以共享该模型 分解数据模型.将共享的模型通过快照共享 9.5 其它形式单线程 为了避免同步或死锁使用单线程，例如访问native方法使用单线程 Part 3 活跃性、性能和测试 Chapter 10. 避免活跃性危险 Java程序不能从死锁中恢复，本章讨论活跃性失效的一些原因以及预防措施 10.1 死锁 哲学家进餐问题：每个人都有另一个人需要的资源，并且等待另一个人持有的资源，在获得自己需要的资源前不会释放自己持有的资源，产生死锁 10.1.1 Lock-ordering死锁 线程之间获取锁的顺序不同导致死锁。 解决方法：如果所有线程以一个固定的顺序获取锁就不会出现Lock-ordering死锁 10.1.2 动态Lock Order死锁 获取锁的顺序依赖参数可能导致死锁。 解决方法：对参数进行排序，统一线程获取锁的顺序 10.1.3 协作对象的死锁 如果在持有锁时调用外部方法,将会出现活跃性问题,这个外部方法可能阻塞,加锁等导致其他线程无法获得当前被持有的锁 解决方法：开放调用 10.1.4 开放调用 如果在方法中调用外部方法时不需要持有锁（比如调用者this）,那么这种调用称为开放调用。实现方式是将调用者的方法的同步范围从方法缩小到块 10.1.5 资源死锁 和循环依赖锁导致死锁类似。例如线程持有数据库连接且等待另一个线程释放，另一个线程也是这样 10.2 避免和诊断死锁 使用两部分策略来审计代码以确保无死锁:首先，确定哪些地方可以获得多个锁(尽量使其成为一个小集合)，然后对所有这些实例进行全局分析，以确保锁的顺序在整个程序中是一致的，尽可能使用开放调用简化分析 10.2.1 定时锁 另一种检测死锁并从死锁中恢复的技术是使用显示锁中的Lock.tryLock()代替内置锁 10.2.2 用Thread Dumps进行死锁分析 线程转储包含每个运行线程的堆栈信息，锁信息（持有哪些锁，从哪个栈帧中获得）以及阻塞的线程正在等待获得哪些锁 10.3 其它活跃性危险 10.3.1 饥饿 线程由于无法获得它所需要的资源而不能继续执行，最常见的资源是CPU 避免使用线程优先级，可能导致饥饿 10.3.2 糟糕的响应性 计算密集型任务会影响响应性，通过降低执行计算密集型任务的线程的优先级可以提高前台任务的响应性 10.3.3 活锁 线程执行任务失败后，任务回滚，又添加到队列头部，导致线程没有阻塞但永远不会有进展。多个相互合作的线程为了响应其它线程而改变状态也会导致活锁 解决方法：在重试机制中引入一些随机性 Chapter 11. 性能和可伸缩性 11.1 对性能的思考 11.1.1 性能和可伸缩性 性能: 可以用任务完成快慢或者数量来衡量，具体指标包括服务时间、延迟、 吞吐量、可伸缩性等 可伸缩性: 增加计算资源时提供程序吞吐量的能力 11.1.2 评估性能权衡 许多性能优化牺牲可读性和可维护性，比如违反面向对象设计原则，需要权衡 11.2 Amdahl定律 N：处理器数量 F：必须串行执行的计算部分 Speedup：加速比 $\\text { Speedup } \\leq \\frac{1}{F+\\frac{1-F}{N}} $ 串行执行的计算部分需要仔细考虑，即使任务之间互不影响可以并行，但是线程从任务队列中需要同步，使用ConcurrentLinkedQueue比同步的LinkedList性能好 11.3 线程引入的开销 上线文切换 内存同步（同步的性能开销包括可见性保证，即内存屏障，可以用jvm逃逸分析和编译器锁粒度粗化进行优化） 阻塞(非竞争的同步可以在JVM处理,竞争的同步需要操作系统介入，竞争失败的线程必定阻塞，JVM可以自旋等待（反复尝试获取锁，直到成功）或者被操作系统挂起进入阻塞态，短时间等待选择自旋等待，长时间等待选择挂起) 11.4 减少锁的竞争 并发程序中,对伸缩性最主要的威胁就是独占方式的资源锁 三种减少锁争用的方法： 减少持有锁的时间 减少请求锁的频率 用允许更大并发的协调机制替换互斥锁 11.4.1 减小锁的范围 锁的范围即持有锁的时间 11.4.2 降低锁的力度 分割锁：将保护多个独立的变量的锁分割成单独的锁，这样锁的请求频率就可以降低 11.4.3 分段锁 分割锁可以扩展到可变大小的独立对象上的分段锁。例如ConcurrentHashMap使用了一个包含16个锁的数组，每个锁保护1/16的哈希桶 分段锁缺点：独占访问集合开销大 11.4.4 避免热点字段 热点字段：缓存 热点字段会限制可伸缩性，例如，为了缓存Map中的元素数量，添加一个计数器，每次增删时修改计数器，size操作的开销就是O（1）。单线程没问题，多线程下又需要同步访问计数器，ConcurrentHashMap每个哈希桶一个计数器 11.4.5 互斥锁的替代品 考虑使用并发集合、读写锁、不可变对象和原子变量 读写锁：只要没有一个写者想要修改共享资源，多个读者可以并发访问，但写者必须独占地获得锁 原子变量：提供细粒度的原子操作，可以降低更新热点字段的开销 11.4.6 监测CPU利用率 cpu利用率低可能是以下原因： 负载不够，可以对程序加压 IO密集，可以通过iostat判断，还可以通过监测网络上的流量水平判断 外部约束，可能在等待数据库或web服务的响应 锁竞争，可以用分析工具分析哪些是“热”锁 11.4.7 不要用对象池 现在JVM分配和回收对象已经很快了，不要用对象池 11.5 例子：比较Map的性能 ConcurrentHashMap单线程性能略好于同步的HashMap，并发时性能超好。ConcurrentHashMap对大多数成功的读操作不加锁，对写操作和少数读操作加分段锁 11.6 减少上下文切换 日志记录由专门的线程负责 请求服务时间不应该过长 将IO移动到单个线程 Chapter 12. 并发程序的测试 大多数并发程序测试安全性和活跃性。安全性可以理解为“永远不会发生坏事”，活跃性可以理解为“最终会有好事发生” 12.1 正确性测试 12.1.1 基础单元测试 和顺序程序的测试类似，调用方法，验证程序的后置条件和不变量 12.1.2 阻塞操作测试 在单独的一个线程中启动阻塞活动，等待线程阻塞，中断它，然后断言阻塞操作完成。 Thread.getState不可靠，因为线程阻塞不一定进入WAITING或TIMED_WAITING状态，JVM可以通过自旋等待实现阻塞。类似地，Object.wait和Condition.wait存在伪唤醒情况，处于WAITING或TIMED_WAITING状态的线程可以暂时过渡到RUNNABLE。 12.1.3 安全性测试 给并发程序编写高效的安全测试的挑战在于识别出容易检查的属性，这些属性在程序错误时出错，同时不能让检查限制并发性，最好检查属性时不需要同步。 生产者和消费者模式中的一种方法是校验和，单生产者单消费者可以使用顺序敏感的校验和计算入队和出队元素的校验和，多生产者多消费者要用顺序不敏感的校验和 12.1.4 资源管理测试 任何保存或管理其他对象的对象都不应该在不必要的时间内继续维护对这些对象的引用。可以用堆检查工具测试内存使用情况 12.1.5 使用回调 回调函数通常是在对象生命周期的已知时刻发出的，这是断言不变量的好机会。例如自定义线程池可以在创建销毁线程时记录线程数 12.1.6 产生更多的交替操作 Thread.yield放弃cpu，保持RUNNABLE状态，重新竞争cpu Thread.sleep放弃cpu进入TIME_WAITING状态，不竞争cpu，sleep较小时间比yield更稳定产生交替操作 tips：Java 线程的RUNNABLE 状态对应了操作系统的 ready 和 running 状态，TIME_WAITING（调用Thread.sleep） 和 WAITING（调用Object.wait） 和 BLOCKED(没有竞争到锁) 对应 waiting 状态。interrupt是种干预手段，如果interrupt一个RUNNABLE线程（可能在执行长时间方法需要终止），如果方法声明抛出InterruptedException，就表示可中断，方法会循环检查isInterrupted状态来响应interrupt，一般情况线程状态变成TERMINATED。如果interrupt一个 waiting 线程（可能是由sleep、wait方法导致，这些方法会抛出InterruptedException），线程重新进入 RUNNALBE 状态，处理InterruptedException 12.2 性能测试 增加计时功能（CyclicBarrier） 多种算法比较（LinkedBlockingQueue在多线程情况下比ArrayBlockingQueue性能好） 衡量响应性 12.3 避免性能测试的陷阱 12.3.1 垃圾回收 垃圾回收不可预测，会导致测试误差，需要长时间测试，多次垃圾回收，得到更准确结果 12.3.2 动态编译 动态编译会影响运行时间，需要运行足够长时间或者与完成动态编译后再开始计时 12.3.3 对代码路径的不真实采样 动态编译器会对单线程测试程序进行优化，最好多线程测试和单线程测试混合使用（测试用例至少用两个线程） 12.3.4 不真实的竞争情况 并发性能测试程序应该尽量接近真实应用程序的线程本地计算，并考虑并发协作。例如，多线程访问同步Map，如果本地计算过长，那么锁竞争情况就较弱，可能得出错误的性能瓶颈结果 12.3.5 无用代码的删除 无用代码：对结果没有影响的代码 由于基准测试通常不计算任何东西，很容易被优化器删除，这样测试的执行时间就会变短 解决方法是计算某个派生类的散列值,与任意值比较,加入相等就输出一个无用且可被忽略的消息 12.4 补充的测试方法 代码审查 静态代码分析 面向切面的测试工具 分析与检测工具 Part 4 高级主题 Chapter 13 显示锁 Java 5 之前，对共享数据的协调访问机制只有 synchronized 和 volatile，Java 5 增加了 ReentrantLock。 13.1 Lock和ReentrantLock Lock接口定义加锁和解锁的操作。 ReentrantLock还提供了可重入特性 显示锁和内置锁很像，显示锁出现的原因是内置锁有一些功能限制 不能中断等待锁的线程 必须在获得锁的地方释放锁 13.1.1 轮询和定时获得锁 tryLock：轮询和定时获得锁 内置锁碰到死锁是致命的，唯一恢复方法是重启，唯一防御方法是统一锁获取顺序，tryLock可以概率避免死锁 13.1.2 可中断的获得锁 lockInterruptibly，调用后一直阻塞直至获得锁，但是接受中断信号 13.1.3 非块结构加锁 内置锁是块结构的加锁和自动释放锁，有时需要更大的灵活性，例如基于hash的集合可以使用分段锁 13.2 性能考虑 从Java 6 开始，内置锁已经不比显式锁性能差 13.3 公平性 内置锁不保证公平，ReentrantLock默认也不保证公平，非公平锁可以插队(不提倡,但是不阻止)，性能相比公平锁会好一些 13.4 在 Synchronized 和 ReentrantLock 中选择 当你需要用到轮询和定时加锁、可中断的加锁、公平等待锁和非块结构加锁，使用 ReentrantLock，否则使用 synchronized 13.5 读写锁 读写锁：资源可以被多个读者同时访问或者单个写者访问 ReadWriteLock 定义读锁和写锁方法，和 Lock 类似，实现类在性能、调度、获得锁的优先条件、公平等方面可以不同 Chapter 14 构建自定义的同步工具 最简单的方式使用已有类进行构造，例如LinkedBlockingQueue、CountDown-Latch、Semaphore和FutureTask等 14.1 状态依赖性的管理 单线程中，基于状态的前置条件不满足就失败。但是多线程中，状态会被其它线程修改，所以多线程程序在不满足前置条件时可以等待直至满足前置条件 14.1.1 将前置条件的失败传播给调用者 不满足前置条件就抛异常是滥用异常。调用者可以自旋等待（RUNNABLE态，占用cpu）或者阻塞（waiting态，不占cpu），即需要调用者编写前置条件管理的代码 14.1.2 通过轮询和睡眠粗鲁的阻塞 通过轮询和睡眠完成前置条件管理，不满足是就阻塞，调用者不需要管理前置条件，但需要处理 InterruptedException 14.1.3 条件队列 Object的wait，notify 和 notifyAll构成内置条件队列的API,wait会释放锁(本质和轮询与休眠是一样的，注意sleep前要释放锁) 14.2 使用条件队列 14.2.1 条件谓词 条件谓词：由类的状态变量构造的表达式，例如缓冲区非空即count&gt;0 给条件队列相关的条件谓词以及等待它成立的操作写Javadoc 条件谓词涉及状态变量，状态变量由锁保护，所以在测试条件谓词之前，需要先获得锁。锁对象和条件队列对象（调用wait和notify的对象）必须是同一个对象 14.2.2 过早唤醒 一个线程由于其它线程调用notifyAll醒来，不意味着它的等待条件谓词一定为真。每当线程醒来必须再次测试条件谓词(使用循环) 14.2.3 丢失的信号 线程必须等待一个已经为真的条件，但是在开始等待之前没有检查条件谓词，发生的原因是编码错误，正确写法是循环测试条件谓词，false就继续wait 14.2.4 通知 优先使用 notifyAll，notify 可能会出现“hijacked signal”问题，唤醒了一个条件还未真的线程，本应被唤醒的线程还在等待。只有所有线程都在等同一个条件谓词且通知最多允许一个线程继续执行才使用notify 14.2.5 例子：门 用条件队列实现一个可以重复开关的线程门 14.2.6 子类的安全问题 一个依赖状态的类应该完全向子类暴露它的等待和通知协议，或者禁止继承 14.2.7 封装条件队列 最好将条件队列封装起来，在使用它的类的外面无法访问 14.2.8 进入和退出协议 进入协议：操作的条件谓词 退出协议：检查该操作修改的所有状态变量,确认他们是否使某个条件谓词成真,若是,通知相关队列 14.3 显示 Condition 显示锁在一些情况下比内置锁更灵活。类似地，Condition 比内置条件队列更灵活 内置条件队列有几个缺点： 每个内置锁只能关联一个条件队列，即多个线程可能会在同一个条件队列上等待不同的条件谓词 最常见的加锁模式会暴露条件队列 一个 Condition 关联一个 Lock，就像内置条件队列关联一个内置锁，使用 Lock.newCondition 来创建 Condition，Condition比内置条件队列功能丰富：每个锁有多个等待集（即一个Lock可以创建多个Condition），可中断和不可中断的条件等待，基于截止时间的等待，以及公平或不公平排队的选择。 Condition中和wait，notify，notifyAll 对应的方法是 await，signal，signalAll 14.4 Synchronizer剖析 ReentrantLock 和 Semaphore 有很多相似的地方。ReentrantLock可以作为只许一个线程进入的 Semaphore，Semaphore 可以用 ReentrantLock 实现 它们和其它同步器一样依赖基类 AbstractQueuedSynchronizer（AQS）。AQS是一个用于构建锁和同步器的框架，使用它可以轻松有效地构建范围广泛的同步器。 14.5 AbstractQueuedSynchronizer 依赖AQS的同步器的基本操作是获取和释放的一些变体。获取是阻塞操作，调用者获取不到会进入WAITING或失败。对于锁或信号量，获取的意思是获取锁或许可。对于CountDownLatch，获取的意思是等待门闩到达终点。对于FutureTask，获取的意思是等待任务完成。释放不是阻塞操作。同步器还会根据各自的语义维护状态信息 14.6 JUC同步器类中的AQS JUC许多类使用AQS，例如 ReentrantLock, Semaphore, ReentrantReadWriteLock, CountDownLatch, SynchronousQueue, FutureTask Chapter 15 原子变量与非阻塞同步机制 非阻塞算法使用原子机器指令，例如 compare-and-swap 取代锁来实现并发下的数据完成性。它的设计比基于锁的算法复杂但可以提供更好的伸缩性和活跃性，非阻塞算法不会出现阻塞、死锁或其它活跃性问题，不会受到单个线程故障的影响 15.1 锁的劣势 JVM对非竞争锁进行优化,但是如果多个线程同时请求锁,就要借助操作系统挂起或者JVM自旋,开销很大。相比之下volatile是更轻量的同步机制,不涉及上下文切换和线程调度，然后volatile相较于锁，它不能构造原子性的复合操作，例如自增 锁还会出现优先级反转(阻塞线程优先级高,但是后执行),死锁等问题 15.2 并发操作的硬件支持 排它锁是悲观锁，总是假设最坏情况，只有确保其它线程不会干扰才会执行 乐观方法依赖碰撞检测来确定在更新过程中是否有其它线程的干扰，若有则操作失败并可以选择重试 为多处理器设计的cpu提供了对共享变量并发访问的特殊指令，例如 compare‐and‐swap，load-linked 15.2.1 Compare and Swap CAS 有三个操作数：内存地址V，期待的旧值A，新值B。CAS在V的旧值是A的情况下原子更新值为B，否则什么都不做。CAS是一种乐观方法：它满怀希望更新变量，如果检测到其它线程更新了变量，它就会失败。CAS失败不会阻塞，允许重试（一般不重试，失败可能意味着别的线程已经完成该工作） 15.2.2 非阻塞的计数器 在竞争不激烈的情况下，性能比锁优秀。缺点是强制调用者处理竞争问题（重试、后退或放弃），而锁通过阻塞自动处理争用，直到锁可用 15.2.3 JVM对CAS的支持 JVM将CAS编译成底层硬件提供的方法，加入底层硬件不支持CAS，JVM会使用自旋锁。原子变量类使用了CAS 15.3 原子变量类 原子变量比锁的粒度更细，重量更轻，能提供volatile不支持的原子性 原子变量可以作为更好的 volatile 在高度竞争情况下，锁性能更好，正常情况下，原子变量性能更好 15.4 非阻塞的算法 如果一个线程的故障或挂起不会导致另一个线程的故障或挂起，则该算法称为非阻塞算法;如果一个算法在每个执行步骤中都有线程能够执行，那么这个算法被称为无锁算法。如果构造正确，只使用CAS进行线程间协调的算法可以是无阻塞和无锁的 15.4.1 非阻塞的栈 创建非阻塞算法的关键是如何在保持数据一致性的同时，将原子性更改的范围限制在单个变量内。 非阻塞的栈使用CAS来修改顶部元素 15.4.2 非阻塞的链表 Michale-scott算法 15.4.3 原子字段更新器 原子字段更新器代表了现有 volatile 字段的基于反射的“视图”，以便可以在现有的 volatile 字段上使用CAS 15.4.4 ABA问题 大部分情况下，CAS会询问“V的值还是A吗？”，是A就更新。但是有时候，我们需要知道“从我上次观察到V是A以来，它的值有没有改变？”。对于某些算法，将V的值从A-&gt;B，再从B-&gt;A，是一种更改，需要重新执行算法。解决方法是使用版本号，即使值从A变成B再变回A，版本号也会不同 Chapter 16 Java 内存模型 16.1 内存模型是什么，为什么我需要一个内存模型？ 在并发没有同步的情况下，有许多原因导致一个线程不能立即或永远不能再另一个线程中看到操作的结果 编译器生成的指令顺序与源码不同 变量存在寄存器中而不是内存中 处理器可以并行或乱序执行指令 缓存可能会改变写入变量到主内存的顺序 存储在处理器本地缓存中的值可能对其它处理器不可见 16.1.1 平台的内存模型 在共享存储的多处理器体系结构中，每个处理器都有自己的高速缓存，这些告诉缓存周期性地与主存协调。 体系结构的内存模型告诉程序可以从内存系统得到什么一致性保证，并制定所需的特殊指令（内存屏障或栅栏），以在共享数据时获得所需的额外内存协调保证。为了不受跨体系结构的影响，Java提供了自己的内存模型，JVM通过在适当的位置插入内存屏障来处理JMM（Java内存模型）和和底层平台的内存模型之间的差异 顺序一致性：程序中所有操作都有一个单一的顺序，而不管他们在什么处理器上执行，并且每次读取变量都会看到任何处理器按执行顺序对该变量的最后一次写入。 现代处理器没有提供顺序一致性，JMM也没有 16.1.2 重排 指令重排会使程序的行为出乎意料。同步限制了编译器、运行时和硬件在重排序时不会破坏JMM提供的可见性保证 16.1.3 Java 内存模型 Java 内存模型由一些操作指定，包括对变量的读写、监视器的锁定和解锁。JMM对所有操作定义了一个称为 happens before 的偏序规则： 程序顺序规则：线程按程序定义的顺序执行操作 监视器锁规则：监视器锁的解锁必须发生在后续的加锁之前 volatile 变量规则：对 volatile 字段的写入操作必须发生在后续的读取之前 线程启动规则：对线程调用Thread.start会在该线程所有操作之前执行 线程结束规则：线程中任何操作必须在其它线程检测到该线程已经结束之前执行或者从Thread.join返回或者Thread.isAlive返回false 中断规则：一个线程对另一个线程调用 interrupt 发生在被中断线程检测到中断之前 终结器规则：对象的构造函数必须在启动该对象的终结器之前执行完成 传递性：A发生在B之前，B发生在C之前，那么A发生在C之前 16.1.4 借用同步 通过类库保证 happens-before顺序： 将元素放入线程安全的容器发生在另一个线程从集合中检索元素之前 在CountDownLatch上进行倒数发生在该线程从门闩的await返回之前 释放信号量的许可发生在获取之前 Future代表的任务执行的操作发生在另一个线程从Future.get返回之前 提交Runnable或者Callable任务给执行器发生在任务开始之前 线程到达CyclicBarrier或Exchanger发生在其它线程释放相同的barrier或者exchange point之前 16.2 发布 16.2.1 不安全的发布 当缺少happens-before关系时候，就可能出现重排序问题，这就解释了为什么在没有同步情况下发布一个对象会导致另一个线程看到一个只被部分构造的对象 除了不可变对象之外，使用由其他线程初始化的对象都是不安全的，除非对象的发布发生在消费线程使用它之前 16.2.2 安全发布 使用锁或者volatile变量可以确保读写操作按照 happens-before 排序 16.2.3 安全初始化 静态字段在声明时就初始化由JVM提供线程安全保证。 延迟初始化，可以写在同步方法里面，或者使用辅助类，在辅助类中声明并初始化 16.2.4 双重检查锁 Java 5之前的双重检查锁会出现引用是新值但是对象是旧值，这意味着可以看到对象不正确的状态，Java5之后给引用声明加上 volatile 可以起到线程安全地延迟初始化作用，但是不如使用辅助类，效果一样且更容易懂 16.3 初始化的安全性 初始化安全性只能保证通过final字段可达的值从构造过程完成时开始的可见性（事实不可变对象以任何形式发布都是安全的）。对于通过非final字段可达的值,或者构成完成之后可能改变的值,必须采用同步确保可见性。","categories":[{"name":"Java","slug":"Java","permalink":"https://zunpan.github.io/categories/Java/"}],"tags":[{"name":"Java并发编程实战","slug":"Java并发编程实战","permalink":"https://zunpan.github.io/tags/Java%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AE%9E%E6%88%98/"},{"name":"Java Concurrency In Practice","slug":"Java-Concurrency-In-Practice","permalink":"https://zunpan.github.io/tags/Java-Concurrency-In-Practice/"}]},{"title":"2022.06-2022.11实习总结","slug":"2022.06-2022.11实习总结","date":"2022-12-03T11:29:40.000Z","updated":"2023-09-24T04:27:40.272Z","comments":true,"path":"2022/12/03/2022.06-2022.11实习总结/","link":"","permalink":"https://zunpan.github.io/2022/12/03/2022.06-2022.11%E5%AE%9E%E4%B9%A0%E6%80%BB%E7%BB%93/","excerpt":"","text":"基于JSON Schema的配置平台 背景：这是公司的新人入职练手项目，总共两周时间，实现一个基于JSON Schema的表单配置平台。 需求：表单配置指的是通过可视化界面配置 表单的每个字段，包括名称、输入类型、输入限制等。mentor给了一个方向叫JSON Schema 调研：JSON Schema是描述json数据的元数据，本身也是json字符串，一般两个作用，1. 后端用 JSON Schema 对前端传的json传进行格式校验；2. 前端通过 JSON Schema生成表单 开发：前端使用Vue、后端使用Spring Boot 难点：JSON Schema和表单的双向转换。用户可以手动编辑JSON Schema生成表单项，也可以通过可视化界面增加表单项来修改JSON Schema。 解决方案：尝试过写一套解析方案，但是dom操作太复杂作罢。调研了一些开源方案，最终选用vue-json-schema-form Excel比对与合并系统 背景：接手的第一个项目，关于Excel比对与合并，主要参与系统的优化与维护工作 主要工作： 批量文件比对的多线程优化（比对方法涉及对象完全栈封闭） 批量文件合并OOM排查（合并需要先反序列化比对结果，若有多个版本的比对结果需要合并到另一分支上的同名文件，需要循环处理，每个比对结果合并之后需要置空，否则内存无法释放，排查工具：visualvm，发现调用合并方法时，Minor GC非常快，内存居高不下导致OOM；根本原因：自己开发的Excel解析工具+Java自带的序列化导致序列化产物非常大） 算法介绍可以参看《Excel比对与合并系统》 助理系统 需求：公司员工反馈行政问题都是在公司的聊天软件里反馈，反馈途径包括事业群、私聊行政助理、以及服务号反馈（类似微信公众号），行政助理需要在多个系统进行处理，助理系统为助理统一了消息来源，助理可以在助理系统中回复所有渠道的问题反馈。 调研：spring-boot-starter-websocket，实现了客户端与服务器全双工通信 难点：助理系统需要给每个反馈问题的员工生成唯一的对话，初次反馈消息时，快速发送消息会创建多个对话。这是因为后端多线程处理消息，每个线程都先去数据库查询此条消息的员工是否存在对话，如果不存在就创建。这里出现了并发经典错误 check-then-act。 解决方案：给会话表的员工id字段建唯一索引，插入新会话使用insert ignore。","categories":[{"name":"杂项","slug":"杂项","permalink":"https://zunpan.github.io/categories/%E6%9D%82%E9%A1%B9/"}],"tags":[{"name":"实习","slug":"实习","permalink":"https://zunpan.github.io/tags/%E5%AE%9E%E4%B9%A0/"}]},{"title":"Effective-Java学习笔记（九）","slug":"Effective-Java学习笔记（九）","date":"2022-08-13T08:11:11.000Z","updated":"2023-09-24T04:27:40.274Z","comments":true,"path":"2022/08/13/Effective-Java学习笔记（九）/","link":"","permalink":"https://zunpan.github.io/2022/08/13/Effective-Java%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B9%9D%EF%BC%89/","excerpt":"","text":"第十章 异常 69. 仅在确有异常条件下使用异常 有人认为用 try-catch 和 while(true) 遍历数组比用 for-each 性能更好，因为 for-each 由编译器隐藏了边界检查，而 try-catch 代码中不包含检查 // Horrible abuse of exceptions. Don&#x27;t ever do this! try &#123; int i = 0; while(true)&#123; range[i++].climb(); &#125; catch (ArrayIndexOutOfBoundsException e) &#123;&#125; &#125; 这个想法有三个误区： 因为异常是为特殊情况设计的，所以 JVM 实现几乎不会让它们像显式测试一样快。 将代码放在 try-catch 块中会抑制 JVM 可能执行的某些优化。 遍历数组的标准习惯用法不一定会导致冗余检查。许多 JVM 实现对它们进行了优化。 基于异常的循环除了不能提高性能外，还容易被异常隐藏循环中的 bug。因此，异常只适用于确有异常的情况；它们不应该用于一般的控制流程。 一个设计良好的 API 不能迫使其客户端为一般的控制流程使用异常。调用具有「状态依赖」方法的类，通常应该有一个单独的「状态测试」方法，表明是否适合调用「状态依赖」方法。例如，Iterator 接口具有「状态依赖」的 next 方法和对应的「状态测试」方法 hasNext。 for (Iterator&lt;Foo&gt; i = collection.iterator(); i.hasNext(); ) &#123; Foo foo = i.next(); ... &#125; 如果 Iterator 缺少 hasNext 方法，客户端将被迫这样做： // Do not use this hideous code for iteration over a collection! try &#123; Iterator&lt;Foo&gt; i = collection.iterator(); while(true) &#123; Foo foo = i.next(); ... &#125; &#125; catch (NoSuchElementException e) &#123; &#125; 这与一开始举例的对数组进行迭代的例子非常相似，除了冗长和误导之外，基于异常的循环执行效果可能很差，并且会掩盖系统中不相关部分的 bug。 提供单独的「状态测试」方法的另一种方式，就是让「状态依赖」方法返回一个空的 Optional 对象（Item-55），或者在它不能执行所需的计算时返回一个可识别的值，比如 null。 状态测试方法、Optional、可识别的返回值之间的选择如下： 如果要在没有外部同步的情况下并发地访问对象，或者受制于外部条件的状态转换，则必须使用 Optional 或可识别的返回值，因为对象的状态可能在调用「状态测试」方法与「状态依赖」方法的间隔中发生变化。 如果一个单独的「状态测试」方法重复「状态依赖」方法的工作，从性能问题考虑，可能要求使用 Optional 或可识别的返回值 在所有其他条件相同的情况下，「状态测试」方法略优于可识别的返回值。它提供了较好的可读性，而且不正确的使用可能更容易被检测：如果你忘记调用「状态测试」方法，「状态依赖」方法将抛出异常，使错误显而易见； 如果你忘记检查一个可识别的返回值，那么这个 bug 可能很难发现。但是这对于返回 Optional 对象的方式来说不是问题。 总之，异常是为确有异常的情况设计的。不要将它们用于一般的控制流程，也不要编写强制其他人这样做的 API。 70. 对可恢复情况使用 checked 异常，对编程错误使用运行时异常 Java 提供了三种可抛出项：checked 异常、运行时异常和错误。决定是使用 checked 异常还是 unchecked 异常的基本规则是：使用 checked 异常的情况是为了合理地期望调用者能够从中恢复。 有两种 unchecked 的可抛出项：运行时异常和错误。它们在行为上是一样的：都是可抛出的，通常不需要也不应该被捕获。如果程序抛出 unchecked 异常或错误，通常情况下是不可能恢复的，如果继续执行，弊大于利。如果程序没有捕获到这样的可抛出项，它将导致当前线程停止，并发出适当的错误消息。 运行时异常 运行时异常用来指示编程错误。大多数运行时异常都表示操作违反了先决条件。违反先决条件是指使用 API 的客户端未能遵守 API 规范所建立的约定。例如，数组访问约定指定数组索引必须大于等于 0 并且小于等于 length-1 （length：数组长度）。ArrayIndexOutOfBoundsException 表示违反了此先决条件 这个建议存在的问题是：并不总能清楚是在处理可恢复的条件还是编程错误。例如，考虑资源耗尽的情况，这可能是由编程错误（如分配一个不合理的大数组）或真正的资源短缺造成的。如果资源枯竭是由于暂时短缺或暂时需求增加造成的，这种情况很可能是可以恢复的。对于 API 设计人员来说，判断给定的资源耗尽实例是否允许恢复是一个问题。如果你认为某个条件可能允许恢复，请使用 checked 异常；如果没有，则使用运行时异常。如果不清楚是否可以恢复，最好使用 unchecked 异常 错误 虽然 Java 语言规范没有要求，但有一个约定俗成的约定，即错误保留给 JVM 使用，以指示：资源不足、不可恢复故障或其他导致无法继续执行的条件。考虑到这种约定被大众认可，所以最好不要实现任何新的 Error 子类。因此，你实现的所有 unchecked 异常都应该继承 RuntimeException（直接或间接）。不仅不应该定义 Error 子类，而且除了 AssertionError 之外，不应该抛出它们。 自定义异常 自定义异常继承 Throwable 类，Java 语言规范把它们当做普通 checked 异常（普通 checked 异常是 Exception 的子类，但不是 RuntimeException的子类）。不要使用自定义异常，它会让 API 的用户困惑 异常附加信息 API 设计人员常常忘记异常是成熟对象，可以为其定义任意方法。此类方法的主要用途是提供捕获异常的代码，并提供有关引发异常的附加信息。如果缺乏此类方法，程序员需要自行解析异常的字符串表示以获取更多信息。这是极坏的做法 因为 checked 异常通常表示可恢复的条件，所以这类异常来说，设计能够提供信息的方法来帮助调用者从异常条件中恢复尤为重要。例如，假设当使用礼品卡购物由于资金不足而失败时，抛出一个 checked 异常。该异常应提供一个访问器方法来查询差额。这将使调用者能够将金额传递给购物者。 总而言之，为可恢复条件抛出 checked 异常，为编程错误抛出 unchecked 异常。当有疑问时，抛出 unchecked 异常。不要定义任何既不是 checked 异常也不是运行时异常的自定义异常。应该为 checked 异常设计相关的方法，如提供异常信息，以帮助恢复。 71. 避免不必要地使用 checked 异常 合理抛出 checked 异常可以提高程序可靠性。过度使用会使得调用它的方法多次 try-catch 或抛出，给 API 用户带来负担，尤其是 Java8 中，抛出checked 异常的方法不能直接在流中使用。 只有在正确使用 API 也无法避免异常且使用 API 的程序员在遇到异常时可以采取一些有用的操作才能使用 checked 异常，否则抛出 unchecked 异常 如果 checked 异常是方法抛出的唯一 checked 异常，那么 checked 异常给程序员带来的额外负担就会大得多。如果还有其他 checked 异常，则该方法一定已经在 try 块中了，因此该异常最多需要另一个 catch 块而已。如果一个方法抛出单个 checked 异常，那么这个异常就是该方法必须出现在 try 块中而不能直接在流中使用的唯一原因。在这种情况下，有必要问问自己是否有办法避免 checked 异常。 消除 checked 异常的最简单方法是返回所需结果类型的 Optional 对象（Item-55）。该方法只返回一个空的 Optional 对象，而不是抛出一个 checked 异常。这种技术的缺点是，该方法不能返回任何详细说明其无法执行所需计算的附加信息。相反，异常具有描述性类型，并且可以导出方法来提供附加信息（Item-70） 总之，如果谨慎使用，checked 异常可以提高程序的可靠性；当过度使用时，它们会使 API 难以使用。如果调用者不应从失败中恢复，则抛出 unchecked 异常。如果恢复是可能的，并且你希望强制调用者处理异常情况，那么首先考虑返回一个 Optional 对象。只有当在失败的情况下，提供的信息不充分时，你才应该抛出一个 checked 异常。 72. 鼓励复用标准异常 Java 库提供了一组异常，涵盖了大多数 API 的大多数异常抛出需求。 复用标准异常有几个好处： 使你的 API 更容易学习和使用，因为它符合程序员已经熟悉的既定约定 使用你的 API 的程序更容易阅读，因为它们不会因为不熟悉的异常而混乱 更少的异常类意味着更小的内存占用和更少的加载类的时间 常见的被复用的异常： IllegalArgumentException。通常是调用者传入不合适的参数时抛出的异常 IllegalStateException。如果因为接收对象的状态导致调用非法，则通常会抛出此异常。例如，调用者试图在对象被正确初始化之前使用它 可以说，每个错误的方法调用都归结为参数非法或状态非法，但是有一些异常通常用于某些特定的参数非法和状态非法。如果调用者在禁止空值的参数中传递 null，那么按照惯例，抛出 NullPointerException 而不是 IllegalArgumentException。类似地，如果调用者将表示索引的参数中的超出范围的值传递给序列，则应该抛出 IndexOutOfBoundsException，而不是 IllegalArgumentException ConcurrentModificationException。如果一个对象被设计为由单个线程使用（或与外部同步），并且检测到它正在被并发地修改，则应该抛出该异常。因为不可能可靠地检测并发修改，所以该异常充其量只是一个提示。 UnsupportedOperationException。如果对象不支持尝试的操作，则抛出此异常。它很少使用，因为大多数对象都支持它们的所有方法。此异常用于一个类没有实现由其实现的接口定义的一个或多个可选操作。例如，对于只支持追加操作的 List 实现，试图从中删除元素时就会抛出这个异常 不要直接复用 Exception、RuntimeException、Throwable 或 Error 此表总结了最常见的可复用异常： Exception Occasion for Use IllegalArgumentException Non-null parameter value is inappropriate（非空参数值不合适） IllegalStateException Object state is inappropriate for method invocation（对象状态不适用于方法调用） NullPointerException Parameter value is null where prohibited（禁止参数为空时仍传入 null） IndexOutOfBoundsException Index parameter value is out of range（索引参数值超出范围） ConcurrentModificationException Concurrent modification of an object has been detected where it is prohibited（在禁止并发修改对象的地方检测到该动作） UnsupportedOperationException Object does not support method（对象不支持该方法调用） 其它异常如果有合适的复用场景也可以复用，例如，如果你正在实现诸如复数或有理数之类的算术对象，那么复用 ArithmeticException 和 NumberFormatException 是合适的 73. 抛出适合底层抽象异常的高层异常 当方法抛出一个与它所执行的任务没有明显关联的异常时，这是令人不安的。这种情况经常发生在由方法传播自低层抽象抛出的异常。它不仅令人不安，而且让实现细节污染了上层的 API。 为了避免这个问题，高层应该捕获低层异常，并确保抛出的异常可以用高层抽象解释。 这个习惯用法称为异常转换： // Exception Translation try &#123; ... // Use lower-level abstraction to do our bidding &#125; catch (LowerLevelException e) &#123; throw new HigherLevelException(...); &#125; 下面是来自 AbstractSequentialList 类的异常转换示例，该类是 List 接口的一个框架实现（Item-20）。在本例中，异常转换是由 List&lt;E&gt; 接口中的 get 方法规范强制执行的： /** * Returns the element at the specified position in this list. * @throws IndexOutOfBoundsException if the index is out of range * (&#123;@code index &lt; 0 || index &gt;= size()&#125;). */ public E get(int index) &#123; ListIterator&lt;E&gt; i = listIterator(index); try &#123; return i.next(); &#125; catch (NoSuchElementException e) &#123; throw new IndexOutOfBoundsException(&quot;Index: &quot; + index); &#125; &#125; 如果低层异常可能有助于调试高层异常的问题，则需要一种称为链式异常的特殊异常转换形式。低层异常（作为原因）传递给高层异常，高层异常提供一个访问器方法（Throwable 的 getCause 方法）来检索低层异常： // Exception Chaining try &#123; ... // Use lower-level abstraction to do our bidding &#125; catch (LowerLevelException cause) &#123; throw new HigherLevelException(cause); &#125; 高层异常的构造函数将原因传递给能够接收链式异常的父类构造函数，因此它最终被传递给 Throwable 的一个接收链式异常的构造函数，比如 Throwable(Throwable)： // Exception with chaining-aware constructor class HigherLevelException extends Exception &#123; HigherLevelException(Throwable cause) &#123; super(cause); &#125; &#125; 大多数标准异常都有接收链式异常的构造函数。对于不支持链式异常的异常，可以使用 Throwable 的 initCause 方法设置原因。异常链不仅允许你以编程方式访问原因（使用 getCause），而且还将原因的堆栈跟踪集成到更高层异常的堆栈跟踪中。 虽然异常转换优于底层异常的盲目传播，但它不应该被过度使用。在可能的情况下，处理底层异常的最佳方法是确保底层方法避免异常。有时，你可以在将高层方法的参数传递到底层之前检查它们的有效性。 如果不可能从底层防止异常，那么下一个最好的方法就是让高层静默处理这些异常，使较高层方法的调用者免受底层问题的影响。在这种情况下，可以使用一些适当的日志工具（如 java.util.logging）来记录异常。这允许程序员研究问题，同时将客户端代码和用户与之隔离。 总之，如果无法防止或处理来自底层的异常，则使用异常转换，但要保证底层方法的所有异常都适用于较高层。链式异常提供了兼顾两方面的最佳服务：允许抛出适当的高层异常，同时捕获并分析失败的潜在原因 74. 为每个方法记录会抛出的所有异常 始终单独声明 checked 异常，并使用 Javadoc 的 @throw 标记精确记录每次抛出异常的条件。如果一个方法抛出多个异常，不要使用快捷方式声明这些异常的父类。作为一个极端的例子，即不要在公共方法声明 throws Exception，除了只被 JVM 调用的 main方法 unchecked 异常不要声明 throws，但应该像 checked 异常一样用 Javadoc 记录他们。特别是接口中的方法要记录可能抛出的 unchecked 异常。 如果一个类中的许多方法都因为相同的原因抛出异常，你可以在类的文档注释中记录异常， 而不是为每个方法单独记录异常。一个常见的例子是 NullPointerException。类的文档注释可以这样描述：「如果在任何参数中传递了 null 对象引用，该类中的所有方法都会抛出 NullPointerException」 总之，记录你所编写的每个方法可能引发的每个异常。对于 unchecked 异常、checked 异常、抽象方法、实例方法都是如此。应该在文档注释中采用 @throw 标记的形式。在方法的 throws 子句中分别声明每个 checked 异常，但不要声明 unchecked 异常。如果你不记录方法可能抛出的异常，其他人将很难或不可能有效地使用你的类和接口。 75. 异常详细消息中应包含捕获失败的信息 程序因为未捕获异常而失败时，系统会自动调用异常的 toString 方法打印堆栈信息，堆栈信息包含异常的类名及详细信息。异常的详细消息应该包含导致异常的所有参数和字段的值。例如，IndexOutOfBoundsException 的详细消息应该包含下界、上界和未能位于下界之间的索引值。 异常详细信息不用过于冗长，程序失败时可以通过阅读文档和源代码收集信息，确保异常包含足够的信息的一种方法是在构造函数中配置异常信息，例如，IndexOutOfBoundsException 构造函数不包含 String 参数，而是像这样： /** * Constructs an IndexOutOfBoundsException. ** @param lowerBound the lowest legal index value * @param upperBound the highest legal index value plus one * @param index the actual index value */ public IndexOutOfBoundsException(int lowerBound, int upperBound, int index) &#123; // Generate a detail message that captures the failure super(String.format(&quot;Lower bound: %d, Upper bound: %d, Index: %d&quot;,lowerBound, upperBound, index)); // Save failure information for programmatic access this.lowerBound = lowerBound; this.upperBound = upperBound; this.index = index; &#125; 76. 尽力保证故障原子性 失败的方法调用应该使对象处于调用之前的状态。 具有此属性的方法称为具备故障原子性。 有几种实现故障原子性的方式： 关于不可变对象 不可变对象在创建后永远处于一致状态 关于可变对象： 在修改状态前，先执行可能抛出异常的操作，例如检查状态，不合法就抛出异常 在临时副本上操作，成功后替换原来的对象，不成功不影响原来的对象。例如，一些排序函数会将入参 list 复制到数组中，对数组进行排序，再转换成 list 编写回滚代码，主要用于持久化的数据 有些情况是不能保证故障原子性的，例如，多线程不同步修改对象，对象可能处于不一致状态，当捕获到 ConcurrentModificationException 后对象不可恢复 总之，作为方法规范的一部分，生成的任何异常都应该使对象保持在方法调用之前的状态。如果违反了这条规则，API 文档应该清楚地指出对象将处于什么状态。 77. 不要忽略异常 异常要么 try-catch 要么抛出，不要写空的 catch 块，如果这样做，写上注释","categories":[{"name":"Java","slug":"Java","permalink":"https://zunpan.github.io/categories/Java/"}],"tags":[{"name":"Effective-Java","slug":"Effective-Java","permalink":"https://zunpan.github.io/tags/Effective-Java/"},{"name":"异常","slug":"异常","permalink":"https://zunpan.github.io/tags/%E5%BC%82%E5%B8%B8/"}]},{"title":"Effective-Java学习笔记（八）","slug":"Effective-Java学习笔记（八）","date":"2022-08-06T07:30:09.000Z","updated":"2023-09-24T04:27:40.275Z","comments":true,"path":"2022/08/06/Effective-Java学习笔记（八）/","link":"","permalink":"https://zunpan.github.io/2022/08/06/Effective-Java%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%85%AB%EF%BC%89/","excerpt":"","text":"第九章 通用程序设计 57. 将局部变量的作用域最小化 本条目在性质上类似于Item-15，即「最小化类和成员的可访问性」。通过最小化局部变量的范围，可以提高代码的可读性和可维护性，并降低出错的可能性。 将局部变量的作用域最小化，最具说服力的方式就是在第一次使用它的地方声明 每个局部变量声明都应该包含一个初始化表达式。 如果你还没有足够的信息来合理地初始化一个变量，你应该推迟声明，直到条件满足 58. for-each循环优于for循环 for-each 更简洁更不容易出错，且没有性能损失，只有三种情况不能用for-each 破坏性过滤。如果需要遍历一个集合并删除选定元素，则需要使用显式的迭代器，以便调用其 remove 方法。通过使用 Collection 在 Java 8 中添加的 removeIf 方法，通常可以避免显式遍历。 转换。如果需要遍历一个 List 或数组并替换其中部分或全部元素的值，那么需要 List 迭代器或数组索引来替换元素的值。 并行迭代。如果需要并行遍历多个集合，那么需要显式地控制迭代器或索引变量，以便所有迭代器或索引变量都可以同步执行 59. 了解并使用库 假设你想要生成 0 到某个上界之间的随机整数，有些程序员会写出如下代码： // Common but deeply flawed! static Random rnd = new Random(); static int random(int n) &#123; return Math.abs(rnd.nextInt()) % n; &#125; 这个方法有三个缺点： 如果 n 是比较小的 2 的幂，随机数序列会在相当短的时间内重复 如果 n 不是 2 的幂，一些数字出现的频率会更高，当 n 很大时效果明显 会返回超出指定范围的数字，当nextInt返回Integer.MIN_VALUE时，abs方法也会返回Integer.MIN_VALUE，假设 n 不是 2 幂，那么Integer.MIN_VALUE % n 将返回负数 我们不需要为这个需求自己编写方法，已经存在经过专家设计测试的标准库Random 的 nextInt(int) 从 Java 7 开始，就不应该再使用 Random。在大多数情况下，选择的随机数生成器现在是 ThreadLocalRandom。 它能产生更高质量的随机数，而且速度非常快。 总而言之，不要白费力气重新发明轮子。如果你需要做一些看起来相当常见的事情，那么库中可能已经有一个工具可以做你想做的事情。如果有，使用它；如果你不知道，查一下。一般来说，库代码可能比你自己编写的代码更好，并且随着时间的推移可能会得到改进。 60. 若需要精确答案就应避免使用 float 和 double 类型 float 和 double 类型主要用于科学计算和工程计算。它们执行二进制浮点运算，该算法经过精心设计，能够在很大范围内快速提供精确的近似值。但是，它们不能提供准确的结果。float 和 double 类型特别不适合进行货币计算，因为不可能将 0.1（或 10 的任意负次幂）精确地表示为 float 或 double 正确做法是使用 BigDecimal、int 或 long 进行货币计算。还要注意 BigDecimal 的构造函数要使用String参数而不是double，避免初始化时就用了不精确的值。int和long可以存储较小单位的值，将小数转换成整数存储 总之，对于任何需要精确答案的计算，不要使用 float 或 double 类型。如果希望系统来处理十进制小数点，并且不介意不使用基本类型带来的不便和成本，请使用 BigDecimal。使用 BigDecimal 的另一个好处是，它可以完全控制舍入，当执行需要舍入的操作时，可以从八种舍入模式中进行选择。如果你使用合法的舍入行为执行业务计算，这将非常方便。如果性能是最重要的，那么你不介意自己处理十进制小数点，而且数值不是太大，可以使用 int 或 long。如果数值不超过 9 位小数，可以使用 int；如果不超过 18 位，可以使用 long。如果数量可能超过 18 位，则使用 BigDecimal。 61. 基本数据类型优于包装类 Java 的类型系统有两部分，基本类型（如 int、double 和 boolean）和引用类型（如String和List）。每个基本类型都有一个对应的引用类型，称为包装类。如Integer、Double 和 Boolean 基本类型和包装类型之间的区别如下 基本类型只有它们的值，而包装类型具有与其值不同的标识。换句话说，两个包装类型实例可以具有相同的值和不同的标识。 基本类型只有全部功能值，而每个包装类型除了对应的基本类型的所有功能值外，还有一个非功能值，即 null 基本类型比包装类型更节省时间和空间 用 == 来比较包装类型几乎都是错的 在操作中混用基本类型和包装类型时，包装类型会自动拆箱，可能导致 NPE，如果操作结果保存到包装类型的变量中，还会发生自动装箱，导致性能问题 包装类型的用途如下： 作为集合的元素、键和值 泛型和泛型方法中的类型参数 反射调用 总之，只要有选择，就应该优先使用基本类型，而不是包装类型。基本类型更简单、更快。如果必须使用包装类型，请小心！自动装箱减少了使用包装类型的冗长，但没有减少危险。 当你的程序使用 == 操作符比较两个包装类型时，它会执行标识比较，这几乎肯定不是你想要的。当你的程序执行包含包装类型和基本类型的混合类型计算时，它将进行拆箱，当你的程序执行拆箱时，将抛出 NullPointerException。 最后，当你的程序将基本类型装箱时，可能会导致代价高昂且不必要的对象创建。 62. 其它类型更合适时应避免使用字符串 本条目讨论一些不应该使用字符串的场景 字符串是枚举类型的糟糕替代品 字符串是聚合类型的糟糕替代品。如果一个对象有多个字段，将其连接成一个字符串会出现许多问题，更好的方法是用私有静态成员类表示聚合 字符串是功能的糟糕替代品。例如全局缓存池要求 key 不能重复，如果用字符串做 key 可能在不同线程中出现问题 总之，当存在或可以编写更好的数据类型时，应避免将字符串用来表示对象。如果使用不当，字符串比其他类型更麻烦、灵活性更差、速度更慢、更容易出错。字符串经常被误用的类型包括基本类型、枚举和聚合类型。 63. 当心字符串连接引起的性能问题 字符串连接符（+）连接 n 个字符串的时间复杂度是 n2n^2n2。这是字符串不可变导致的 如果要连接的字符串数量较多，可以使用 StringBuilder 代替 String 64. 通过接口引用对象 如果存在合适的接口类型，那么应该使用接口类型声明参数、返回值、变量和字段。好处是代码可以非常方便切换性能更好或功能更丰富的实现，例如 HashMap 替换成 LinkedHashMap 可以保证迭代顺序和插入一致 如果没有合适的接口存在，那么用类引用对象是完全合适的。 值类，如 String 和 BigInteger。值类很少在编写时考虑到多个实现。它们通常是 final 的，很少有相应的接口。使用这样的值类作为参数、变量、字段或返回类型非常合适。 如果一个对象属于一个基于类的框架，那么就用基类引用它 接口的实现类有接口没有的方法，例如 PriorityQueue 类有 Queue 接口没有的比较器方法 65. 接口优于反射 反射有几个缺点： 失去了编译时类型检查的所有好处，包括异常检查。如果一个程序试图反射性地调用一个不存在的或不可访问的方法，它将在运行时失败，除非你采取了特殊的预防措施(大量 try-catch) 反射代码既笨拙又冗长。写起来很乏味，读起来也很困难 反射调用方法比普通调用方法更慢 反射优点： 对于许多程序，它们必须用到在编译时无法获取的类，在编译时存在一个适当的接口或父类来引用该类（Item-64）。如果是这种情况，可以用反射方式创建实例，并通过它们的接口或父类正常地访问它们。 例如，这是一个创建 Set&lt;String&gt; 实例的程序，类由第一个命令行参数指定。程序将剩余的命令行参数插入到集合中并打印出来。不管第一个参数是什么，程序都会打印剩余的参数，并去掉重复项。然而，打印这些参数的顺序取决于第一个参数中指定的类。如果你指定 java.util.HashSet，它们显然是随机排列的；如果你指定 java.util.TreeSet，它们是按字母顺序打印的，因为 TreeSet 中的元素是有序的 // Reflective instantiation with interface access public static void main(String[] args) &#123; // Translate the class name into a Class object Class&lt;? extends Set&lt;String&gt;&gt; cl = null; try &#123; cl = (Class&lt;? extends Set&lt;String&gt;&gt;) // Unchecked cast! Class.forName(args[0]); &#125; catch (ClassNotFoundException e) &#123; fatalError(&quot;Class not found.&quot;); &#125; // Get the constructor Constructor&lt;? extends Set&lt;String&gt;&gt; cons = null; try &#123; cons = cl.getDeclaredConstructor(); &#125; catch (NoSuchMethodException e) &#123; fatalError(&quot;No parameterless constructor&quot;); &#125; // Instantiate the set Set&lt;String&gt; s = null; try &#123; s = cons.newInstance(); &#125; catch (IllegalAccessException e) &#123; fatalError(&quot;Constructor not accessible&quot;); &#125; catch (InstantiationException e) &#123; fatalError(&quot;Class not instantiable.&quot;); &#125; catch (InvocationTargetException e) &#123; fatalError(&quot;Constructor threw &quot; + e.getCause()); &#125; catch (ClassCastException e) &#123; fatalError(&quot;Class doesn&#x27;t implement Set&quot;); &#125; // Exercise the set s.addAll(Arrays.asList(args).subList(1, args.length)); System.out.println(s); &#125; private static void fatalError(String msg) &#123; System.err.println(msg); System.exit(1); &#125; 反射的合法用途（很少）是管理类对运行时可能不存在的其他类、方法或字段的依赖关系。如果你正在编写一个包，并且必须针对其他包的多个版本运行，此时反射将非常有用。该技术是根据支持包所需的最小环境（通常是最老的版本）编译包，并反射性地访问任何较新的类或方法。如果你试图访问的新类或方法在运行时不存在，要使此工作正常进行，则必须采取适当的操作。适当的操作可能包括使用一些替代方法来完成相同的目标，或者使用简化的功能进行操作 总之，反射是一种功能强大的工具，对于某些复杂的系统编程任务是必需的，但是它有很多缺点。如果编写的程序必须在编译时处理未知的类，则应该尽可能只使用反射实例化对象，并使用在编译时已知的接口或父类访问对象。 66. 明智地使用本地方法 JNI 允许 Java 调用本地方法，这些方法是用 C 或 C++ 等本地编程语言编写的。 历史上，本地方法主要有三个用途： 提供对特定于平台的设施（如注册中心）的访问 提供对现有本地代码库的访问，包括提供对遗留数据访问 通过本地语言编写应用程序中注重性能的部分，以提高性能 关于用途一：随着 Java 平台的成熟，它提供了对许多以前只能在宿主平台中上找到的特性。例如，Java 9 中添加的流 API 提供了对 OS 进程的访问。在 Java 中没有等效库时，使用本地方法来使用本地库也是合法的。 关于用途3：在早期版本（Java 3 之前），这通常是必要的，但是从那时起 JVM 变得更快了。对于大多数任务，现在可以在 Java 中获得类似的性能 本地方法的缺点： 会受到内存损坏错误的影响 垃圾收集器无法自动追踪本地方法的内存使用情况，导致性能下降 总之，在使用本地方法之前要三思。一般很少需要使用它们来提高性能。如果必须使用本地方法来访问底层资源或本地库，请尽可能少地使用本地代码，并对其进行彻底的测试。本地代码中的一个错误就可以破坏整个应用程序。 67. 明智地进行优化 关于优化的三条名言 以效率的名义(不一定能达到效率)犯下的计算错误比任何其他原因都要多——包括盲目的愚蠢 不要去计较效率上的一些小小的得失，在 97% 的情况下，不成熟的优化才是一切问题的根源。 在优化方面，我们应该遵守两条规则：规则 1：不要进行优化。规则 2 （仅针对专家）：还是不要进行优化，也就是说，在你还没有绝对清晰的未优化方案之前，请不要进行优化。 在设计系统时，我们要仔细考虑架构，好的架构允许它在后面优化，不良的架构导致很难优化。设计中最难更改的是组件之间以及组件与外部世界交互的组件，主要是 API、线路层协议和数据持久化格式，尽量避免做限制性能的设计 JMH 是一个微基准测试框架，主要是基于方法层面的基准测试，精度可以达到纳秒级 总而言之，不要努力写快的程序，要努力写好程序；速度自然会提高。但是在设计系统时一定要考虑性能，特别是在设计API、线路层协议和持久数据格式时。当你完成了系统的构建之后，请度量它的性能。如果足够快，就完成了。如果没有，利用分析器找到问题的根源，并对系统的相关部分进行优化。第一步是检查算法的选择：再多的底层优化也不能弥补算法选择的不足。根据需要重复这个过程，在每次更改之后测量性能，直到你满意为止。 68. 遵守被广泛认可的命名约定 Java 平台有一组完善的命名约定，其中许多约定包含在《The Java Language Specification》。不严格地讲，命名约定分为两类：排版和语法。 排版 和排版有关的命名约定，包括包、类、接口、方法、字段和类型变量 包名和模块名应该是分层的，组件之间用句点分隔。组件应该由小写字母组成，很少使用数字。任何在你的组织外部使用的包，名称都应该以你的组织的 Internet 域名开头，并将组件颠倒过来，例如，edu.cmu、com.google、org.eff。以 java 和 javax 开头的标准库和可选包是这个规则的例外。用户不能创建名称以 java 或 javax 开头的包或模块。 包名的其余部分应该由描述包的一个或多个组件组成。组件应该很短，通常为 8 个或更少的字符。鼓励使用有意义的缩写，例如 util 而不是 utilities。缩写词是可以接受的，例如 awt。组件通常应该由一个单词或缩写组成。 类和接口名称，包括枚举和注释类型名称，应该由一个或多个单词组成，每个单词的首字母大写，例如 List 或 FutureTask 方法和字段名遵循与类和接口名相同的排版约定，除了方法或字段名的第一个字母应该是小写，例如 remove 或 ensureCapacity 前面规则的唯一例外是「常量字段」，它的名称应该由一个或多个大写单词组成，由下划线分隔，例如 VALUES 或 NEGATIVE_INFINITY 局部变量名与成员名具有类似的排版命名约定，但允许使用缩写，也允许使用单个字符和短字符序列，它们的含义取决于它们出现的上下文，例如 i、denom、houseNum。输入参数是一种特殊的局部变量。它们的命名应该比普通的局部变量谨慎得多，因为它们的名称是方法文档的组成部分。 类型参数名通常由单个字母组成。最常见的是以下五种类型之一：T 表示任意类型，E 表示集合的元素类型，K 和 V 表示 Map 的键和值类型，X 表示异常。函数的返回类型通常为 R。任意类型的序列可以是 T、U、V 或 T1、T2、T3。 为了快速参考，下表显示了排版约定的示例。 Identifier Type Example Package or module org.junit.jupiter.api, com.google.common.collect Class or Interface Stream, FutureTask, LinkedHashMap,HttpClient Method or Field remove, groupingBy, getCrc Constant Field MIN_VALUE, NEGATIVE_INFINITY Local Variable i, denom, houseNum Type Parameter T, E, K, V, X, R, U, V, T1, T2 语法 语法命名约定比排版约定更灵活，也更有争议 可实例化的类，包括枚举类型，通常使用一个或多个名词来命名，例如 Thread、PriorityQueue 或 ChessPiece 不可实例化的工具类通常用复数名词来命名，例如 Collectors 和 Collections 接口的名称类似于类，例如 Collection 或 Comparator，或者以 able 或 ible 结尾的形容词，例如 Runnable、Iterable 或 Accessible 因为注解类型有很多的用途，所以没有哪部分占主导地位。名词、动词、介词和形容词都很常见，例如，BindingAnnotation、Inject、ImplementedBy 或 Singleton。 执行某些操作的方法通常用动词或动词短语（包括对象）命名，例如，append 或 drawImage。 返回布尔值的方法的名称通常以单词 is 或 has（通常很少用）开头，后面跟一个名词、一个名词短语，或者任何用作形容词的单词或短语，例如 isDigit、isProbablePrime、isEmpty、isEnabled 或 hasSiblings。 返回被调用对象的非布尔函数或属性的方法通常使用以 get 开头的名词、名词短语或动词短语来命名，例如 size、hashCode 或 getTime。有一种说法是，只有第三种形式（以 get 开头）才是可接受的，但这种说法几乎没有根据。前两种形式的代码通常可读性更强 。以 get 开头的形式起源于基本过时的 Java bean 规范，该规范构成了早期可复用组件体系结构的基础。有一些现代工具仍然依赖于 bean 命名约定，你应该可以在任何与这些工具一起使用的代码中随意使用它。如果类同时包含相同属性的 setter 和 getter，则遵循这种命名约定也有很好的先例。在本例中，这两个方法通常被命名为 getAttribute 和 setAttribute。 转换对象类型（返回不同类型的独立对象）的实例方法通常称为 toType，例如 toString 或 toArray。 返回与接收对象类型不同的视图（Item-6）的方法通常称为 asType，例如 asList 返回与调用它们的对象具有相同值的基本类型的方法通常称为类型值，例如 intValue 静态工厂的常见名称包括 from、of、valueOf、instance、getInstance、newInstance、getType 和 newType 字段名的语法约定没有类、接口和方法名的语法约定建立得好，也不那么重要，因为设计良好的 API 包含很少的公开字段。类型为 boolean 的字段的名称通常类似于 boolean 访问器方法，省略了开头「is」，例如 initialized、composite。其他类型的字段通常用名词或名词短语来命名，如 height、digits 和 bodyStyle。局部变量的语法约定类似于字段的语法约定，但要求更少。","categories":[{"name":"Java","slug":"Java","permalink":"https://zunpan.github.io/categories/Java/"}],"tags":[{"name":"Effective-Java","slug":"Effective-Java","permalink":"https://zunpan.github.io/tags/Effective-Java/"},{"name":"通用程序设计","slug":"通用程序设计","permalink":"https://zunpan.github.io/tags/%E9%80%9A%E7%94%A8%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/"}]},{"title":"Effective-Java学习笔记（七）","slug":"Effective-Java学习笔记（七）","date":"2022-08-01T11:27:09.000Z","updated":"2023-09-24T04:27:40.273Z","comments":true,"path":"2022/08/01/Effective-Java学习笔记（七）/","link":"","permalink":"https://zunpan.github.io/2022/08/01/Effective-Java%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%83%EF%BC%89/","excerpt":"","text":"第八章 方法 49. 检查参数的有效性 大多数方法和构造函数都对入参有一些限制，例如非空非负，你应该在方法主体的开头检查参数并在Javadoc中用@throws记录参数限制 Java7 添加了 Objects.requireNonNull 方法执行空检查 Java9 在 Objects 中添加了范围检查：checkFromIndexSize、checkFromToIndex 和 checkIndex 对于未导出的方法，作为包的作者，你应该定制方法调用的环境，确保只传递有效的参数值。因此，非 public 方法可以使用断言检查参数 // Private helper function for a recursive sort private static void sort(long a[], int offset, int length) &#123; assert a != null; assert offset &gt;= 0 &amp;&amp; offset &lt;= a.length; assert length &gt;= 0 &amp;&amp; length &lt;= a.length - offset; ... // Do the computation &#125; 断言只适合用来调试，可以关闭可以打开，public 方法一定要显示检查参数并抛出异常，断言失败只会抛出AssertionError，不利于定位错误 总而言之，每次编写方法或构造函数时，都应该考虑参数存在哪些限制。你应该在文档中记录这些限制，并在方法主体的开头显式地检查。 50. 在需要时制作防御性副本 Java 是一种安全的语言，这是它的一大优点。这意味着在没有 native 方法的情况下，它不受缓冲区溢出、数组溢出、非法指针和其他内存损坏错误的影响，这些错误困扰着 C 和 C++ 等不安全语言。在一种安全的语言中，可以编写一个类并确定它们的不变量将保持不变，而不管在系统的任何其他部分发生了什么。在将所有内存视为一个巨大数组的语言中，这是不可能的。 即使使用一种安全的语言，如果你不付出一些努力，也无法与其他类隔离。你必须进行防御性的设计，并假定你的类的客户端会尽最大努力破坏它的不变量。 通过可变参数破坏类的不变量 虽然如果没有对象的帮助，另一个类是不可能修改对象的内部状态的，但是要提供这样的帮助却出奇地容易。例如，考虑下面的类，它表示一个不可变的时间段： // Broken &quot;immutable&quot; time period class public final class Period &#123; private final Date start; private final Date end; /** * @param start the beginning of the period * @param end the end of the period; must not precede start * @throws IllegalArgumentException if start is after end * @throws NullPointerException if start or end is null */ public Period(Date start, Date end) &#123; if (start.compareTo(end) &gt; 0) throw new IllegalArgumentException(start + &quot; after &quot; + end); this.start = start; this.end = end; &#125; public Date start() &#123; return start; &#125; public Date end() &#123; return end; &#125; ... // Remainder omitted &#125; 乍一看，这个类似乎是不可变的，并且要求一个时间段的开始时间不能在结束时间之后。然而，利用 Date 是可变的这一事实很容易绕过这个约束： // Attack the internals of a Period instance Date start = new Date(); Date end = new Date(); Period p = new Period(start, end); end.setYear(78); // Modifies internals of p! 从 Java8 开始，解决这个问题可以用 Instant 或 LocalDateTime 或 ZonedDateTime 来代替 Date，因为它们是不可变类。 防御性副本 但是有时必须在 API 和内部表示中使用可变值类型，这时候可以对可变参数进行防御性副本而不是使用原始可变参数。对上面的 Period 类改进如下 // Repaired constructor - makes defensive copies of parameters public Period(Date start, Date end) &#123; this.start = new Date(start.getTime()); this.end = new Date(end.getTime()); if (this.start.compareTo(this.end) &gt; 0) throw new IllegalArgumentException(this.start + &quot; after &quot; + this.end); &#125; 先进行防御性复制，再在副本上检查参数，保证在检查参数和复制参数之间的空窗期，类不受其他线程更改参数的影响，这个攻击也叫time-of-check/time-of-use 或 TOCTOU 攻击 对可被不受信任方子类化的参数类型，不要使用 clone 方法进行防御性复制。 访问器也要对可变字段进行防御性复制 如果类信任它的调用者不会破坏不变量，比如类和调用者都是同一个包下，那么应该避免防御性复制 当类的作用就是修改可变参数时不用防御性复制，客户端承诺不直接修改对象 破坏不变量只会对损害客户端时不用防御性复制，例如包装类模式，客户端在包装对象之后可以直接访问对象，破坏类的不变量，但这通常只会损害客户端 总而言之，如果一个类具有从客户端获取或返回给客户端的可变组件，则该类必须防御性地复制这些组件。如果复制的成本过高，并且类信任它的客户端不会不适当地修改组件，那么可以不进行防御性的复制，取而代之的是在文档中指明客户端的职责是不得修改受到影响的组件。 51. 仔细设计方法签名 仔细选择方法名称。目标是选择可理解的、与同一包中其它名称风格一致的名称；选择广泛认可的名字；避免长方法名 不要提供过于便利的方法。每种方法都应该各司其职。太多的方法使得类难以学习、使用、记录、测试和维护。对于接口来说更是如此，在接口中，太多的方法使实现者和用户的工作变得复杂。对于类或接口支持的每个操作，请提供一个功能齐全的方法。 避免长参数列表。可以通过分解方法减少参数数量；也可以通过静态成员类 helper 类来存参数；也可以从对象构建到方法调用都采用建造者模式 参数类型优先选择接口而不是类 双元素枚举类型优于 boolean 参数。枚举比 boolean 可读性强且可以添加更多选项 52. 明智地使用重载 考虑下面使用了重载的代码： // Broken! - What does this program print? public class CollectionClassifier &#123; public static String classify(Set&lt;?&gt; s) &#123; return &quot;Set&quot;; &#125; public static String classify(List&lt;?&gt; lst) &#123; return &quot;List&quot;; &#125; public static String classify(Collection&lt;?&gt; c) &#123; return &quot;Unknown Collection&quot;; &#125; public static void main(String[] args) &#123; Collection&lt;?&gt;[] collections = &#123; new HashSet&lt;String&gt;(),new ArrayList&lt;BigInteger&gt;(),new HashMap&lt;String, String&gt;().values() &#125;; for (Collection&lt;?&gt; c : collections) System.out.println(classify(c)); &#125; &#125; 这段代码打印了三次 Unknown Collection。因为 classify 方法被重载，并且在编译时就决定了要调用哪个重载，编译时是 Collections&lt;?&gt; 类型，所以调用的就是第三个重载方法 重载VS覆盖 重载方法的选择是静态的，在编译时决定要调用哪个重载方法。而覆盖方法的选择是动态的， 在运行时根据调用方法的对象的运行时类型选择覆盖方法的正确版本 因为覆盖是常态，而重载是例外，所以覆盖满足了人们对方法调用行为的期望，重载很容易混淆这些期望。 重载 安全、保守的策略是永远不导出具有相同数量参数的两个重载。你可以为方法提供不同的名称而不是重载它们。 构造函数只能重载，我们可以用静态工厂代替构造函数 不要在重载方法的相同参数位置上使用不同的函数式接口。不同的函数式接口并没有本质的不同 总而言之，方法可以重载，但并不意味着就应该这样做。通常，最好避免重载具有相同数量参数的多个签名的方法。在某些情况下，特别是涉及构造函数的情况下，可能难以遵循这个建议。在这些情况下，你至少应该避免同一组参数只需经过类型转换就可以被传递给不同的重载方法。如果这是无法避免的，例如，因为要对现有类进行改造以实现新接口，那么应该确保在传递相同的参数时，所有重载的行为都是相同的。如果你做不到这一点，程序员将很难有效地使用重载方法或构造函数，他们将无法理解为什么它不能工作。 53. 明智地使用可变参数 可变参数首先创建一个数组，其大小是在调用点上传递的参数数量，然后将参数值放入数组，最后将数组传递给方法。 当你需要定义具有不确定数量参数的方法时，可变参数是非常有用的。在可变参数之前加上任何必需的参数，并注意使用可变参数可能会引发的性能后果。 54. 返回空集合或数组，而不是 null 在方法中用空集合或空数组代替 null 返回可以让客户端不用显示判空 55. 明智地返回 Optional 在 Java8 之前，方法可能无法 return 时有两种处理方法，一种是抛异常，一种是 返回 null。抛异常代价高，返回 null 需要客户端显示判空 Java8 添加了第三种方法来处理可能无法返回值的方法。Optional&lt;T&gt; 类表示一个不可变的容器，它可以包含一个非空的 T 引用，也可以什么都不包含。不包含任何内容的 Optional 被称为空。一个值被认为存在于一个非空的 Optional 中。Optional 的本质上是一个不可变的集合，它最多可以容纳一个元素。 理论上应返回 T，但在某些情况下可能无法返回 T 的方法可以将返回值声明为 Optional&lt;T&gt;。这允许该方法返回一个空结果来表明它不能返回有效的结果。具备 Optional 返回值的方法比抛出异常的方法更灵活、更容易使用，并且比返回 null 的方法更不容易出错。 Item-30 有一个求集合最大值方法 // Returns maximum value in collection - throws exception if empty public static &lt;E extends Comparable&lt;E&gt;&gt; E max(Collection&lt;E&gt; c) &#123; if (c.isEmpty()) throw new IllegalArgumentException(&quot;Empty collection&quot;); E result = null; for (E e : c) if (result == null || e.compareTo(result) &gt; 0) result = Objects.requireNonNull(e); return result; &#125; 当入参集合为空时，这个方法会抛出 IllegalArgumentException。更好的方式是返回 Optional // Returns maximum value in collection as an Optional&lt;E&gt; public static &lt;E extends Comparable&lt;E&gt;&gt; Optional&lt;E&gt; max(Collection&lt;E&gt; c) &#123; if (c.isEmpty()) return Optional.empty(); E result = null; for (E e : c) if (result == null || e.compareTo(result) &gt; 0) result = Objects.requireNonNull(e); return Optional.of(result); &#125; Optional.empty() 返回一个空的 Optional，Optional.of(value) 返回一个包含给定非空值的 Optional。将 null 传递给 Optional.of(value) 是一个编程错误。如果你这样做，该方法将通过抛出 NullPointerException 来响应。Optional.ofNullable(value) 方法接受一个可能为空的值，如果传入 null，则返回一个空的 Optional。永远不要让返回 optional 的方法返回 null : 它违背了这个功能的设计初衷。 为什么选择返回 Optional 而不是返回 null 或抛出异常？Optional 在本质上类似于受检查异常（Item-71），因为它们迫使 API 的用户面对可能没有返回值的事实。抛出不受检查的异常或返回 null 允许用户忽略这种可能性，抛出受检查异常会让客户端添加额外代码 如果一个方法返回一个 Optional，客户端可以选择如果该方法不能返回值该采取什么操作。你可以使用 orElse 方法指定一个默认值，或者使用 orElseGet 方法在必要时生成默认值；也可以使用 orElseThrow 方法抛出异常 关于 Optional 用法的一些Tips： isPresent 方法可以判断 Optional中有没有值，谨慎使用 isPrensent 方法，它的许多用途可以用上面的方法代替 并不是所有的返回类型都能从 Optional 处理中获益。容器类型，包括集合、Map、流、数组和 Optional，不应该封装在 Optional 中。 你应该简单的返回一个空的 List&lt;T&gt;，而不是一个空的 Optional&lt;List&lt;T&gt;&gt; 永远不应该返回包装类的 Optional，除了「次基本数据类型」，如 Boolean、Byte、Character、Short 和 Float 之外 在集合或数组中使用 Optional 作为键、值或元素几乎都是不合适的。 总之，如果你发现自己编写的方法不能总是返回确定值，并且你认为该方法的用户在每次调用时应该考虑这种可能性，那么你可能应该让方法返回一个 Optional。但是，你应该意识到，返回 Optional 会带来实际的性能后果；对于性能关键的方法，最好返回 null 或抛出异常。最后，除了作为返回值之外，你几乎不应该以任何其他方式使用 Optional。 56. 为所有公开的 API 元素编写文档注释","categories":[{"name":"Java","slug":"Java","permalink":"https://zunpan.github.io/categories/Java/"}],"tags":[{"name":"Effective-Java","slug":"Effective-Java","permalink":"https://zunpan.github.io/tags/Effective-Java/"},{"name":"方法","slug":"方法","permalink":"https://zunpan.github.io/tags/%E6%96%B9%E6%B3%95/"}]},{"title":"Effective-Java学习笔记（六）","slug":"Effective-Java学习笔记（六）","date":"2022-07-30T06:30:09.000Z","updated":"2023-09-24T04:27:40.276Z","comments":true,"path":"2022/07/30/Effective-Java学习笔记（六）/","link":"","permalink":"https://zunpan.github.io/2022/07/30/Effective-Java%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%85%AD%EF%BC%89/","excerpt":"","text":"第七章 λ 表达式和流 42. λ 表达式优于匿名类 函数对象 在历史上，带有单个抽象方法的接口（或者抽象类，但这种情况很少）被用作函数类型。它们的实例（称为函数对象）表示函数或操作。自从 JDK 1.1 在 1997 年发布以来，创建函数对象的主要方法就是匿名类（Item-24）。下面是一个按长度对字符串列表进行排序的代码片段，使用一个匿名类来创建排序的比较函数（它强制执行排序顺序）： // Anonymous class instance as a function object - obsolete! Collections.sort(words, new Comparator&lt;String&gt;() &#123; public int compare(String s1, String s2) &#123; return Integer.compare(s1.length(), s2.length()); &#125; &#125;); 函数式接口 在 Java 8 中官方化了一个概念，即具有单个抽象方法的接口是特殊的，应该得到特殊处理。这些接口现在被称为函数式接口 lambda 表达式 可以使用 lambda 表达式为函数式接口创建实例，lambda 表达式在功能上类似于匿名类，但更简洁 // Lambda expression as function object (replaces anonymous class) Collections.sort(words,(s1, s2) -&gt; Integer.compare(s1.length(), s2.length())); lambda 表达式的入参和返回值类型一般不写，由编译器做类型推断，类型能不写就不写。 Item26告诉你不要用原始类型，Item29，30告诉你要用泛型和泛型方法，编译器从泛型中获得了大部分类型推断所需的类型信息 lambda 表达式缺少名称和文档；如果一个算法较复杂，或者有很多行代码，不要把它放在 lambda 表达式中。 一行是理想的，三行是合理的最大值 lambda 表达式仅限于函数式接口。如果想创建抽象类实例或者多个抽象方法的接口，只能用匿名类 lambda 表达式编译后会成为外部类的一个私有方法，而匿名类会生成单独的class文件，所以 lambda 表达式的this是外部类实例，匿名类的this是匿名类实例 lambda表达式和匿名类都不能可靠的序列化，如果想序列化函数对象，可以用私有静态嵌套类 43. 方法引用优于 λ 表达式 Java 提供了一种比 lambda 表达式更简洁的方法来生成函数对象：方法引用。下面这段代码的功能是，如果数字 1 不在映射中，则将其与键关联，如果键已经存在，则将关联值递增： map.merge(key, 1, (count, incr) -&gt; count + incr); 上面代码的 lambda 表达式作用是返回两个入参的和，在 Java 8 中，Integer（和所有其它基本类型的包装类）提供了一个静态方法 sum，它的作用完全相同，我们可以传入一个方法引用，并得到相同结果，同时减少视觉混乱： map.merge(key, 1, Integer::sum); 函数对象的参数越多，方法引用就显得越简洁，但是 lambda 表达式指明了参数名，使得 lambda表达式比方法引用更容易阅读和维护，没有什么是方法引用能做而 lambda 表达式做不了的 许多方法引用引用静态方法，但是有四种方法不引用静态方法。其中两个是绑定和非绑定实例方法引用。在绑定引用中，接收对象在方法引用中指定。绑定引用在本质上与静态引用相似：函数对象接受与引用方法相同的参数。在未绑定引用中，在应用函数对象时通过方法声明参数之前的附加参数指定接收对象。在流管道中，未绑定引用通常用作映射和筛选函数（Item-45）。最后，对于类和数组，有两种构造函数引用。构造函数引用用作工厂对象。五种方法参考文献汇总如下表： Method Ref Type Example Lambda Equivalent Static Integer::parseInt str -&gt; Bound Instant.now()::isAfter Instant then =Instant.now(); t -&gt;then.isAfter(t) Unbound String::toLowerCase str -&gt;str.toLowerCase() Class Constructor TreeMap&lt;K,V&gt;::new () -&gt; new TreeMap&lt;K,V&gt; Array Constructor int[]::new len -&gt; new int[len] 总之，方法引用通常为 lambda 表达式提供了一种更简洁的选择。如果方法引用更短、更清晰，则使用它们；如果没有，仍然使用 lambda 表达式。 44. 优先使用标准函数式接口 java.util.function 包提供了大量的标准函数接口。优先使用标准函数式接口而不是自己写。 通过减少 API ，使得你的 API 更容易学习，并将提供显著的互操作性优势，因为许多标准函数式接口提供了有用的默认方法 java.util.function 中有 43 个接口。不能期望你记住所有的接口，但是如果你记住了 6 个基本接口，那么你可以在需要时派生出其余的接口。基本接口操作对象引用类型。Operator 接口表示结果和参数类型相同的函数。Predicate 接口表示接受参数并返回布尔值的函数。Function 接口表示参数和返回类型不同的函数。Supplier 接口表示一个无参并返回（或「供应」）值的函数。最后，Consumer 表示一个函数，该函数接受一个参数，但不返回任何内容，本质上是使用它的参数。六个基本的函数式接口总结如下： Interface Function Signature Example UnaryOperator&lt;T&gt; T apply(T t) String::toLowerCase BinaryOperator&lt;T&gt; T apply(T t1, T t2) BigInteger::add Predicate&lt;T&gt; boolean test(T t) Collection::isEmpty Function&lt;T,R&gt; R apply(T t) Arrays::asList Supplier&lt;T&gt; T get() Instant::now Consumer&lt;T&gt; void accept(T t) System.out::println 还有 6 个基本接口的 3 个变体，用于操作基本类型 int、long 和 double。它们的名称是通过在基本接口前面加上基本类型前缀而派生出来的。例如，一个接受 int 的 Predicate 就是一个 IntPredicate，一个接受两个 long 值并返回一个 long 的二元操作符就是一个 LongBinaryOperator。没有变体类型是参数化的除了由返回类型参数化的 Function 变体外。例如，LongFunction&lt;int[]&gt; 使用 long 并返回一个 int[]。 Function 接口还有 9 个额外的变体，在结果类型为基本数据类型时使用。源类型和结果类型总是不同的，因为入参只有一个且与出参类型相同的函数本身都是 UnaryOperator。如果源类型和结果类型都是基本数据类型，则使用带有 SrcToResult 的前缀函数，例如 LongToIntFunction（六个变体）。如果源是一个基本数据类型，而结果是一个对象引用，则使用带前缀 SrcToObj 的 Function 接口，例如 DoubleToObjFunction（三个变体）。 三个基本函数式接口有两个参数版本，使用它们是有意义的：BiPredicate&lt;T,U&gt;、BiFunction&lt;T,U,R&gt;、BiConsumer&lt;T,U&gt;。也有 BiFunction 变体返回三个相关的基本类型：ToIntBiFunction&lt;T,U&gt;、 ToLongBiFunction&lt;T,U&gt;、ToDoubleBiFunction&lt;T,U&gt;。Consumer 有两个参数变体，它们接受一个对象引用和一个基本类型：ObjDoubleConsumer&lt;T&gt;、ObjIntConsumer&lt;T&gt;、ObjLongConsumer&lt;T&gt;。总共有9个基本接口的双参数版本。 最后是 BooleanSupplier 接口，它是 Supplier 的一个变体，返回布尔值。这是在任何标准函数接口名称中唯一显式提到布尔类型的地方，但是通过 Predicate 及其四种变体形式支持布尔返回值。前面描述的 BooleanSupplier 接口和 42 个接口占了全部 43 个标准函数式接口。 总之，既然 Java 已经有了 lambda 表达式，你必须在设计 API 时考虑 lambda 表达式。在输入时接受函数式接口类型，在输出时返回它们。一般情况下，最好使用 java.util.function 中提供的标准函数式接口，但请注意比较少见的一些情况，在这种情况下，你最好编写自己的函数式接口。 45. 明智地使用流 Java8添加了流API，用来简化序列或并行执行批量操作，API有两个关键的抽象：流（表示有限或无限的数据元素序列）和流管道（表示对这些元素的多阶段计算）。 流 流中的元素可以来自任何地方。常见的源包括集合、数组、文件、正则表达式的 Pattern 匹配器、伪随机数生成器和其他流。流中的数据元素可以是对象的引用或基本数据类型。支持三种基本数据类型：int、long 和 double。 流管道 流管道由源流跟着零个或多个中间操作和一个终止操作组成。每个中间操作以某种方式转换流，例如将每个元素映射到该元素的一个函数，或者过滤掉不满足某些条件的所有元素。中间操作都将一个流转换为另一个流，其元素类型可能与输入流相同，也可能与输入流不同。终止操作对最后一次中间操作所产生的流进行最终计算，例如将其元素存储到集合中、返回特定元素、或打印其所有元素。 流管道的计算是惰性的：直到调用终止操作时才开始计算，并且对完成终止操作不需要的数据元素永远不会计算。这种惰性的求值机制使得处理无限流成为可能。 流 API 是流畅的：它被设计成允许使用链式调用将组成管道的所有调用写到单个表达式中。 谨慎使用流，全部都是流操作的代码可读性不高 谨慎命名 lambda 表达式的参数名以提高可读性，复杂表达式使用 helper 方法 流 VS 迭代 迭代代码使用代码块表示重复计算，流管道使用函数对象（通常是 lambda 表达式或方法引用）表示重复计算。 迭代优势： 代码块可以读取修改局部变量，lambda表达式只能读取final变量或实际上final变量（初始化后不再修改，编译器会帮我们声明为final）。 代码块可以控制迭代，包括return，break，continue，throw，但是lambda不行 流适合用在以下操作： 元素序列的一致变换 过滤元素序列 组合元素序列（例如添加它们，连接它们或计算它们的最小值） 聚合元素序列到一个集合中，例如按属性分组 在元素序列中搜索满足某些条件的元素 总之，有些任务最好使用流来完成，有些任务最好使用迭代来完成。许多任务最好通过结合这两种方法来完成。 46. 在流中使用无副作用的函数 考虑以下代码，它用于构建文本文件中单词的频率表 // Uses the streams API but not the paradigm--Don&#x27;t do this! Map&lt;String, Long&gt; freq = new HashMap&lt;&gt;(); try (Stream&lt;String&gt; words = new Scanner(file).tokens()) &#123; words.forEach(word -&gt; &#123; freq.merge(word.toLowerCase(), 1L, Long::sum); &#125;); &#125; 这段代码可以得出正确答案，但它不是流代码，而是伪装成流代码的迭代代码。它比迭代代码更长，更难阅读，更难维护，问题出在：forEach修改了外部状态。正确使用的流代码如下： // Proper use of streams to initialize a frequency table Map&lt;String, Long&gt; freq; try (Stream&lt;String&gt; words = new Scanner(file).tokens()) &#123; freq = words.collect(groupingBy(String::toLowerCase, counting())); &#125; forEach 操作应该只用于报告流计算的结果，而不是执行计算 正确的流代码使用了 Collectors 的生成收集器的方法，将元素收集到集合中的方法有三种：toList()、toSet() 和 toCollection(collectionFactory)。它们分别返回 List、Set 和程序员指定的集合类型 Collectors其它方法大部分是将流收集到Map中。最简单的Map收集器是toMap(keyMapper, valueMapper)，它接受两个函数，一个将流元素映射到键，另一个将流元素映射到值，Item34用到了这个收集器 // Using a toMap collector to make a map from string to enum private static final Map&lt;String, Operation&gt; stringToEnum =Stream.of(values()).collect(toMap(Object::toString, e -&gt; e)); 这个收集器不能处理重复键的问题，我们可以加入第三个参数，即merge函数，处理重复键，它的参数类型是Map的值类型。我们还可以加入第四个参数用来指定特定的Map实现（如EnumMap或TreeMap） 除了toMap方法，groupingBy方法也将元素收集到Map中，键是类别，值是这个类别的所有元素的列表；groupingBy方法第二个参数是一个下游收集器，例如 counting() 作为下游收集器，最终的Map的键是类别，值是这个类别所有元素的数量；groupingBy也支持指定特定的Map实现 minBy和maxBy，它们接受一个Comparator并返回最小或最大元素；join方法将字符序列（如字符串）连接起来 总之，流管道编程的本质是无副作用的函数对象。这适用于传递给流和相关对象的所有函数对象。中间操作 forEach 只应用于报告由流执行的计算结果，而不应用于执行计算。为了正确使用流，你必须了解 collector。最重要的 collector 工厂是 toList、toSet、toMap、groupingBy 和 join。 47. 优先使用 Collection 而不是 Stream 作为返回类型 Stream 没有继承 Iterable，不能用 for-each 循环遍历。Collection 接口继承了 Iterable 而且提供了转换为流的方法，因此，Collection 或其适当的子类通常是公有返回序列的方法的最佳返回类型。 48. 谨慎使用并行流 如果流来自 Stream.iterate 或者中间操作 limit，并行化管道也不太可能提高其性能 通常，并行性带来的性能提升在 ArrayList、HashMap、HashSet 和 ConcurrentHashMap 实例上的流效果最好；int 数组和 long 数组也在其中。 这些数据结构的共同之处在于，它们都可以被精确且廉价地分割成任意大小的子程序，这使得在并行线程之间划分工作变得很容易。另一个共同点是，当按顺序处理时，它们提供了极好的引用位置：顺序元素引用一起存储在内存中，这些引用的引用对象在内存中可能彼此不太接近，这减少了引用的位置。引用位置对于并行化批量操作非常重要，如果没有它，线程将花费大量时间空闲，等待数据从内存传输到处理器的缓存中。具有最佳引用位置的数据结构是基本数组，因为数据本身是连续存储在内存中的。 如果在终止操作中完成大量工作，并且该操作本质上是顺序的，那么管道的并行化效果有限。并行化的最佳终止操作是 reduce 方法或者预先写好的reduce 方法，包括min、max、count、and。anyMatch、allMatch 和 noneMatch 的短路操作也适用于并行性。流的 collect 方法执行的操作称为可变缩减，它们不是并行化的好候选，因为组合集合的开销是昂贵的。 并行化流不仅会导致糟糕的性能，包括活动失败；它会导致不正确的结果和不可预知的行为（安全故障）。 总之，不要尝试并行化流管道，除非你有充分的理由相信它将保持计算的正确性以及提高速度。不适当地并行化流的代价可能是程序失败或性能灾难。","categories":[{"name":"Java","slug":"Java","permalink":"https://zunpan.github.io/categories/Java/"}],"tags":[{"name":"Effective-Java","slug":"Effective-Java","permalink":"https://zunpan.github.io/tags/Effective-Java/"},{"name":"lambda表达式","slug":"lambda表达式","permalink":"https://zunpan.github.io/tags/lambda%E8%A1%A8%E8%BE%BE%E5%BC%8F/"},{"name":"Stream","slug":"Stream","permalink":"https://zunpan.github.io/tags/Stream/"}]},{"title":"Effective-Java学习笔记（五）","slug":"Effective-Java学习笔记（五）","date":"2022-07-22T16:44:09.000Z","updated":"2023-09-24T04:27:40.275Z","comments":true,"path":"2022/07/23/Effective-Java学习笔记（五）/","link":"","permalink":"https://zunpan.github.io/2022/07/23/Effective-Java%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%94%EF%BC%89/","excerpt":"","text":"第六章 枚举和注解 34. 用枚举类型代替 int 常量 int枚举 int枚举类型中有一系列public static final int常量，每个常量表示一个类型。 它有许多缺点: 不提供类型安全性，容易误用（入参是Apple常量，但是传任何int都可以） 常量值修改，客户端也必须修改 调试麻烦 枚举类型 Java 的枚举类型是成熟的类，其他语言中的枚举类型本质上是 int 值。 枚举优势： 实例受控 Java 枚举类型通过 public static final 修饰的字段为每个枚举常量导出一个实例。枚举类型实际上是 final 类型，因为构造函数是私有的。客户端既不能创建枚举类型的实例，也不能继承它，所以除了声明的枚举常量之外，不能有任何实例。换句话说，枚举类型是实例受控的类（Item-1）。它们是单例（Item-3）的推广应用，单例本质上是单元素的枚举。 提供编译时类型安全性。 如果将参数声明为 Apple 枚举类型，则可以保证传递给该参数的任何非空对象引用都是 Apple 枚举值之一。尝试传递错误类型的值将导致编译时错误，将一个枚举类型的表达式赋值给另一个枚举类型的变量，或者使用 == 运算符比较不同枚举类型的值同样会导致错误。 名称相同的枚举类型常量能共存 名称相同的枚举类型常量能和平共存，因为每种类型都有自己的名称空间。你可以在枚举类型中添加或重新排序常量，而无需重新编译其客户端，因为导出常量的字段在枚举类型及其客户端之间提供了一层隔离：常量值不会像在 int 枚举模式中那样编译到客户端中。最后，你可以通过调用枚举的 toString 方法将其转换为可打印的字符串。 允许添加任意方法和字段并实现任意接口 枚举类型允许添加任意方法和字段并实现任意接口。它们提供了所有 Object 方法的高质量实现（参阅 Chapter 3），还实现了 Comparable（Item-14）和 Serializable（参阅 Chapter 12），并且它们的序列化形式被设计成能够适应枚举类型的可变性。如果方法是常量特有的，可以在枚举类型中添加抽象方法，在声明枚举实例时覆盖抽象方法 // Enum type with constant-specific class bodies and data public enum Operation &#123; PLUS(&quot;+&quot;) &#123; public double apply(double x, double y) &#123; return x + y; &#125; &#125;, MINUS(&quot;-&quot;) &#123; public double apply(double x, double y) &#123; return x - y; &#125; &#125;, TIMES(&quot;*&quot;) &#123; public double apply(double x, double y) &#123; return x * y; &#125; &#125;, DIVIDE(&quot;/&quot;) &#123; public double apply(double x, double y) &#123; return x / y; &#125; &#125;; private final String symbol; Operation(String symbol) &#123; this.symbol = symbol; &#125; @Override public String toString() &#123; return symbol; &#125; // Implementing a fromString method on an enum type private static final Map&lt;String, Operation&gt; stringToEnum =Stream.of(values()).collect(toMap(Object::toString, e -&gt; e)); // Returns Operation for string, if any public static Optional&lt;Operation&gt; fromString(String symbol) &#123; return Optional.ofNullable(stringToEnum.get(symbol)); &#125; public abstract double apply(double x, double y); &#125; 上述实现的缺点是如果常量特有的方法有可以复用的代码，那么会造成许多冗余。你可以把冗余代码抽出成方法，常量覆盖抽象方法时调用抽出的方法，这种实现同样有许多冗余代码；你也可以把抽象方法改成具体方法，里面是可以复用的代码，添加常量时如果不覆盖那就用默认的，这种实现问题在于添加常量时如果忘记覆盖那就用默认方法了。 当常量特有的方法有可以复用的代码时，我们采用策略枚举模式，将可复用代码移到私有嵌套枚举中。当常量调用方法时，调用私有嵌套枚举常量的方法。 在枚举上实现特定常量的行为时 switch 语句不是一个好的选择，只有在枚举不在你的控制之下，你希望它有一个实例方法来返回每个常量的特定行为，这时候才用 switch 总之，枚举类型相对于 int 常量的优势是毋庸置疑的。枚举更易于阅读、更安全、更强大。许多枚举不需要显式构造函数或成员，但有些枚举则受益于将数据与每个常量关联，并提供行为受数据影响的方法。将多个行为与一个方法关联起来，这样的枚举更少。在这种相对少见的情况下，相对于使用 switch 的枚举，特定常量方法更好。如果枚举常量有一些（但不是全部）共享公共行为，请考虑策略枚举模式。 35. 使用实例字段替代序数 每个枚举常量都有一个 ordinal 方法，返回枚举常量在枚举类中的位置。不要用这个方法返回与枚举常量关联的值，一旦常量位置变动，值就是错的，所以不要用序数生成与枚举常量关联的值，而是将其存在实例字段中 36. 用 EnumSet 替代位字段 位字段 如果枚举类型的元素主要在 Set 中使用，传统上使用 int 枚举模式（Item34），通过不同的 2 的幂次为每个常量赋值： // Bit field enumeration constants - OBSOLETE! public class Text &#123; public static final int STYLE_BOLD = 1 &lt;&lt; 0; // 1 public static final int STYLE_ITALIC = 1 &lt;&lt; 1; // 2 public static final int STYLE_UNDERLINE = 1 &lt;&lt; 2; // 4 public static final int STYLE_STRIKETHROUGH = 1 &lt;&lt; 3; // 8 // Parameter is bitwise OR of zero or more STYLE_ constants public void applyStyles(int styles) &#123; ... &#125; &#125; 使用位运算的 OR 操作将几个常量组合成一个集合，这个集合叫做位字段： text.applyStyles(STYLE_BOLD | STYLE_ITALIC); 位字段表示方式允许使用位运算高效地执行集合操作，如并集和交集。但是位字段具有 int 枚举常量所有缺点，甚至更多。当位字段被打印为数字时，它比简单的 int 枚举常量更难理解。没有一种简单的方法可以遍历由位字段表示的所有元素。最后，你必须预测在编写 API 时需要的最大位数，并相应地为位字段（通常是 int 或 long）选择一种类型。一旦选择了一种类型，在不更改 API 的情况下，不能超过它的宽度（32 或 64 位）。 EnumSet 一些使用枚举而不是 int 常量的程序员在需要传递常量集合时仍然坚持使用位字段。没有理由这样做，因为存在更好的选择。java.util 包提供 EnumSet 类来有效地表示从单个枚举类型中提取的值集。这个类实现了 Set 接口，提供了所有其他 Set 实现所具有的丰富性、类型安全性和互操作性。但在内部，每个 EnumSet 都表示为一个位向量。如果底层枚举类型有 64 个或更少的元素（大多数都是），则整个 EnumSet 用一个 long 表示，因此其性能与位字段的性能相当。批量操作（如 removeAll 和 retainAll）是使用逐位算法实现的，就像手动处理位字段一样。但是，你可以避免因手工修改导致产生不良代码和潜在错误：EnumSet 为你完成了这些繁重的工作。 当之前的示例修改为使用枚举和 EnumSet 而不是位字段时。它更短，更清晰，更安全： // EnumSet - a modern replacement for bit fields public class Text &#123; public enum Style &#123; BOLD, ITALIC, UNDERLINE, STRIKETHROUGH &#125; // Any Set could be passed in, but EnumSet is clearly best public void applyStyles(Set&lt;Style&gt; styles) &#123; ... &#125; &#125; 下面是将 EnumSet 实例传递给 applyStyles 方法的客户端代码。EnumSet 类提供了一组丰富的静态工厂，可以方便地创建集合，下面的代码演示了其中的一个： text.applyStyles(EnumSet.of(Style.BOLD, Style.ITALIC)); 总之，如果枚举类型将在 Set 中使用，不要用位字段表示它。 EnumSet 类结合了位字段的简洁性和性能，以及 (Item-34) 中描述的枚举类型的许多优点。EnumSet 的一个真正的缺点是，从 Java 9 开始，它不能创建不可变的 EnumSet，在未来发布的版本中可能会纠正这一点。同时，可以用 Collections.unmodifiableSet 包装 EnumSet，但简洁性和性能将受到影响 37. 使用 EnumMap 替换序数索引 序数索引 当需要将枚举常量映射到其它值时，有一种方法是序数索引，使用 ordinal 方法返回的序数表示key。这种方式存在Item35提到的问题 EnumMap EnumMap使用枚举常量作为key，功能丰富、类型安全 38. 使用接口模拟可扩展枚举 枚举不可以扩展（继承）另一个枚举，但可以实现接口。可以用接口模拟可扩展的枚举。下面的例子将Item34的枚举类型的Operation改成了接口类型。通过实现接口来模拟继承枚举。 // Emulated extensible enum using an interface public interface Operation &#123; double apply(double x, double y); &#125; public enum BasicOperation implements Operation &#123; PLUS(&quot;+&quot;) &#123; public double apply(double x, double y) &#123; return x + y; &#125; &#125;, MINUS(&quot;-&quot;) &#123; public double apply(double x, double y) &#123; return x - y; &#125; &#125;, TIMES(&quot;*&quot;) &#123; public double apply(double x, double y) &#123; return x * y; &#125; &#125;, DIVIDE(&quot;/&quot;) &#123; public double apply(double x, double y) &#123; return x / y; &#125; &#125;; private final String symbol; BasicOperation(String symbol) &#123; this.symbol = symbol; &#125; @Override public String toString() &#123; return symbol; &#125; &#125; // Emulated extension enum public enum ExtendedOperation implements Operation &#123; EXP(&quot;^&quot;) &#123; public double apply(double x, double y) &#123; return Math.pow(x, y); &#125; &#125;, REMAINDER(&quot;%&quot;) &#123; public double apply(double x, double y) &#123; return x % y; &#125; &#125;; private final String symbol; ExtendedOperation(String symbol) &#123; this.symbol = symbol; &#125; @Override public String toString() &#123; return symbol; &#125; &#125; 可以用接口类型引用指向不同子枚举类型实例。 public static void main(String[] args) &#123; Operation op = BasicOperation.DIVIDE; System.out.println(op.apply(15, 3)); op=ExtendedOperation.EXP; System.out.println(op.apply(2,5)); &#125; java.nio.file.LinkOption 使用了这个Item描述的方法，它实现了 CopyOption 和 OpenOption 接口。 总之，枚举不可以扩展或被扩展，但是你可以通过接口模拟枚举之间的层级关系 39. 注解优于命名模式 命名模式 用来标明某些程序元素需要工具或框架特殊处理。 例如JUnit3及之前，用户必须写test开头的测试方法，不然测试都不能运行。 缺点：1.写错方法名会导致测试通过，但是实际上根本没有运行；2.不能将参数值和程序元素关联，例如测试方法只有在抛出特定异常时才成功，虽然可以精心设计命名模式，将异常名称写在测试方法名上，但是编译器无法检查异常类是否存在 注解 Junit4 开始使用注解，框架会根据注解执行测试方法，也可以将异常和测试方法绑定。注解本身不修改代码语义，而是通过反射（框架所用的技术）对其特殊处理 40. 坚持使用 @Override 注解 请在要覆盖父类声明的每个方法声明上使用 @Override 注解。只有一个例外，那就是具体类覆盖父类的抽象方法，因为具体类必须要实现父类的抽象方法，所以不必加 @Override 注解 41. 使用标记接口定义类型 标记接口 标记接口是一个空接口，它的作用是标记实现类具有某些属性。例如，实现 Serializable 接口表示类的实例可以写入 ObjectOutputStream（序列化） 标记接口VS标记注解 与标记注解（Item39）相比，标记接口有两个优点： 标记接口定义的类型由标记类的实例实现；标记注解不会。因此标记接口类型的存在允许你在编译时捕获错误，标记注解只能在运行时捕获。ObjectOutputStream.writeObject方法入参必须是实现了Serializable的实例，否则会报编译错误（JDK设计缺陷，writeObject入参是Object类型） 标记接口相对于标记注解的另一个优点是可以更精确地定位子类型。例如 Set 是一个标记接口，它继承 Collection接口，Set标记了所有它的实现类都具有Collection功能。Set相较于标记接口的特殊之处在于它还细化了Collection方法 add、equals 和 hashCode 的约定 标记注解的优势：与基于使用注解的框架保持一致性 使用规则 如果标记用在类或接口之外的任何程序元素，必须用标记注解；如果框架使用注解，那就用标记注解；其它情况都用标记接口","categories":[{"name":"Java","slug":"Java","permalink":"https://zunpan.github.io/categories/Java/"}],"tags":[{"name":"Effective-Java","slug":"Effective-Java","permalink":"https://zunpan.github.io/tags/Effective-Java/"},{"name":"枚举","slug":"枚举","permalink":"https://zunpan.github.io/tags/%E6%9E%9A%E4%B8%BE/"},{"name":"注解","slug":"注解","permalink":"https://zunpan.github.io/tags/%E6%B3%A8%E8%A7%A3/"}]},{"title":"Effective-Java学习笔记（四）","slug":"Effective-Java学习笔记（四）","date":"2022-07-14T12:17:09.000Z","updated":"2023-09-24T04:27:40.276Z","comments":true,"path":"2022/07/14/Effective-Java学习笔记（四）/","link":"","permalink":"https://zunpan.github.io/2022/07/14/Effective-Java%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%9B%9B%EF%BC%89/","excerpt":"","text":"第五章 泛型 26. 不要使用原始类型 泛型和原始类型 声明中具有一个或多个类型参数的类或接口就是泛型类或泛型接口。泛型类和泛型接口统称为泛型。 每个泛型都定义了一个原始类型，它是没有任何相关类型参数的泛型的名称。例如，List&lt;E&gt; 对应的原始类型是 List。原始类型的行为就好像所有泛型信息从类型声明中删除了一样。它们的存在主要是为了与之前的代码兼容。 泛型优势 泛型可以帮助编译器在编译过程中发现潜在的类型转换异常，而原始类型不行。 泛型的子类型规则 原始类型 List 和 参数化类型 List&lt;Object&gt; 都可以保存任何类型的对象 ,但是不能把其它泛型对象 , 例如List&lt;String&gt;对象赋给List&lt;Object&gt;引用而可以赋给 List 引用。泛型有子类型规则，List&lt;String&gt; 是原始类型 List 的子类型，而不是参数化类型 List&lt;Object&gt; 的子类型（Item-28）。假如List&lt;String&gt;可以是List&lt;Object&gt;的子类型，那么List&lt;String&gt;的对象赋给List&lt;Object&gt;，通过List&lt;Object&gt;插入非String元素，违反了List&lt;String&gt;只放String的约定 无界通配符 如果你想使用泛型，但不知道或不关心实际的类型参数是什么，那么可以使用无界通配符 ? 代替。例如，泛型集合 Set&lt;E&gt; 的无界通配符类型是 Set&lt;?&gt;。它是最通用的参数化集合类型，能够容纳任何集合 无界通配符类型 Set&lt;?&gt; 和原始类型 Set 之间的区别在于通配符类型是安全的，而原始类型不是。将任何元素放入具有原始类型的集合中，很容易破坏集合的类型一致性；而无界通配符类型不能放入元素（除了null） Set&lt;Integer&gt; integerSet = new HashSet&lt;&gt;(); // 无界通配符类型Set&lt;?&gt;可以引用任何Set&lt;E&gt;和Set,但是不能往里面放除了null的元素 Set&lt;?&gt; set1 = integerSet; // 原始类型Set也可以引用任何Set&lt;E&gt;和Set,但是可以往里面添加元素,会存在类型转换异常 Set set2 = integerSet; 使用泛型而不用原始类型的例外 类字面量。该规范不允许使用参数化类型（尽管它允许数组类型和基本类型）。换句话说，List.class，String[].class 和 int.class 都是合法的，但是 List&lt;String&gt;.class 和 List&lt;?&gt;.class 不是。 instanceof 运算符。由于泛型信息在运行时被删除，因此在不是无界通配符类型之外的参数化类型使用 instanceof 操作符是非法的。使用无界通配符类型代替原始类型不会以任何方式影响 instanceof 运算符的行为。在这种情况下，尖括号和问号只是多余的。下面的例子是使用通用类型 instanceof 运算符的首选方法： // Legitimate use of raw type - instanceof operator if (o instanceof Set) &#123; // Raw type Set&lt;?&gt; s = (Set&lt;?&gt;) o; // Wildcard type ... &#125; 总之，使用原始类型可能会在运行时导致异常，所以不要轻易使用它们。它们仅用于与引入泛型之前的遗留代码进行兼容。快速回顾一下，Set&lt;Object&gt; 是一个参数化类型，表示可以包含任何类型的对象的集合，Set&lt;?&gt; 是一个无界通配符类型，表示只能包含某种未知类型的对象的集合，Set 是一个原始类型，它没有使用泛型。前两个是安全的，后一个不安全 为便于参考，本条目中介绍的术语（以及后面将要介绍的一些术语）总结如下： Term Example Item Parameterized type List&lt;String&gt; Item-26 Actual type parameter String Item-26 Generic type List&lt;E&gt; Item-26, Item-29 Formal type parameter E Item-26 Unbounded wildcard type List&lt;?&gt; Item-26 Raw type List Item-26 Bounded type parameter &lt;E extends Number&gt; Item-29 Recursive type bound &lt;T extends Comparable&lt;T&gt;&gt; Item-30 Bounded wildcard type List&lt;? extends Number&gt; Item-31 Generic method static &lt;E&gt; List&lt;E&gt; asList(E[] a) Item-30 Type token String.class Item-33 27. 消除 unchecked 警告 消除所有 unchecked 警告可以确保代码是类型安全的，运行时不会出现 ClassCastException。如果不能消除警告，但是可以证明引发警告的代码是类型安全的，那么可以使用 SuppressWarnings(“unchecked”) 注解来抑制警告。 SuppressWarnings注解 SuppressWarnings 注解可以用于任何声明中，从单个局部变量声明到整个类。请总是在尽可能小的范围上使用 SuppressWarnings 注解。通常用在一个变量声明或一个非常短的方法或构造函数。不要在整个类中使用 SuppressWarnings。这样做可能会掩盖关键警告。 如果你发现自己在一个超过一行的方法或构造函数上使用 SuppressWarnings 注解，那么你可以将其移动到局部变量声明中 将 SuppressWarnings 注释放在 return 语句上是非法的，因为它不是声明。你可能想把注释放在整个方法上，但是不要这样做。相反，应该声明一个局部变量来保存返回值并添加注解 每次使用 SuppressWarnings(“unchecked”) 注解时，要添加一条注释，说明这样做是安全的。这将帮助他人理解代码，更重要的是，它将降低其他人修改代码而产生不安全事件的几率。如果你觉得写这样的注释很难，那就继续思考合适的方式，你最终可能会发现，unchecked 操作毕竟是不安全的。 总之，unchecked 警告很重要。不要忽视他们。每个 unchecked 警告都代表了在运行时发生 ClassCastException 的可能性。尽最大努力消除这些警告。如果不能消除 unchecked 警告，但是可以证明引发该警告的代码是类型安全的，那么可以在尽可能狭窄的范围内使用 @SuppressWarnings(“unchecked”) 注释来抑制警告。在注释中记录你决定抑制警告的理由。 28. list 优于数组 数组与泛型的区别 数组是协变的, 如果 Sub 是 Super 的一个子类型，那么数组类型 Sub[] 就是数组类型 Super[] 的一个子类型。相比之下，泛型是不变的：对于任何两个不同类型 Type1 和 Type2，List&lt;Type1&gt; 既不是 List&lt;Type2&gt; 的子类型，也不是 List&lt;Type2&gt; 的父类型。 数组是具体化的。这意味着数组在运行时知道并强制执行他们的元素类型。如前所述，如果试图将 String 元素放入一个 Long 类型的数组中，就会得到 ArrayStoreException。相比之下，泛型是通过擦除来实现的，这意味着它们只在编译时执行类型约束，并在运行时丢弃（或擦除）元素类型信息。擦除允许泛型与不使用泛型的遗留代码自由交互操作（Item-26），确保在 Java 5 中平稳过渡 泛型数组的创建是非法的 由于这些基本差异，数组和泛型不能很好地混合。例如，创建泛型、参数化类型或类型参数的数组是非法的。因此，这些数组创建表达式都不是合法的：new List&lt;E&gt;[]、new List&lt;String&gt;[]、new E[]。所有这些都会在编译时导致泛型数组创建错误。 考虑以下代码片段： // Why generic array creation is illegal - won&#x27;t compile! List&lt;String&gt;[] stringLists = new List&lt;String&gt;[1]; // (1) List&lt;Integer&gt; intList = List.of(42); // (2) Object[] objects = stringLists; // (3) objects[0] = intList; // (4) String s = stringLists[0].get(0); // (5) 假设创建泛型数组的第 1 行是合法的。第 2 行创建并初始化一个包含单个元素的 List&lt;Integer&gt;。第 3 行将 List&lt;String&gt; 数组存储到 Object 类型的数组变量中，这是合法的，因为数组是协变的。第 4 行将 List&lt;Integer&gt; 存储到 Object 类型的数组的唯一元素中，这是成功的，因为泛型是由擦除实现的：List&lt;Integer&gt; 实例的运行时类型是 List，List&lt;String&gt;[] 实例的运行时类型是 List[]，因此这个赋值不会生成 ArrayStoreException。现在我们有麻烦了。我们将一个 List&lt;Integer&gt; 实例存储到一个数组中，该数组声明只保存 List&lt;String&gt; 实例。在第 5 行，我们从这个数组的唯一列表中检索唯一元素。编译器自动将检索到的元素转换为 String 类型，但它是一个 Integer 类型的元素，因此我们在运行时得到一个 ClassCastException。为了防止这种情况发生，第 1 行（创建泛型数组）必须生成编译时错误。 用List替代数组 当你在转换为数组类型时遇到泛型数组创建错误或 unchecked 强制转换警告时，通常最好的解决方案是使用集合类型 List&lt;E&gt;，而不是数组类型 E[]。你可能会牺牲一些简洁性和性能，但作为交换，你可以获得更好地类型安全性和互操作性。 总之，数组和泛型有非常不同的类型规则。数组是协变的、具体化的；泛型是不可变的和可被擦除的。因此，数组提供了运行时类型安全性，而不是编译时类型安全性，对于泛型来说相反。一般来说，数组和泛型不能很好的混合，如果你发现将它们混合在一起并得到编译时错误或警告，那么你的第一个反应应该是将数组替换为 list。 29. 优先使用泛型 编写泛型 在将原始类型修改成泛型时, 可能会遇到不能创建泛型数组的问题，有两种解决方法 创建 Object 数组并将其强制转换为 E[] 类型（字段的类型是 E[] ,将Object数组强转成 E[] 可以成功，但是方法返回 E[]给客户端，由编译器添加隐式强转就会失败,因为隐式强转是将Object数组转成声明的具体类型数组）。现在，编译器将发出一个警告来代替错误。这种用法是合法的，但（一般而言）不是类型安全的。编译器可能无法证明你的程序是类型安全的，但你可以。你必须说服自己，unchecked 的转换不会损害程序的类型安全性。所涉及的数组（元素）存储在私有字段中，从未返回给客户端或传递给任何其它方法(传到外面就会发生隐式强转)。添加元素时，元素也是 E 类型，因此 unchecked 的转换不会造成任何损害。一旦你证明了 unchecked 的转换是安全的，就将警告限制在尽可能小的范围内（Item-27）。 将字段的类型从 E[] 更改为 Object[]。编译器会产生类似的错误和警告，处理方法也和上面类似 方法1优势：可读性更好，因为数组声明为 E[] 类型，这清楚地表明它只包含 E 的实例。它也更简洁，只需要在创建数组的地方做一次强转，其它地方读取数组元素不用强转成 E 类型 方法1劣势：会造成堆污染（Item-32）：数组的运行时类型与其编译时类型不匹配（除非 E 恰好是 Object） 为啥要创建泛型数组 Item-28 鼓励优先使用列表而不是数组。但是在泛型中使用列表并不总是可能或可取的。Java 本身不支持列表，因此一些泛型（如ArrayList）必须在数组之上实现。其它泛型（如HashMap）用数组实现来提高性能 总之，泛型比需要在客户端代码中转换的类型更安全、更容易使用。在设计新类型时，请确保客户端可以在不使用类型转换的情况下使用它们。这通常意味着使类型具有通用性。如果你有任何应该是泛型但不是泛型的现有类型，请对它们进行泛化。这将使这些类型的新用户在不破坏现有客户端的情况下更容易使用。 30. 优先使用泛型方法 类可以是泛型的，方法也可以是泛型的 编写泛型方法 编写泛型方法类似于编写泛型。类型参数列表声明类型参数，它位于方法的修饰符与其返回类型之间。例如，类型参数列表为 &lt;E&gt;，返回类型为 Set&lt;E&gt; // Generic method public static &lt;E&gt; Set&lt;E&gt; union(Set&lt;E&gt; s1, Set&lt;E&gt; s2) &#123; Set&lt;E&gt; result = new HashSet&lt;&gt;(s1); result.addAll(s2); return result; &#125; 至少对于简单的泛型方法，这就是（要注意细节的）全部。该方法编译时不生成任何警告，并且提供了类型安全性和易用性。这里有一个简单的程序来演示。这个程序不包含转换，编译时没有错误或警告： // Simple program to exercise generic method public static void main(String[] args) &#123; Set&lt;String&gt; guys = Set.of(&quot;Tom&quot;, &quot;Dick&quot;, &quot;Harry&quot;); Set&lt;String&gt; stooges = Set.of(&quot;Larry&quot;, &quot;Moe&quot;, &quot;Curly&quot;); Set&lt;String&gt; aflCio = union(guys, stooges); System.out.println(aflCio); &#125; 当你运行程序时，它会打印出 [Moe, Tom, Harry, Larry, Curly, Dick]。（输出元素的顺序可能不同）。 union 方法的一个限制是，所有三个集合（输入参数和返回值）的类型必须完全相同。你可以通过使用有界通配符类型（Item-31）使方法更加灵活。 31. 使用有界通配符增加 API 的灵活性 PECS 为了获得满足里氏代换原则，应对表示生产者或消费者入参使用通配符类型。如果输入参数既是生产者优势消费者，使用精确的类型 PECS助记符：PECS 表示生产者应使用 extends，消费者应使用 super。换句话说，如果参数化类型表示 T 生产者，则使用 &lt;? extends T&gt;；如果它表示一个 T 消费者，则使用 &lt;? super T&gt;。不要使用有界通配符类型作为返回类型。 它将强制用户在客户端代码中使用通配符类型，而不是为用户提供额外的灵活性 一个例子 接下来让我们将注意力转移到 Item-30 中的 max 方法，以下是原始声明： public static &lt;T extends Comparable&lt;T&gt;&gt; T max(List&lt;T&gt; list) 下面是使用通配符类型的修正声明： public static &lt;T extends Comparable&lt;? super T&gt;&gt; T max(List&lt;? extends T&gt; list) 这里使用了两次 PECS。第一次是参数列表，list作为生产者生成 T 的实例，所以我们将类型从 List&lt;T&gt; 更改为 List&lt;? extends T&gt;(个人认为这里没有必要改，调用一次max方法只能传入一个类型的List，如果是两个参数，第一个参数的类型决定了T，第二个参数的类型如果是List&lt;? extends T&gt;，那么类型参数必须是T或T的子类。但是如果是泛型类声明的T，那这个改动是有意义的)。第二次是是类型参数 T。这是我们第一次看到通配符应用于类型参数。最初，T 被指定为继承 Comparable&lt;T&gt;，但是 Comparable&lt;T&gt; 消费 T 实例。因此，将参数化类型 Comparable&lt;T&gt; 替换为有界通配符类型 Comparable&lt;? super T&gt;，Comparable 始终是消费者，所以一般应优先使用 Comparable&lt;? super T&gt; 而不是 Comparable&lt;T&gt;，比较器也是如此；因此，通常应该优先使用 Comparator&lt;? super T&gt; 而不是 Comparator&lt;T&gt;。 修订后的 max 声明可能是本书中最复杂的方法声明。增加的复杂性真的能给你带来什么好处吗？是的。下面是一个简单的列表案例，它在原来的声明中不允许使用，但经修改的声明允许： List&lt;ScheduledFuture&lt;?&gt;&gt; scheduledFutures = ... ; 不能将原始方法声明应用于此列表的原因是 ScheduledFuture 没有实现 Comparable&lt;ScheduledFuture&gt;。相反，它是 Delayed 的一个子接口，继承了 Comparable&lt;Delayed&gt;。换句话说，ScheduledFuture 的实例不仅仅可以与其它 ScheduledFuture 实例进行比较，还可以与任何 Delayed 实例比较，但是原始方法只能和 ScheduledFuture 实例比较。更通俗来说，通配符用于支持不直接实现 Comparable（或 Comparator）但继承了实现 Comparable（或 Comparator）的类型的类型。 类型参数和通配符的对偶性 类型参数和通配符之间存在对偶性，对偶性指实现方式不同但效果一样。例如，下面是swap方法的两种可能声明，第一个使用无界类型参数（Item-30），第二个使用无界通配符： // Two possible declarations for the swap method public static &lt;E&gt; void swap(List&lt;E&gt; list, int i, int j); public static void swap(List&lt;?&gt; list, int i, int j); 这两个声明哪个更好？在公共API中第二个更好，因为它更简单(客户端可以传入原始类型)。传入一个任意列表，该方法交换索引元素，不需要担心类型参数。通常，如果类型参数在方法声明中只出现一次，则用通配符替换它。如果它是一个无界类型参数，用一个无界通配符替换它；如果它是有界类型参数，则用有界通配符替换它。 交换的第二个声明有一个问题。下面的实现无法编译， list 的类型是 List&lt;?&gt;，你不能在 List&lt;?&gt; 中放入除 null 以外的任何值。 public static void swap(List&lt;?&gt; list, int i, int j) &#123; list.set(i, list.set(j, list.get(i))); &#125; 幸运的是，有一种方法可以实现，而无需求助于不安全的强制类型转换或原始类型。其思想是编写一个私有助手方法来捕获通配符类型。为了捕获类型，helper 方法必须是泛型方法。它看起来是这样的： public static void swap(List&lt;?&gt; list, int i, int j) &#123; swapHelper(list, i, j); &#125; // Private helper method for wildcard capture private static &lt;E&gt; void swapHelper(List&lt;E&gt; list, int i, int j) &#123; list.set(i, list.set(j, list.get(i))); &#125; swapHelper 方法指导 list 是一个 List&lt;E&gt;。因此，它指导它从这个列表中得到的任何值都是 E 类型的，并且将 E 类型的任何值放入这个列表中都是安全的。这个稍微复杂的实现可以正确编译。它允许我们导出基于通配符的声明，同时在内部利用更复杂的泛型方法。swap 方法的客户端不必面对更复杂的 swapHelper 声明，但它们确实从中受益。值得注意的是，helper 方法具有我们认为对于公共方法过于复杂而忽略的签名。 总之，在 API 中使用通配符类型虽然很棘手，但可以使其更加灵活。如果你编写的库将被广泛使用，则必须考虑通配符类型的正确使用。记住基本规则：生产者使用 extends，消费者使用 super（PECS）。还要记住，所有的 comparable 和 comparator 都是消费者 32. 明智地合用泛型和可变参数 抽象泄露 可变参数方法（Item-53）和泛型都是在 Java 5 中添加，因此你可能认为它们能够优雅地交互；可悲的是，它们并不能。可变参数的目的是允许客户端向方法传递可变数量的参数，但这是一个抽象泄漏：当你调用可变参数方法时，将创建一个数组来保存参数；该数组本应是实现细节，却是可见的。因此，当可变参数具有泛型或参数化类型时，会出现令人困惑的编译器警告。 不可具体化 回想一下 Item-28，不可具体化类型是指其运行时表示的信息少于其编译时表示的信息，并且几乎所有泛型和参数化类型都是不可具体化的。如果方法声明其可变参数为不可具体化类型，编译器将在声明生生成警告。如果方法是在其推断类型不可具体化的可变参数上调用的，编译器也会在调用时生成警告。生成的警告就像这样： warning: [unchecked] Possible heap pollution from parameterized vararg type List&lt;String&gt; 堆污染 当参数化类型的变量引用不属于该类型的对象时，就会发生堆污染。它会导致编译器自动生成的强制类型转换失败，违反泛型系统的基本保证。 例如，考虑这个方法，它来自 Item-26，但做了些修改： // Mixing generics and varargs can violate type safety! // 泛型和可变参数混合使用可能违反类型安全原则！ static void dangerous(List&lt;String&gt;... stringLists) &#123; List&lt;Integer&gt; intList = List.of(42); Object[] objects = stringLists; objects[0] = intList; // Heap pollution String s = stringLists[0].get(0); // ClassCastException &#125; 方法声明使用泛型可变参数是合法的 这个例子提出了一个有趣的问题：为什么方法声明中使用泛型可变参数是合法的，而显式创建泛型数组是非法的？答案是，带有泛型或参数化类型的可变参数的方法在实际开发中非常有用，因此语言设计人员选择忍受这种不一致性。事实上，Java 库导出了几个这样的方法，包括 Arrays.asList(T... a)、Collections.addAll(Collection&lt;? super T&gt; c, T... elements) 以及 EnumSet.of(E first, E... rest)。它们与前面显示的危险方法不同，这些库方法是类型安全的。 在 Java 7 之前，使用泛型可变参数的方法的作者对调用点上产生的警告无能为力。使得这些 API 难以使用。用户必须忍受这些警告，或者在每个调用点（Item-27）使用 @SuppressWarnings(“unchecked”) 注释消除这些警告。这种做法乏善可陈，既损害了可读性，也忽略了标记实际问题的警告。 在 Java 7 中添加了 SafeVarargs 注释，以允许使用泛型可变参数的方法的作者自动抑制客户端警告。本质上，SafeVarargs 注释构成了方法作者的一个承诺，即该方法是类型安全的。 作为这个承诺的交换条件，编译器同意不对调用可能不安全的方法的用户发出警告。 使用SafeVarargs的条件 方法没有修改数组元素 数组的引用没有逃逸（这会使不受信任的代码能够访问数组） 换句话说，如果可变参数数组仅用于将可变数量的参数从调用方传输到方法（毕竟这是可变参数的目的），那么该方法是安全的。 一个反例 // UNSAFE - Exposes a reference to its generic parameter array! static &lt;T&gt; T[] toArray(T... args) &#123; return args; &#125; 这个方法直接返回了泛型可变参数数组引用，违反了上面的条件2，它可以将堆污染传播到调用堆栈上。 考虑下面的泛型方法，该方法接受三个类型为 T 的参数，并返回一个包含随机选择的两个参数的数组： public static void main(String[] args) &#123; String[] attributes = pickTwo(&quot;Good&quot;, &quot;Fast&quot;, &quot;Cheap&quot;); &#125; static &lt;T&gt; T[] pickTwo(T a, T b, T c) &#123; switch(ThreadLocalRandom.current().nextInt(3)) &#123; case 0: return toArray(a, b); case 1: return toArray(a, c); case 2: return toArray(b, c); &#125; throw new AssertionError(); // Can&#x27;t get here &#125; 这段代码编译时不会生成任何警告，运行时会抛出 ClassCastException，尽管它不包含可见的强制类型转换。你没有看到的是，编译器在 pickTwo 返回的值上生成了一个隐藏的 String[] 转换。转换失败，因为 Object[] 实际指向的数组是Object类型的不是String，强转失败。 这个示例的目的是让人明白，让另一个方法访问泛型可变参数数组是不安全的，只有两个例外：将数组传递给另一个使用 @SafeVarargs 正确注释的可变参数方法是安全的，将数组传递给仅计算数组内容的某个函数的非可变方法也是安全的。 扩展 如果 main 方法直接调用 toArray 方法，不会出现 ClassCastException，为什么？泛型不是被擦除了吗，args在运行时不是一个Object[] 吗？非也，运行时args的类型是Object[]，但是指向的数组是 String 类型的, 所以编译器添加的强转是可以成功的。下面的例子里，toArray1方法在字节码层面和toArray3方法是一样的，在调用toArray1方法前会先生成 String[]，将引用传递给toArray1， public static void main(String[] args) &#123; String [] arr1 = toArray1(&quot;Alice&quot;, &quot;Bob&quot;, &quot;Cat&quot;); // ClassCastException String [] arr2 = toArray2(&quot;Alice&quot;, &quot;Bob&quot;, &quot;Cat&quot;); String [] arr3 = toArray3(new String[]&#123;&quot;Alice&quot;, &quot;Bob&quot;, &quot;Cat&quot;&#125;); // ClassCastException,多层调用泛型可变参数数组会导致编译器没有足够信息创建真实类型的对象数组(存疑，从字节码来看也是创建了String[]) String[] arr4 = toArray1Crust1(&quot;Alice&quot;, &quot;Bob&quot;, &quot;Cat&quot;); String[] arr5 = toArray1Crust2(new String[]&#123;&quot;Alice&quot;, &quot;Bob&quot;, &quot;Cat&quot;&#125;); String[] arr6 = toArray1Crust3(new String[]&#123;&quot;Alice&quot;, &quot;Bob&quot;, &quot;Cat&quot;&#125;); // 泛型方法嵌套没问题 String s = toString1(&quot;Alice&quot;); &#125; static &lt;T&gt; T[] toArray1(T... args) &#123; return args; &#125; static &lt;T&gt; T[] toArray2(T a, T b, T c) &#123; System.out.println(a.getClass()); // 泛型数组实际类型是Object T[] result = (T[]) new Object[3]; result[0] = a; result[1] = b; result[2] = c; return result; &#125; static &lt;T&gt; T[] toArray3(T[] arr)&#123; return arr; &#125; static &lt;T&gt; T[] toArray1Crust1(T...args)&#123; return toArray1(args); &#125; static &lt;T&gt; T[] toArray1Crust2(T[] args)&#123; return toArray1(args); &#125; static String[] toArray1Crust3(String[] args)&#123; return toArray1(args); &#125; private static &lt;T&gt; T toString1(T s) &#123; return toString2(s); &#125; public static &lt;T&gt; T toString2(T s)&#123; return s; &#125; 一个正确例子 下面是一个安全使用泛型可变参数的典型示例。该方法接受任意数量的列表作为参数，并返回一个包含所有输入列表的元素的序列列表。因为该方法是用 @SafeVarargs 注释的，所以它不会在声明或调用点上生成任何警告： // Safe method with a generic varargs parameter @SafeVarargs static &lt;T&gt; List&lt;T&gt; flatten(List&lt;? extends T&gt;... lists) &#123; List&lt;T&gt; result = new ArrayList&lt;&gt;(); for (List&lt;? extends T&gt; list : lists) result.addAll(list); return result; &#125; 何时使用SafeVarargs 决定何时使用 SafeVarargs 注释的规则很简单：在每个带有泛型或参数化类型的可变参数的方法上使用 @SafeVarargs，这样它的用户就不会被不必要的和令人困惑的编译器警告所困扰。 请注意，SafeVarargs 注释只能出现在不能覆盖的方法上，因为不可能保证所有可能覆盖的方法都是安全的。在 Java 8 中，注释只能出现在静态方法和final实例方法；在 Java 9 中，它在私有实例方法上也是合法的。 使用 SafeVarargs 注释的另一种选择是接受 Item-28 的建议，并用 List 参数替换可变参数（它是一个伪装的数组）。下面是将这种方法应用到我们的 flatten 方法时的效果。注意，只有参数声明发生了更改： // List as a typesafe alternative to a generic varargs parameter static &lt;T&gt; List&lt;T&gt; flatten(List&lt;List&lt;? extends T&gt;&gt; lists) &#123; List&lt;T&gt; result = new ArrayList&lt;&gt;(); for (List&lt;? extends T&gt; list : lists) result.addAll(list); return result; &#125; 然后可以将此方法与静态工厂方法 List.of 一起使用，以允许可变数量的参数。注意，这种方法依赖于 List.of 声明是用 @SafeVarargs 注释的： audience = flatten(List.of(friends, romans, countrymen)); 这种方法的优点是编译器可以证明该方法是类型安全的。你不必使用 SafeVarargs 注释来保证它的安全性，也不必担心在确定它的安全性时可能出错。主要的缺点是客户端代码比较冗长，可能会比较慢。 这种技巧也可用于无法编写安全的可变参数方法的情况，如第 147 页中的 toArray 方法。它的列表类似于 List.of 方法，我们甚至不用写；Java 库的作者为我们做了这些工作。pickTwo 方法变成这样： static &lt;T&gt; List&lt;T&gt; pickTwo(T a, T b, T c) &#123; switch(rnd.nextInt(3)) &#123; case 0: return List.of(a, b); case 1: return List.of(a, c); case 2: return List.of(b, c); &#125; throw new AssertionError(); &#125; main 方法是这样的： public static void main(String[] args) &#123; List&lt;String&gt; attributes = pickTwo(&quot;Good&quot;, &quot;Fast&quot;, &quot;Cheap&quot;); &#125; 生成的代码是类型安全的，因为它只使用泛型，而不使用数组（List在运行时没有强转，数组会强转）。 总之，可变参数方法和泛型不能很好地交互，并且数组具有与泛型不同的类型规则。虽然泛型可变参数不是类型安全的，但它们是合法的。如果选择使用泛型（或参数化）可变参数编写方法，首先要确保该方法是类型安全的，然后使用 @SafeVarargs 对其进行注释。 33. 考虑类型安全的异构容器 参数化容器 如果你需要存储某种类型的集合，例如存储String的Set，那么Set&lt;String&gt;就足够了，又比如存储 String-Value 键值对，那么Map&lt;String,Integer&gt;就足够了 参数化容器的键 如果你要存储任意类型的集合。例如，一个数据库行可以有任意多列，我们希望用一个Map保存该行每列元素。那么我们可以使用参数化容器的键 例子 Favorites 类，允许客户端存储和检索任意多种类型对象。Class 类的对象将扮演参数化键的角色。Class 对象被称为类型标记 // Typesafe heterogeneous container pattern - client public static void main(String[] args) &#123; Favorites f = new Favorites(); f.putFavorite(String.class, &quot;Java&quot;); f.putFavorite(Integer.class, 0xcafebabe); f.putFavorite(Class.class, Favorites.class); String favoriteString = f.getFavorite(String.class); int favoriteInteger = f.getFavorite(Integer.class); Class&lt;?&gt; favoriteClass = f.getFavorite(Class.class); System.out.printf(&quot;%s %x %s%n&quot;, favoriteString,favoriteInteger, favoriteClass.getName()); &#125; Favorites实例是类型安全的：当你向它请求一个 String 类型时，它永远不会返回一个 Integer 类型。 Favorites实例也是异构的：所有键都是不同类型的，普通 Map 的键是固定一个类型，因此，我们将 Favorites 称为一个类型安全异构容器。 Favorites 的实现非常简短： // Typesafe heterogeneous container pattern - implementation public class Favorites &#123; private Map&lt;Class&lt;?&gt;, Object&gt; favorites = new HashMap&lt;&gt;(); public &lt;T&gt; void putFavorite(Class&lt;T&gt; type, T instance) &#123; favorites.put(Objects.requireNonNull(type), instance); &#125; public &lt;T&gt; T getFavorite(Class&lt;T&gt; type) &#123; return type.cast(favorites.get(type)); &#125; &#125; 通配符类型的键意味着每个键都可以有不同的参数化类型：一个可以是 Class&lt;String&gt;，下一个是 Class&lt;Integer&gt;，等等。这就是异构的原理。 favorites 的值类型仅仅是 Object。换句话说，Map 不保证键和值之间的类型关系 putFavorite 的实现很简单：它只是将从给定 Class 对象到给定对象的映射关系放入 favorites 中。如前所述，这将丢弃键和值之间的「类型关联」；将无法确定值是键的实例。但这没关系，因为 getFavorites 方法可以重新建立这个关联。 getFavorite 的实现比 putFavorite 的实现更复杂。首先，它从 favorites 中获取与给定 Class 对象对应的值。这是正确的对象引用返回，但它有错误的编译时类型：它是 Object（favorites 的值类型），我们需要返回一个 T。因此，getFavorite 的实现通过使用 Class 的 cast 方法，将对象引用类型动态转化为所代表的 Class 对象。 cast 方法是 Java 的 cast 运算符的动态模拟。它只是检查它的参数是否是类对象表示的类型的实例。如果是，则返回参数；否则它将抛出 ClassCastException。我们知道 getFavorite 中的强制转换调用不会抛出 ClassCastException。也就是说，我们知道 favorites 中的值总是与其键的类型匹配。 如果 cast 方法只是返回它的参数，那么它会为我们做什么呢？cast 方法的签名充分利用了 Class 类是泛型的这一事实。其返回类型为 Class 对象的类型参数： public class Class&lt;T&gt; &#123; T cast(Object obj); &#125; 这正是 getFavorite 方法所需要的。它使我们能够使 Favorites 类型安全，而不需要对 T 进行 unchecked 的转换。 Favorites 类有两个值得注意的限制。 恶意客户端很容易通过使用原始形式的类对象破坏 Favorites 实例的类型安全。通过使用原始类型 HashSet（Item-26），可以轻松地将 String 类型放入 HashSet&lt;Integer&gt; 中。为了获得运行时的类型安全，让 putFavorite 方法检查实例是否是 type 表示的类型的实例，使用动态转换： // Achieving runtime type safety with a dynamic cast public &lt;T&gt; void putFavorite(Class&lt;T&gt; type, T instance) &#123; favorites.put(type, type.cast(instance)); &#125; Favorites 类不能用于不可具体化的类型（Item-28）。换句话说，你可以存储的 Favorites 实例类型为 String 或 String[]，但不能存储 List&lt;String&gt;。原因是你不能为 List&lt;String&gt; 获取 Class 对象，List&lt;String&gt;.class 是一个语法错误 Favorites 使用的类型标记是无界的：getFavorite 和 put-Favorite 接受任何 Class 对象。有时你可能需要限制可以传递给方法的类型。这可以通过有界类型标记来实现，它只是一个类型标记，使用有界类型参数（Item-30）或有界通配符（Item-31）对可以表示的类型进行绑定。 annotation API（Item-39）广泛使用了有界类型标记。例如，下面是在运行时读取注释的方法。这个方法来自 AnnotatedElement 接口，它是由表示类、方法、字段和其他程序元素的反射类型实现的： public &lt;T extends Annotation&gt; T getAnnotation(Class&lt;T&gt; annotationType); 参数 annotationType 是表示注释类型的有界类型标记。该方法返回该类型的元素注释（如果有的话），或者返回 null（如果没有的话）。本质上，带注释的元素是一个类型安全的异构容器，其键是注释类型。 假设你有一个 Class&lt;?&gt; 类型的对象，并且希望将其传递给一个需要有界类型标记（例如 getAnnotation）的方法。你可以将对象强制转换为 Class&lt;? extends Annotation&gt;，但是这个强制转换是 unchecked 的，因此它将生成一个编译时警告（Item-27）。幸运的是，Class 类提供了一个实例方法，可以安全地（动态地）执行这种类型的强制转换。该方法叫做 asSubclass，它将 Class 对象强制转换为它所调用的类对象，以表示由其参数表示的类的子类。如果转换成功，则该方法返回其参数；如果失败，则抛出 ClassCastException。 下面是如何使用 asSubclass 方法读取在编译时类型未知的注释。这个方法编译没有错误或警告： // Use of asSubclass to safely cast to a bounded type token static Annotation getAnnotation(AnnotatedElement element,String annotationTypeName) &#123; Class&lt;?&gt; annotationType = null; // Unbounded type token try &#123; annotationType = Class.forName(annotationTypeName); &#125; catch (Exception ex) &#123; throw new IllegalArgumentException(ex); &#125; return element.getAnnotation(annotationType.asSubclass(Annotation.class)); &#125; 总之，以集合的 API 为例的泛型在正常使用时将每个容器的类型参数限制为固定数量。你可以通过将类型参数放置在键上而不是容器上来绕过这个限制。你可以使用 Class 对象作为此类类型安全异构容器的键。以这种方式使用的 Class 对象称为类型标记。还可以使用自定义键类型。例如，可以使用 DatabaseRow 类型表示数据库行（容器），并使用泛型类型 Column&lt;T&gt; 作为它的键","categories":[{"name":"Java","slug":"Java","permalink":"https://zunpan.github.io/categories/Java/"}],"tags":[{"name":"Effective-Java","slug":"Effective-Java","permalink":"https://zunpan.github.io/tags/Effective-Java/"},{"name":"泛型","slug":"泛型","permalink":"https://zunpan.github.io/tags/%E6%B3%9B%E5%9E%8B/"}]},{"title":"Effective-Java学习笔记（三）","slug":"Effective-Java学习笔记（三）","date":"2022-07-07T14:31:09.000Z","updated":"2023-09-24T04:27:40.274Z","comments":true,"path":"2022/07/07/Effective-Java学习笔记（三）/","link":"","permalink":"https://zunpan.github.io/2022/07/07/Effective-Java%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%89%EF%BC%89/","excerpt":"","text":"第四章 类和接口 15. 尽量减少类和成员的可访问性 信息隐藏的作用 将API与实现完全分离。组件之间只能通过它们的API进行通信，而不知道彼此的内部工作方式。 解耦系统的组件，允许它们被独立开发、测试、优化、使用、理解和修改。 增加了软件的复用性，降低了构建大型系统的风险，即使系统没有成功，单个组件也可能被证明是成功的 访问控制机制 访问控制机制指定了类、接口和成员的可访问性。 对于顶级（非嵌套）类和接口，只有两个可能的访问级别：包私有和公共。如果用 public 修饰符声明，它将是公共的，即API的一部分，修改会损害客户端；否则，它将是包私有的。 如果包私有顶级类或接口只被一个类使用，那么可以考虑变成这个类的私有静态嵌套类（Item-24） 对于成员（字段、方法、嵌套类和嵌套接口），有四个访问级别，这里按可访问性依次递增的顺序列出： 私有，成员只能从声明它的顶级类内部访问 包私有，成员可以从声明它的类的包中访问，包私有就是默认访问，即如果没有指定访问修饰符（接口的成员除外，默认情况下，接口的成员是公共的），就是这个访问级别 保护，成员可以从声明它的类的子类和声明它的类的包中访问 公共，成员可以从任何地方访问 设计类和成员的可访问性 总体规则：让每个类或成员尽可能不可访问。换句话说，在不影响软件正常功能时，使用尽可能低的访问级别。 仔细设计类的公共API，所有成员声明为私有。私有成员和包私有成员都是类实现的一部分，通常不会影响其导出的API。但是，如果类实现了Serializable（Item-86和Item-87），这些字段可能会「泄漏」到导出的 API 中 少用保护成员。保护成员是类导出API的一部分，必须永远支持 子类覆盖父类的方法，可访问性不能缩小 为了测试，将公共类的成员由私有变为包私有是可以接受的，但是进一步提高可访问性是不可接受的 带有公共可变字段的类通常不是线程安全的 为了得到不可变数组，不要公开或通过访问器返回长度非零的数组的引用，客户端将能够修改数组的内容。可以将公共数组设置为私有，并添加一个公共不可变 List；或者，将数组设置为私有，并添加一个返回私有数组副本的公共方法： 总之，你应该尽可能减少程序元素的可访问性（在合理的范围内）。在仔细设计了一个最小的公共 API 之后，你应该防止任何游离的类、接口或成员成为 API 的一部分。除了作为常量的公共静态 final 字段外，公共类应该没有公共字段。确保public static final 字段引用的对象是不可变的。 16. 在公共类中，使用访问器方法，而不是公共字段 访问器方法的作用 如果类是公共的，即可以在包外访问，提供访问器方法可以维持类内部数据表示形式的灵活性 一些建议 如果类是包私有或者是私有嵌套类，公开数据字段比提供访问器方法更清晰 公共类公开不可变字段的危害会小一点，但仍不建议公开字段 17. 减少可变性 不可变类是实例创建后不能被修改的类。Java库包含许多不可变的类，包括 String、基本类型的包装类、BigInteger和BigDecimal。 创建不可变类的规则 要使类不可变，请遵守以下5条规则： 不要提供修改对象状态的方法 确保类不能被继承。这可以防止可变子类损害父类的不可变。防止子类化通常用 final 修饰父类 所有字段用 final 修饰。 所有字段设为私有 确保对任何可变组件的独占访问。如果你的类有任何引用可变对象的字段，请确保该类的客户端无法获得对这些对象的引用。 不可变类的优点 不可变对象线程安全，复用程度高开销小，不需要防御性拷贝 不仅可以复用不可变对象，还可以复用它们的内部实现 不可变对象很适合作为其它对象的构建模块，例如 Map 的键和 Set 的元素 不可变对象自带提供故障原子性 不可变类的缺点 不可变类的主要缺点是每个不同的值都需要一个单独的对象。不可变类BigInteger的flipBit方法会创建一个和原来对象只有一个bit不同的新对象。可变类BigSet可以在固定时间内修改对象单个bit 如果执行多步操作，在每一步生成一个新对象，最终丢弃除最终结果之外的所有对象，那么性能问题就会被放大。解决方法是使用伴随类，如果能预测客户端希望在不可变类上执行哪些复杂操作就可以使用包私有可变伴随可变类（BigInteger和内部的伴随类）；否则提供一个公共可变伴随类，例如String和它的伴随类StringBuilder 设计不可变类 为了保证类不被继承，可以用final修饰类，也可以将构造函数变为私有或包私有，通过静态工厂提供对象 BigInteger 和 BigDecimal没有声明为final，为了确保你的类依赖是不可变的BigInteger或BigDecimal，你需要检查对象类型，如果是BigInteger或BigDecimal的子类，那必须防御性复制 适当放松不可变类所有字段必须是 final 的限制可以提供性能，例如，用可变字段缓存计算结果，例如 hashCode 方法在第一次调用时缓存了hash 如果你选择让不可变类实现 Serializable，并且该类包含一个或多个引用可变对象的字段，那么你必须提供一个显式的 readObject 或 readResolve 方法，或者使用 ObjectOutputStream.writeUnshared 或 ObjectInputStream.readUnshared 方法 18. 优先选择组合而不是继承 在同一个包中使用继承是安全的，因为子类和父类的实现都由相同程序员控制。在对专为继承而设计和有文档的类时使用继承也是安全的（Item-19）。然而，对普通的非抽象类进行跨包继承是危险的。与方法调用不同，继承破坏了封装性。换句话说，子类的功能正确与否依赖于它的父类的实现细节。父类的实现可能在版本之间发生变化，如果发生了变化，子类可能会崩溃，即使子类的代码没有被修改过。因此，子类必须与其父类同步更新，除非父类是专门为继承的目的而设计的，并具有很明确的文档说明。 继承的风险 子类覆盖父类的多个方法，父类的多个方法之间有调用关系，因为多态，父类方法在调用其它父类方法时会调用到子类的方法 父类可以添加新方法，新方法没有确保在添加的元素满足断言，子类没有覆盖这个方法，导致调用这个方法时添加了非法元素 父类添加了新方法，但是子类继承原来的父类时也添加了相同签名和不同返回类型的方法，这时子类不能编译，如果签名和返回类型都相同，必须声明覆盖 组合 为新类提供一个引用现有类实例的私有字段，这种设计称为组合，因为现有的类是新类的一个组件。新类中的每个实例方法调用现有类实例的对应方法，并返回结果，这称为转发。比较好的写法是包装类+转发类。 总结 只有子类确实是父类的子类型的情况下，继承才合适。换句话说，两个类 A、B 之间只有 B 满足「is-a」关系时才应该扩展 A。如果你想让 B 扩展 A，那就问问自己：每个 B 都是 A 吗？如果不能对这个问题给出肯定回答，B 不应该扩展 A；如果答案是否定的，通常情况下，B 应该包含 A 的私有实例并暴露不同的 API：A 不是 B 的基本组成部分，而仅仅是其实现的一个细节。 19. 继承要设计良好并且有文档，否则禁止使用 必须精确地在文档中描述覆盖任何方法的效果。文档必须指出方法调用了哪些可覆盖方法、调用顺序以及每次调用的结果如何影响后续处理过程。描述由 Javadoc 标签 @implSpec 生成 20. 接口优于抽象类 Java 有两种机制来定义允许多重实现的类型：接口和抽象类。 接口优势 可以定义 mixin（混合类型） 接口是定义 mixin（混合类型）的理想工具。粗略的说，mixin 是类除了「基本类型」之外还可以实现的类型，用于声明它提供了一些可选的行为。例如，Comparable 是一个 mixin 接口，它允许类的实例可以与其他的可相互比较的对象进行排序。这样的接口称为 mixin，因为它允许可选功能「混合」到基本类型中。抽象类不能用于定义 mixin，原因是：一个类不能有多个父类，而且在类层次结构中没有插入 mixin 的合理位置。‘ 允许构造非层次化类型框架 如果系统中有 n 个属性（例如唱、跳、rap），如果每个属性组合都封装成一个抽象类，组成一个层次化的类型框架，总共有2n2^n2n个类，而接口只需要 n 个 接口劣势 接口不能给 equals 和 hashCode 方法提供默认实现 接口不允许包含实例字段或者非公共静态成员（私有静态方法除外） 结合接口和抽象类的优势 模板方法模式：接口定义类型，提供默认方法，抽象类（骨架实现类）实现接口其余方法。继承骨架实现类已经完成直接实现接口的大部分工作。 按照惯例，骨架实现类称为 AbstractInterface，其中 Interface 是它们实现的接口的名称。例如 Collections Framework 提供了一个骨架实现来配合每个主要的集合接口：AbstractCollection、AbstractSet、AbstractList 和 AbstractMap。可以说，把它们叫做 SkeletalCollection、SkeletalSet、SkeletalList 和 SkeletalMap 更合理，但 Abstract 的用法现在已经根深蒂固。 编写骨架实现类的过程 研究接口有哪些方法，哪些方法可以提供默认实现，如果都可以提供默认实现就不需要骨架实现类，否则，声明一个实现接口的骨架实现类，实现所有剩余的接口方法 21. 为后代设计接口 Java 8 之前，往接口添加方法会导致实现它的类缺少方法，编译错误。Java 8 添加了默认方法，目的是允许向现有接口添加方法，但是向现有接口添加新方法有风险。 默认方法的风险 接口实现类需要同步调用每个方法，但是没有覆盖接口新加入的默认方法，导致调用默认方法时出现 ConcurrentModificationException 接口的现有实现类可以在没有错误或警告的情况下通过编译，但是运行时会出错 22. 接口只用于定义类型 接口只用于定义类型。类实现接口表明客户端可以用类的实例做什么。将接口定义为任何其他目的都是不合适的。 有一个反例是常量接口，接口内只包含 public static final 字段，它的问题在于类使用什么常量是实现细节，而实现常量接口会导致实现细节泄露到 API 中。 导出常量，有几个合理的选择。 如果这些常量与现有的类或接口紧密绑定，则应该将它们添加到类或接口。例如，所有数值包装类，比如 Integer 和 Double，都导出 MIN_VALUE 和 MAX_VALUE 常量。 枚举或者不可实例化的工具类，使用工具类的常量推荐静态导入 23. 类层次结构优于带标签的类 标签类 类的实例有两种或两种以上的样式，并且包含一个标签字段来表示实例的样式。标签类有许多缺点，可读性差、内存占用多、容易出错、添加新样式复杂 类层次结构 标签类只是类层次结构的简易模仿。要将已标签的类转换为类层次结构， 抽取标签类都有的方法、字段到一个抽象类中 继承抽象类实现都有的方法和子类特有的方法 24. 静态成员类优于非静态成员类 嵌套类是在另一个类中定义的类。嵌套类应该只为它的外部类服务。如果嵌套类在其它环境中有用，那么它应该是顶级类。有四种嵌套类：静态成员类、非静态成员类、匿名类和局部类。除了静态成员类，其它嵌套类被称为内部类。 静态成员类 静态成员类是最简单的嵌套类。最好把它看作是一个普通的类，只是碰巧在另一个类中声明而已，并且可以访问外部类的所有成员，甚至是那些声明为 private 的成员。静态成员类是其外部类的静态成员，并且遵守与其它静态成员相同的可访问性规则。如果声明为私有，则只能在外部类中访问，等等。 静态成员类作用 作为公共的辅助类，只有与它的外部类一起使用时才有意义，例如 Calculator 类和公有静态成员类 Operation 枚举 表示由其外部类表示的组件。例如，Map实现类内部的Entry类。Entry对象不需要访问 Map 。因此，使用非静态成员类来表示 entry 是浪费，私有静态成员类是最好的。 非静态成员类 从语法上讲，静态成员类和非静态成员类之间的唯一区别是静态成员类在其声明中具有修饰符 static。尽管语法相似，但这两种嵌套类有很大不同。非静态成员类的每个实例都隐式地与外部类的实例相关联，非静态成员类的实例方法可以调用外部实例上的方法，或者使用受限制的 this （父类.this）构造获得对外部实例的引用。如果嵌套类的实例可以独立于外部类的实例存在，那么嵌套类必须是静态成员类；非静态成员类的实例依赖外部类的实例 非静态成员类实例与外部类实例之间的关联是在创建成员类实例时建立的，之后无法修改。通常，关联是通过从外部类的实例方法中调用非静态成员类构造函数自动建立的。使用 enclosingInstance.new MemberClass(args) 表达式手动建立关联是可能的，尽管这种情况很少见。正如你所期望的那样，关联占用了非静态成员类实例中的空间，并增加了构造时间。 如果声明的成员类不需要访问外部类的实例，那么应始终在声明中添加 static 修饰符，如果省略这个修饰符，每个实例都有一个隐藏的对其外部实例的额外引用。存储此引用需要时间和空间，更糟糕的是，外部类可能不能被垃圾回收。 非静态成员类作用 非静态成员类的一个常见用法是定义一个 Adapter，它允许外部类的实例被视为某个不相关类的实例。例如，Map 接口的实现类通常使用非静态成员类来实现它们的集合视图， Set 和 List，通常使用非静态成员类来实现它们的迭代器 匿名类 匿名类没有名称。它不是外部类的成员。它不是与其它成员一起声明的，而是在使用时同时声明和实例化。匿名类可以在代码中用在任何一个可以用表达式的地方。当且仅当它们出现在非静态环境（没有写在静态方法里面）时，匿名类才持有外部类实例。但是，即使它们出现在静态环境中，它们也不能有除常量以外的任何静态成员。 匿名类的使用有很多限制。你只能在声明它们的时候实例化，你不能执行 instanceof 测试，也不能执行任何其它需要命名类的操作。你不能声明一个匿名类来实现多个接口或继承一个类并同时实现一个接口。匿名类的使用者除了从父类继承的成员外，不能调用任何成员。因为匿名类出现在表达式中，所以它们必须保持简短——大约10行或更短，否则会影响可读性。 匿名类作用 在 lambda 表达式被添加的 Java 之前，匿名类是动态创建小型函数对象和进程对象的首选方法，但 lambda 表达式现在是首选方法（Item-42）。匿名类的另一个常见用法是实现静态工厂方法（参见 Item-20 中的 intArrayAsList 类） 局部类 局部类是四种嵌套类中最不常用的。局部类几乎可以在任何能够声明局部变量的地方使用，并且遵守相同的作用域规则。局部类具有与其它嵌套类相同属性。与成员类一样，它们有名称，可以重复使用呢。与匿名类一样，他们只有在非静态环境中定义的情况下才具有外部类实例，而且它们不能包含静态静态成员。和匿名类一样，它们应该保持简短，以免损害可读性。 嵌套类总结 简单回顾一下，有四种不同类型的嵌套类，每一种都有自己的用途。如果嵌套的类需要在单个方法之外可见，或者太长，不适合放入方法中，则使用成员类。除非成员类的每个实例都需要引用其外部类实例，否则让它保持静态。假设嵌套类属于方法内部，如果你只需要从一个位置创建实例，并且存在一个能够描述类的现有类型，那么将其设置为匿名类；否则，将其设置为局部类。 25. 源文件仅限有单个顶层类 虽然 Java 编译器允许你在单个源文件中定义多个顶层类，但这样做没有任何好处，而且存在重大风险。这种风险源于这样一个事实：在源文件中定义多个顶层类使得为一个类提供多个定义成为可能。所使用的定义受源文件传给编译器的顺序的影响。","categories":[{"name":"Java","slug":"Java","permalink":"https://zunpan.github.io/categories/Java/"}],"tags":[{"name":"Effective-Java","slug":"Effective-Java","permalink":"https://zunpan.github.io/tags/Effective-Java/"},{"name":"类和接口","slug":"类和接口","permalink":"https://zunpan.github.io/tags/%E7%B1%BB%E5%92%8C%E6%8E%A5%E5%8F%A3/"}]},{"title":"Effective-Java学习笔记（二）","slug":"Effective-Java学习笔记（二）","date":"2022-06-30T11:50:05.000Z","updated":"2023-09-24T04:27:40.274Z","comments":true,"path":"2022/06/30/Effective-Java学习笔记（二）/","link":"","permalink":"https://zunpan.github.io/2022/06/30/Effective-Java%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/","excerpt":"","text":"第三章 对象的通用方法 10. 覆盖 equals 方法时应遵守的约定 不覆盖equals方法的情况 类的每个实例本质上都是唯一的。例如Thread类，它是活动实体类而不是值类 该类不需要提供逻辑相等测试。例如java.util.regex.Pattern可以覆盖equals方法来检查两个Pattern实例是否表示完全相同的正则表达式，但是这个类的设计人员认为客户端不需要这个功能，所以没有覆盖 父类已经覆盖了equals方法，父类的行为也适合于这个类。例如大多数Set的equals从AbstractSet继承，List从AbstractList继承，Map从AbstractMap继承 类是私有的并且你确信它的equals方法永远不会被调用。保险起见，你可以按如下方式覆盖equals方法，以确保它不会被意外调用 @Override public boolean equals(Object o) &#123; throw new AssertionError(); // Method is never called &#125; 覆盖equals方法的时机 当一个类有一个逻辑相等的概念，而这个概念不同于仅判断对象的同一性（相同对象的引用），并且父类没有覆盖 equals。对于值类通常是这样。值类只是表示值的类，例如 Integer 或 String。程序员希望发现它们在逻辑上是否等价，而不是它们是否引用相同的对象。覆盖 equals 方法不仅是为了满足程序员的期望，它还使实例能够作为 Map 的键或 Set 元素时，具有可预测的、理想的行为。 单例模式的值类不需要覆盖equals方法。例如，枚举类型就是单例值类。逻辑相等就是引用相等。 覆盖equals方法的规范 反身性：对于任何非空的参考值x，x.equals(x)必须返回true 对称性：x.equals(y)与y.equals(x)的值要么都为true要么为false 传递性：对于非空引用x，y，z，如果x.equals(y)返回true，y.equals(z)返回true，那么x.equals(z)也返回true 一致性：对于任何非空的引用值 x 和 y，x.equals(y) 的多次调用必须一致地返回 true 或一致地返回 false，前提是不修改 equals 中使用的信息。 非空性：对于非空引用x，x.equals(null)返回false，不需要显示判断是否为null，因为equals方法需要将参数转为成相同类型，转换之前会使用instanceof运算符来检查类型是否正确，如果为null，也会返回false @Override public boolean equals(Object o) &#123; if (!(o instanceof MyType)) return false; MyType mt = (MyType) o; ... &#125; 高质量构建equals方法的步骤 1、使用==检查参数是否是this对象的引用，如果是，返回true，这是一种性能优化，如果比较的开销很大，这种优化很有必要 2、使用instanceof运算符检查参数是否有正确类型。 3、将参数转换为正确的类型。因为在转换前进行了instanceof判断，所以肯定可以强转成功 4、对类中的每个有意义的字段检查是否和参数的相应字段匹配 对于不是float和double的基本类型字段，使用==比较；对应对象引用字段，递归调用equals方法；对于float字段，使用静态方法Float.compare(float,float)方法；对于double字段，使用Double.compare(double,double)。float和double字段的特殊处理是由于Float.NaN,-0.0f 和类似的双重值的存在；对于数组字段，使用Arrays.equals方法。 一些对象引用字段可能允许null出现。为了避免可能出现NullPointerException，请使用Objects.equals(Object,Object)来检查对象的字段是否相等。 对于某些类，字段比较非常复杂，如果是这样，可以存储字段的规范形式，以便equals方法进行开销较小的比较。这种技术最适合于不可变类；如果对象可以修改，则必须使规范形式保持最新 equals 方法的性能可能会受到字段比较顺序的影响。为了获得最佳性能，你应该首先比较那些更可能不同、比较成本更低的字段。不能比较不属于对象逻辑状态的字段，例如用于同步操作的锁字段。派生字段可以不比较，这样可以提高性能，但是如果派生字段包括了对象的所有信息，比如说多边形面积，可以从边和顶点计算得出，那么先比较面积，面积不一样就肯定不是同一个对象，这样可以减小开销 一些警告 覆盖equals方法时，也要覆盖hashCode方法 考虑任何形式的别名都不是一个好主意。例如，File类不应该尝试将引用同一文件的符号链接等同起来 不要用别的类型替换equals方法的Object类型 11. 当覆盖equals方法时，总是覆盖hashCode方法 Object类中hashCode方法的规范 应用程序执行期间对对象重复调用hashCode方法，它必须返回相同的值，前提是不修改equals方法中用于比较的信息。这个值不需要在应用程序的不同执行之间保持一致 如果equals(Object)方法返回true，那么在这两个对象上调用hashCode方法必须产生相同的整数结果 如果equals(Object)方法返回false，hashCode方法的值不需要一定不同，但是，不同对象的hashCode不一样可以提高散列表性能 当没有覆盖hashCode方法时，将违反第二条规范：逻辑相等的对象必须有相等的散列码。两个不同的对象在逻辑上是相等的，但它们的hashCode一般不相等。例如用Item-10中的PhoneNumber类实例作为HashMap的键 Map&lt;PhoneNumber, String&gt; m = new HashMap&lt;&gt;(); m.put(new PhoneNumber(707, 867, 5309), &quot;Jenny&quot;); 此时，你可能期望m.get(new PhoneNumber(707, 867,5309)) 返回「Jenny」，但是它返回 null。因为PhoneNumber类没有覆盖hashCode方法，插入到HashMap和从HashMap中获取的实例具有不相同的散列码，这违法了hashCode方法规范。因此，get方法查找电话号码的散列桶与put方法存储电话号码的散列桶不同。 解决这个问题有一个最简单但很糟糕的实现 // The worst possible legal hashCode implementation - never use! @Override public int hashCode() &#123; return 42; &#125; 它确保了逻辑相等的对象具有相同的散列码。同时它也很糟糕，因为每个对象的散列码都相同了。每个对象都分配到一个存储桶中，散列表退化成链表。 散列算法设计步骤 一个好的散列算法大概率为逻辑不相等对象生成不相同的散列码。理想情况下，一个散列算法应该在所有int值上均匀合理分布所有不相等对象。实现理想情况很困难，但实现一个类似的并不难，这里有一个简单的方式： 1、声明一个名为 result 的int变量，并将其初始化为对象中第一个重要字段的散列码c，如步骤2.a中计算的那样 2、对象中剩余的重要字段f，执行以下操作： a. 为字段计算一个整数散列码c : 如果字段是基本数据类型,计算Type.hashCode(f),其中type是 f 类型对应的包装类 ; 如果字段是对象引用,并且该类的equals方法通过递归调用equals方法来比较字段,则递归调用字段上的hashCode方法。如果需要更复杂的比较，则为该字段计算一个【canonical representation】，并在canonical representation上调用hashCode方法。如果字段的值为空，则使用0（或其它常数，但0是惯用的）；如果字段是一个数组，则对数组中每个重要元素都计算散列码，并用2.b步骤逐个组合。如果数组中没有重要元素，则使用常量，最好不是0。如果所有元素都很重要，那么使用Arrays.hashCode b. 将2.a步骤中计算的散列码合并到result变量，如下所示 result = 31 * result + c; 3、返回result变量 步骤2.b中的乘法说明result依赖字段的顺序，如果类具有多个类似字段，那么乘法会产生更好的hash性能。例如字符串hash算法中如果省略乘法，那么不同顺序的字符串都会有相同的散列码。 选择31是因为它是奇素数。如果是偶数，乘法运算就会溢出，信息就会丢失，因为乘法运算等同于移位。使用素数的好处不太明显，但它是传统用法。31有一个很好的特性，可以用移位和减法来代替乘法，从而在某些体系结构上获得更好的性能：31 * i == (i &lt;&lt;5) – i。现代虚拟机自动进行这种优化。 根据前面的步骤，给PhoneNumber类写一个hashCode方法 // Typical hashCode method @Override public int hashCode() &#123; int result = Short.hashCode(areaCode); result = 31 * result + Short.hashCode(prefix); result = 31 * result + Short.hashCode(lineNum); return result; &#125; 因为这个方法返回一个简单的确定的计算结果，它的唯一输入是 PhoneNumber 实例中的三个重要字段，所以很明显，相等的 PhoneNumber 实例具有相等的散列码。实际上，这个方法是 PhoneNumber 的一个非常好的 hashCode 方法实现，与 Java 库中的 hashCode 方法实现相当。它很简单，速度也相当快，并且合理地将不相等的电话号码分散到不同的散列桶中。 虽然这个Item里的方法可以提供一个相当不错的散列算法，但它不是最先进的，对于大多数用途是足够的，如果需要不太可能产生冲突的散列算法。请参阅 Guava 的 com.google.common.hash.Hashing Objects类有一个静态方法，它接受任意数量的对象并返回它们的散列码。这个名为hash的方法允许你编写只有一行代码的hashCode方法，它的质量可以与本Item提供的编写方法媲美。但不幸的是它们运行得很慢，因为它需要创建数组来传递可变数量的参数，如果有参数是原始类型的，则需要进行装箱和拆箱。推荐只在性能不重要的情况下使用这种散列算法。下面是使用这个静态方法编写的PhoneNumber的散列算法 // One-line hashCode method - mediocre performance @Override public int hashCode() &#123; return Objects.hash(lineNum, prefix, areaCode); &#125; 缓存散列值 如果类是不可变的，并且计算散列码的成本非常高，那么可以考虑在对象中缓存散列码，而不是每次调用重新计算。如果这个类的对象会被用作散列键，那么应该在创建对象时就计算散列码。要不然就在第一次调用时计算散列码 12. 始终覆盖toString方法 虽然 Object 提供 toString 方法的实现，但它返回的字符串通常不是类的用户希望看到的。它由后跟「at」符号（@）的类名和散列码的无符号十六进制表示（例如 PhoneNumber@163b91）组成。toString 的通用约定是这么描述的，返回的字符串应该是「简洁但信息丰富的表示，易于阅读」。虽然有人认为 PhoneNumber@163b91 简洁易懂，但与 707-867-5309 相比，它的信息量并不大。toString 约定接着描述，「建议所有子类覆盖此方法。」好建议，确实！ 虽然它不如遵守euals和hashCode约定（Item10和Item11）那么重要，但是提供一个好的toString方法更便于调试。当对象被传递给 println、printf、字符串连接操作符或断言或由调试器打印时，将自动调用 toString 方法。即使你从来没有调用 toString 对象，其他人也可能使用。例如，使用该对象的组件可以在日志错误消息中包含对象的字符串表示。如果你不覆盖 toString，该消息可能完全无用。 13. 明智地覆盖clone方法 Cloneable接口的作用 Cloneable接口的作用是声明类可克隆，但是接口不包含任何方法，类的clone方法继承自Object，并且Object类的clone方法是受保护的，无法跨包调用，虽然可以通过反射调用，但也不能保证对象具有可访问的 clone 方法（如果类没有覆盖clone方法可以通过获取父类Object调用clone方法，但是如果类没实现Cloneable接口调用会抛出CloneNotSupportedException）。 既然 Cloneable 接口不包含任何方法，用它来做什么呢？它决定了 Object 类受保护的 clone 实现的行为：如果一个类实现了 Cloneable 接口，Object 类的 clone 方法则返回该类实例的逐字段拷贝；没实现 Cloneable 接口调用 clone 方法会抛出 CloneNotSupportedException。这是接口非常不典型的一种使用方式，不应该效仿。通常，类实现接口可以表明类能够为其客户端做些什么。在本例中，它修改了父类上受保护的方法的行为。 clone 方法规范 虽然规范没有说明，但是在实践中，实现 Cloneable 接口的类应该提供一个功能正常的 public clone 方法。 clone方法的一般约定很薄弱。下面的内容是从Object规范复制过来的 Creates and returns a copy of this object. The precise meaning of “copy” may depend on the class of the object. The general intent is that, for any object x,the expression x.clone() != x will be true, and the expression x.clone().getClass() == x.getClass() will be true, but these are not absolute requirements. While it is typically the case that x.clone().equals(x) will be true, this is not an absolute requirement. clone方法创建并返回对象的副本。「副本」的确切含义可能取决于对象的类。通常，对于任何对象 x，表达式 x.clone() != x、x.clone().getClass() == x.getClass() 以及 x.clone().equals(x) 的值都将为 true，但都不是绝对的。（equals方法应覆盖为比较对象中的字段才能得到true，默认实现是比较对象地址，结果永远为false） 按照约定，clone方法返回的对象应该通过调用super.clone() 来获得。如果一个类和它的所有父类（Object类除外）都遵守这个约定，表达式 x.clone().getClass() == x.getClass() 则为 true 按照约定，返回的对象应该独立于被克隆的对象。为了实现这种独立性，可能需要在super.clone() 前修改对象的一个或多个字段 这种机制有点类似于构造方法链，只是没有强制执行： 如果一个类的clone方法返回的实例不是通过调用 super.clone() 而是通过调用构造函数获得的，编译器不会报错，但是如果这个类的子类调用super.clone()，由此产生的对象将是错误的，影响子类clone方法正常工作 如果覆盖clone方法的类是final修饰的，那么可以忽略这个约定，因为不会有子类 如果一个final修饰的类的clone方法不调用super.clone()。该类没有理由实现Cloneable接口，因为它不依赖于Object的clone方法 覆盖clone方法 如果类是不可变的，不要提供clone方法 如果类的字段都是基本类型或不可变对象的引用，那么直接这个类的clone方法直接调用super.clone()即可 如果类的字段包含可变对象的引用，需要递归调用可变对象的深拷贝方法 一些细节 和构造函数一样，不要在clone方法中调用可覆盖方法。如果clone方法调用一个在子类中被覆盖的方法，这个方法将在子类修复其在克隆中的状态之前执行，很可能导致克隆和原始对象的破坏。 Object的clone方法被声明为抛出CloneNotSupportedException，但是覆盖方法时 try-catch 异常就行，不抛出受检查异常的方法更容易使用 设计可继承的类时不要实现 Cloneable接口（如果实现了，子类就必须对外提供clone方法）。你可以选择通过实现一个功能正常的受保护克隆方法来模拟 Object 的行为，该方法声明为抛出 CloneNotSupportedException。这给子类实现 Cloneable 或不实现 Cloneable 的自由。或者，你可以选择不实现一个有效的克隆方法，并通过提供以下退化的克隆实现来防止子类实现它： // clone method for extendable class not supporting Cloneable @Override protected final Object clone() throws CloneNotSupportedException &#123; throw new CloneNotSupportedException(); &#125; 如果你编写了一个实现了 Cloneable 接口的线程安全类，请记住它的 clone 方法必须正确同步，就像其他任何方法一样。 总结 回顾一下，所有实现 Cloneable 接口的类都应该使用一个返回类型为类本身的公有方法覆盖 clone。这个方法应该首先调用 super.clone()，然后「修复」任何需要「修复」的字段。通常，这意味着复制任何包含对象内部「深层结构」的可变对象，并将克隆对象对这些对象的引用替换为对其副本的引用。虽然这些内部副本通常可以通过递归调用 clone 来实现，但这并不总是最好的方法。如果类只包含基本数据类型的字段或对不可变对象的引用，那么很可能不需要修复任何字段。这条规则也有例外。例如，表示序列号或其他唯一 ID 的字段需要修复，即使它是基本数据类型或不可变的。 搞这么复杂真的有必要吗？答案是否定的。如果你扩展了一个已经实现了 Cloneable 接口的类，那么除了实现行为良好的 clone 方法之外，你别无选择。否则，最好提供对象复制的替代方法。一个更好的对象复制方法是提供一个复制构造函数或复制工厂。复制构造函数是一个简单的构造函数，它接受单个参数，其类型是包含构造函数的类，例如 // Copy constructor public Yum(Yum yum) &#123; ... &#125;; 复制工厂与复制构造函数的静态工厂（Item-1）类似： // Copy factory public static Yum newInstance(Yum yum) &#123; ... &#125;; 复制构造函数方法及其静态工厂变体与克隆方法相比有许多优点： 它们不依赖于易发生风险的语言外对象创建机制（Object的clone方法是native的）； 他们不要求无法强制执行的约定（clone方法一定要先调用super.clone()）； 它们与 final 字段不冲突（clone方法不能修改final字段）； 它们不会抛出不必要的 checked 异常； 而且不需要强制类型转换。 可以提供类型转换构造函数 考虑到与 Cloneable 相关的所有问题，新的接口不应该继承它，新的可扩展类不应该实现它。通常，复制功能最好由构造函数或工厂提供。这个规则的一个明显的例外是数组，最好使用 clone 方法来复制数组。 14. 考虑实现 Comparable 接口 与本章讨论的其它方法不同，compareTo 方法不是 Object 中声明的，而是 Comparable 接口中的唯一方法。一个类实现Comparable，表明实例具有自然顺序（字母或数字或时间顺序）。Java 库中的所有值类以及所有枚举类型（Item-34）都实现了 Comparable接口 compareTo方法约定 compareTo 方法的一般约定类似于 equals 方法： 将一个对象与指定对象进行顺序比较。当该对象小于、等于或大于指定对象时，对应返回一个负整数、零或正整数。如果指定对象的类型阻止它与该对象进行比较，则抛出 ClassCastException 在下面的描述中， sgn(expression) 表示数学中的符号函数，它被定义为：根据传入表达式的值是负数、零或正数，对应返回-1、0或1。 实现类必须确保所有 x 和 y 满足 sgn(x.compareTo(y)) == -sgn(y.compareTo(x))（这意味着 x.compareTo(y) 当且仅当 y.compareTo(x) 抛出异常时才抛出异常） 实现类还必须确保关系是可传递的：(x.compareTo(y) &gt; 0 &amp;&amp; y.compareTo(z) &gt; 0) 意味着 x.compareTo(z) &gt; 0 最后，实现类必须确保 x.compareTo(y) == 0 时，所有的 z 满足 sgn(x.compareTo(z)) == sgn(y.compareTo(z)) 强烈建议 (x.compareTo(y)== 0) == (x.equals(y)) 成立，但这不是必需的。一般来说，任何实现 Comparable 接口并违法此条件的类都应该清除地注明这一事实。推荐的表述是“Note: This class has a natural ordering that is inconsistent with equals.” 与equals方法不同，equals方法入参是Object类型，而compareTo方法不需要和不同类型的对象比较：当遇到不同类型的对象时，允许抛出ClassCastException 就像违反 hashCode 约定的类可以破坏依赖 hash 的其他类一样，违反 compareTo 约定的类也可以破坏依赖 Comparable 的其他类。依赖 Comparable 的类包括排序集合 TreeSet 和 TreeMap，以及实用工具类 Collections 和 Arrays，它们都包含搜索和排序算法。 compareTo的约定和equals约定有相同的限制：反身性、对称性和传递性。如果要向实现 Comparable 的类中添加值组件，不要继承它；编写一个不相关的类，其中包含第一个类的实例。然后提供返回所包含实例的「视图」方法。这使你可以自由地在包含类上实现你喜欢的任何 compareTo 方法，同时允许它的客户端在需要时将包含类的实例视为被包含类的实例。 compareTo 约定的最后一段是一个强烈建议而不是要求，它只是简单地说明了 compareTo 方法所施加地同等性检验通常应该与 equals 方法返回相同的结果。如果一个类的 compareTo 方法强加了一个与 equals 不一致的顺序，那么这个类仍然可以工作，但是包含该类元素的有序集合可能无法遵守集合接口（Collection、Set 或 Map）的一般约定。这是因为这些接口的一般约定是根据 equals 方法定义的，但是有序集合使用 compareTo 代替了 equals 实施同等性建议，这是需要注意的地方 例如，考虑 BigDecimal 类，它的 compareTo 方法与 equals 不一致。如果你创建一个空的 HashSet 实例，然后添加 new BigDecimal(“1.0”) 和 new BigDecimal(“1.00”)，那么该 HashSet 将包含两个元素，因为添加到该集合的两个 BigDecimal 实例在使用 equals 方法进行比较时结果是不相等的。但是，如果你使用 TreeSet 而不是 HashSet 执行相同的过程，那么该集合将只包含一个元素，因为使用 compareTo 方法比较两个 BigDecimal 实例时结果是相等的。（有关详细信息，请参阅 BigDecimal 文档。） 编写 compareTo 方法 递归调用引用字段的 compareTo 方法。如果引用字段没有实现 Comparable，或者需要一个非标准的排序，那么应使用比较器 基本字段类型使用包装类的静态 compare 方法比较 从最重要字段开始比较 考虑使用 java.util.Comparator接口的比较器构造方法","categories":[{"name":"Java","slug":"Java","permalink":"https://zunpan.github.io/categories/Java/"}],"tags":[{"name":"Effective-Java","slug":"Effective-Java","permalink":"https://zunpan.github.io/tags/Effective-Java/"},{"name":"对象的通用方法","slug":"对象的通用方法","permalink":"https://zunpan.github.io/tags/%E5%AF%B9%E8%B1%A1%E7%9A%84%E9%80%9A%E7%94%A8%E6%96%B9%E6%B3%95/"}]},{"title":"Effective-Java学习笔记（一）","slug":"Effective-Java学习笔记（一）","date":"2022-06-23T10:11:15.000Z","updated":"2023-09-24T04:27:40.273Z","comments":true,"path":"2022/06/23/Effective-Java学习笔记（一）/","link":"","permalink":"https://zunpan.github.io/2022/06/23/Effective-Java%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89/","excerpt":"","text":"第二章 创建和销毁对象 1. 考虑用静态工厂方法代替构造函数 静态工厂方法是一个返回该类实例的 public static 方法。例如，Boolean类的valueOf方法 public static Boolean valueOf(boolean b) &#123; return b ? Boolean.TRUE : Boolean.FALSE; &#125; 要注意静态工厂方法与设计模式中的工厂方法不同。 静态工厂方法优点： 静态工厂方法有确切名字，客户端实例化对象时代码更易懂。例如，BigInteger类中返回可能为素数的BigInteger对象静态工厂方法叫BigInteger.probablePrime。此外，每个类的构造函数签名是唯一的，但是程序员可以通过调整参数类型、个数或顺序修改构造函数的签名，这样会给客户端实例化对象带来困惑，因为静态工厂方法有确切名称所以不会出现这个问题 静态工厂方法不需要在每次调用时创建新对象。例如Boolean.valueOf(boolean)，true和false会返回预先创建好的对应Boolean对象。这种能力允许类在任何时候都能严格控制存在的实例，常用来实现单例 静态工厂方法可以获取任何子类的对象。这种能力的一个应用是API可以不公开子类或者实现类的情况下返回对象。例如Collections类提供静态工厂生成不是public的子类对象，不可修改集合和同步集合等 静态工厂方法返回对象的类型可以根据输入参数变换。例如，EnumSet类的noneOf方法，当enum的元素个数小于等于64，noneOf方法返回RegularEnumSet类型的对象，否则返回JumboEnumSet类型的对象 静态工厂方法的返回对象的类不需要存在。这种灵活的静态工厂方法构成了服务提供者框架的基础。service provider框架有三个必要的组件：代表实现的service interface；provider registration API，提供者用来注册实现；service access API，客户端使用它来获取服务的实例，服务访问API允许客户端选择不同实现，是一个灵活的静态工厂方法。service provider第四个可选的组件是service provider interface，它描述了产生service interface实例的工厂对象。在JDBC中，Connection扮演service interface角色，DriverManager.registerDriver是provider registration API，DriverManager.getConnection是service access API，Driver是service provider interface 静态工厂方法的缺点： 只提供静态工厂方法而没有public或者protected的构造方法就不能被继承，例如Collections类就不能被继承 静态工厂方法没有构造函数那么显眼，常见的静态工厂方法名字如下： from，一种类型转换方法，该方法接受单个参数并返回该类型的相应实例，例如： Date d = Date.from(instant); of，一个聚合方法，它接受多个参数并返回一个包含这些参数的实例，例如： Set&lt;Rank&gt; faceCards = EnumSet.of(JACK, QUEEN, KING); valueOf，一种替代 from 和 of 但更冗长的方法 BigInteger prime = BigInteger.valueOf(Integer.MAX_VALUE); instance 或 getInstance，返回一个实例，该实例由其参数（如果有的话）描述，但不具有相同的值，例如： StackWalker luke = StackWalker.getInstance(options); create 或 newInstance，与 instance 或 getInstance 类似，只是该方法保证每个调用都返回一个新实例，例如： Object newArray = Array.newInstance(classObject, arrayLen); getType，类似于 getInstance，但如果工厂方法位于不同的类中，则使用此方法。其类型是工厂方法返回的对象类型，例如： FileStore fs = Files.getFileStore(path); newType，与 newInstance 类似，但是如果工厂方法在不同的类中使用。类型是工厂方法返回的对象类型，例如： BufferedReader br = Files.newBufferedReader(path);` type，一个用来替代 getType 和 newType 的比较简单的方式，例如： List&lt;Complaint&gt; litany = Collections.list(legacyLitany); 2. 当构造函数有多个参数时，考虑改用Builder 静态工厂和构造函数都有一个局限：不能对大量可选参数做很好扩展。例如，一个表示食品营养标签的类，必选字段有净含量、热量，另外有超过20个可选字段，比如反式脂肪、钠等。 为这种类编写构造函数，通常是使用可伸缩构造函数，这里展示四个可选字段的情况 // Telescoping constructor pattern - does not scale well! public class NutritionFacts &#123; private final int servingSize; // (mL) required private final int servings; // (per container) required private final int calories; // (per serving) optional private final int fat; // (g/serving) optional private final int sodium; // (mg/serving) optional private final int carbohydrate; // (g/serving) optional public NutritionFacts(int servingSize, int servings) &#123; this(servingSize, servings, 0); &#125; public NutritionFacts(int servingSize, int servings, int calories) &#123; this(servingSize, servings, calories, 0); &#125; public NutritionFacts(int servingSize, int servings, int calories, int fat) &#123; this(servingSize, servings, calories, fat, 0); &#125; public NutritionFacts(int servingSize, int servings, int calories, int fat, int sodium) &#123; this(servingSize, servings, calories, fat, sodium, 0); &#125; public NutritionFacts(int servingSize, int servings, int calories, int fat, int sodium, int carbohydrate) &#123; this.servingSize = servingSize; this.servings = servings; this.calories = calories; this.fat = fat; this.sodium = sodium; this.carbohydrate = carbohydrate; &#125; &#125; 当你想创建指定carbohydrate的实例，就必须调用最后一个构造函数，这样就必须给出calories、fat、sodium的值。当有很多可选参数时，这种模式可读性很差。 当遇到许多可选参数时，另一种选择是使用JavaBean模式，在这种模式中，调用一个无参数的构造函数来创建对象，然后调用setter方法来设置参数值 // JavaBeans Pattern - allows inconsistency, mandates mutability public class NutritionFacts &#123; // Parameters initialized to default values (if any) private int servingSize = -1; // Required; no default value private int servings = -1; // Required; no default value private int calories = 0; private int fat = 0; private int sodium = 0; private int carbohydrate = 0; public NutritionFacts() &#123; &#125; // Setters public void setServingSize(int val) &#123; servingSize = val; &#125; public void setServings(int val) &#123; servings = val; &#125; public void setCalories(int val) &#123; calories = val; &#125; public void setFat(int val) &#123; fat = val; &#125; public void setSodium(int val) &#123; sodium = val; &#125; public void setCarbohydrate(int val) &#123; carbohydrate = val; &#125; &#125; 这种模式比可伸缩构造函数模式更易读，但有严重的缺点。因为对象的构建要调用多个set方法，所以对象可能会在构建过程中处于不一致状态（多线程下，其它线程使用了未构建完成的对象） 第三种选择是建造者模式，它结合了可伸缩构造函数模式的安全性和JavaBean模式的可读性。客户端不直接生成所需的对象，而是使用所有必需的参数调用构造函数或静态工厂方法生成一个builder对象。然后，客户端在builder对象上调用像set一样的方法来设置可选参数，最后调用build方法生成所需对象。Builder通常是它构建的类的静态成员类 // Builder Pattern public class NutritionFacts &#123; private final int servingSize; private final int servings; private final int calories; private final int fat; private final int sodium; private final int carbohydrate; public static class Builder &#123; // Required parameters private final int servingSize; private final int servings; // Optional parameters - initialized to default values private int calories = 0; private int fat = 0; private int sodium = 0; private int carbohydrate = 0; public Builder(int servingSize, int servings) &#123; this.servingSize = servingSize; this.servings = servings; &#125; public Builder calories(int val) &#123; calories = val; return this; &#125; public Builder fat(int val) &#123; fat = val; return this; &#125; public Builder sodium(int val) &#123; sodium = val; return this; &#125; public Builder carbohydrate(int val) &#123; carbohydrate = val; return this; &#125; public NutritionFacts build() &#123; return new NutritionFacts(this); &#125; &#125; private NutritionFacts(Builder builder) &#123; servingSize = builder.servingSize; servings = builder.servings; calories = builder.calories; fat = builder.fat; sodium = builder.sodium; carbohydrate = builder.carbohydrate; &#125; &#125; NutritionFacts类是不可变的。Builder的set方法返回builder对象本身，这样就可以链式调用。下面是客户端代码的样子： NutritionFacts cocaCola = new NutritionFacts.Builder(240, 8) .calories(100).sodium(35).carbohydrate(27).build(); 为了简介，这里省略的参数校验。参数校验在Builder的构造函数和方法中。多参数校验在build方法中 建造者模式也适用于抽象类，抽象类有抽象Builder import java.util.EnumSet; import java.util.Objects; import java.util.Set; // Builder pattern for class hierarchies public abstract class Pizza &#123; public enum Topping &#123;HAM, MUSHROOM, ONION, PEPPER, SAUSAGE&#125; final Set&lt;Topping&gt; toppings; abstract static class Builder&lt;T extends Builder&lt;T&gt;&gt; &#123; EnumSet&lt;Topping&gt; toppings = EnumSet.noneOf(Topping.class); public T addTopping(Topping topping) &#123; toppings.add(Objects.requireNonNull(topping)); return self(); &#125; abstract Pizza build(); // Subclasses must override this method to return &quot;this&quot; protected abstract T self(); &#125; Pizza(Builder&lt;?&gt; builder) &#123; toppings = builder.toppings.clone(); // See Item 50 &#125; &#125; import java.util.Objects; public class NyPizza extends Pizza &#123; public enum Size &#123;SMALL, MEDIUM, LARGE&#125; private final Size size; public static class Builder extends Pizza.Builder&lt;Builder&gt; &#123; private final Size size; public Builder(Size size) &#123; this.size = Objects.requireNonNull(size); &#125; @Override public NyPizza build() &#123; return new NyPizza(this); &#125; @Override protected Builder self() &#123; return this; &#125; &#125; private NyPizza(Builder builder) &#123; super(builder); size = builder.size; &#125; &#125; public class Calzone extends Pizza &#123; private final boolean sauceInside; public static class Builder extends Pizza.Builder&lt;Builder&gt; &#123; private boolean sauceInside = false; // Default public Builder sauceInside() &#123; sauceInside = true; return this; &#125; @Override public Calzone build() &#123; return new Calzone(this); &#125; @Override protected Builder self() &#123; return this; &#125; &#125; private Calzone(Builder builder) &#123; super(builder); sauceInside = builder.sauceInside; &#125; &#125; 客户端实例化对象代码如下： NyPizza pizza = new NyPizza.Builder(SMALL) .addTopping(SAUSAGE).addTopping(ONION).build(); Calzone calzone = new Calzone.Builder() .addTopping(HAM).sauceInside().build(); 建造者模式非常灵活，一个builder对象可以反复构建多个对象，可以通过builder中的方法调用生成不同的对象。建造者模式的缺点就是生成一个对象前要先创建它的builder对象，性能会稍差一些，但为了未来字段更好扩展，建议还是用建造者模式 3. 使用私有构造函数或枚举类型创建单例 单例是只实例化一次的类。当类不保存状态或状态都一致，那么它的对象本质上都是一样的，可以用单例模式创建 实现单例有两种方法。两者都基于私有化构造函数和对外提供 public static 成员，在第一个方法中，该成员是个用final修饰的字段 // Singleton with public final field public class Elvis &#123; public static final Elvis INSTANCE = new Elvis(); private Elvis() &#123; ... &#125; public void leaveTheBuilding() &#123; ... &#125; &#125; 私有构造函数只调用一次，用于初始化public static final 修饰的Elvis类型字段INSTANCE。一旦初始化Elvis类，就只会存在一个Elvis实例。客户端不能再创建别的实例，但是要注意的是拥有特殊权限的客户端可以利用反射调用私有构造函数生成实例 Constructor&lt;?&gt;[] constructors = Elvis.class.getDeclaredConstructors(); AccessibleObject.setAccessible(constructors, true); Arrays.stream(constructors).forEach(name -&gt; &#123; if (name.toString().contains(&quot;Elvis&quot;)) &#123; Elvis instance = (Elvis) name.newInstance(); instance.leaveTheBuilding(); &#125; &#125;); 第二种方法，对外提供的 public static 成员是个静态工厂方法 // Singleton with static factory public class Elvis &#123; private static final Elvis INSTANCE = new Elvis(); private Elvis() &#123; ... &#125; public static Elvis getInstance() &#123; return INSTANCE; &#125; public void leaveTheBuilding() &#123; ... &#125; &#125; 所有对getInsance()方法的调用都返回相同的对象，但是同样可以通过反射调用私有构造函数创建对象。 这两种方法要实现可序列化，仅仅在声明中添加implements Serializable是不够的。还要声明所有实例字段未transient，并提供readResolve方法，否则，每次反序列化都会创建一个新实例。JVM在反序列化时会自动调用readResolve方法 // readResolve method to preserve singleton property private Object readResolve() &#123; // Return the one true Elvis and let the garbage collector // take care of the Elvis impersonator. return INSTANCE; &#125; 实现单例的第三种方法时声明一个单元素枚举 // Enum singleton - the preferred approach public enum Elvis &#123; INSTANCE; public void leaveTheBuilding() &#123; ... &#125; &#125; 这种方法类似于 public 字段方法，但是它更简洁，默认提供了序列化机制，提供了对多个实例化的严格保证，即使面对复杂的序列化或反射攻击也是如此。这种方法可能有点不自然，但是单元素枚举类型通常是实现单例的最佳方法。但是，如果你的单例要继承父类，那么就不能用这种方法 4. 用私有构造函数实施不可实例化 工具类不需要实例化，它里面的方法都是public static的，可以通过私有化构造函数使类不可实例化 // Noninstantiable utility class public class UtilityClass &#123; // Suppress default constructor for noninstantiability private UtilityClass() &#123; throw new AssertionError(); &#125; ... // Remainder omitted &#125; AssertionError不是必须有的，但可以防止构造函数被意外调用（反射） 这种用法也防止了类被继承。因为所有子类构造函数都必须显示或者隐式调用父类构造函数，但父类构造函数私有化后就无法调用 5. 依赖注入优于硬连接资源 有些类依赖于一个或多个资源。例如拼写检查程序依赖于字典。错误实现如下： // Inappropriate use of static utility - inflexible &amp; untestable! public class SpellChecker &#123; private static final Lexicon dictionary = ...; private SpellChecker() &#123;&#125; // Noninstantiable public static boolean isValid(String word) &#123; ... &#125; public static List&lt;String&gt; suggestions(String typo) &#123; ... &#125; &#125; // Inappropriate use of singleton - inflexible &amp; untestable! public class SpellChecker &#123; private final Lexicon dictionary = ...; private SpellChecker(...) &#123;&#125; public static INSTANCE = new SpellChecker(...); public boolean isValid(String word) &#123; ... &#125; public List&lt;String&gt; suggestions(String typo) &#123; ... &#125; &#125; 这两种写法分别是工具类和单例，都假定使用同一个字典，实际情况是不同拼写检查程序依赖不同的字典。 你可能会想取消dictionary的final修饰，并让SpellChecker类添加更改dictionary的方法。但这种方法在并发环境下会出错。工具类和单例不适用于需要参数化依赖对象 参数化依赖对象的一种简单模式是，创建对象时将被依赖的对象传递给构造函数。这是依赖注入的一种形式。 // Dependency injection provides flexibility and testability public class SpellChecker &#123; private final Lexicon dictionary; public SpellChecker(Lexicon dictionary) &#123; this.dictionary = Objects.requireNonNull(dictionary); &#125; public boolean isValid(String word) &#123; ... &#125; public List&lt;String&gt; suggestions(String typo) &#123; ... &#125; &#125; 依赖注入适用于构造函数、静态工厂方法、建造者模式。 依赖注入的一个有用变体是将工厂传递给构造函数，这样可以反复创建被依赖的对象。Java8中引入的Supplier&lt;T&gt;非常适合作为工厂。下面是一个生产瓷砖的方法 Mosaic create(Supplier&lt;? extends Tile&gt; tileFactory) &#123; ... &#125; 尽管依赖注入极大提高了灵活性和可测试性，但它可能会使大型项目变得混乱。通过使用依赖注入框架（如Spring、Dagger、Guice）可以消除这种混乱 6. 避免创建不必要的对象 复用对象可以加快程序运行速度，如果对象是不可变的，那么它总是可以被复用的。 一个不复用对象的极端例子如下 String s = new String(&quot;bikini&quot;); // DON&#x27;T DO THIS! 该语句每次执行都会创建一个新的String实例。&quot;bikini&quot;本身就是一个String实例，改进的代码如下 String s = &quot;bikini&quot;; 这个版本使用单个String实例，而不是每次执行时都会创建一个新的实例。此外，可以保证在同一虚拟机中运行的其它代码都可以复用该对象，只要它们都包含相同的字符串字面量 通常可以使用静态工厂方法来避免创建不必要的对象。例如，Boolean.valueOf(String) 比构造函数Boolean(String) 更可取，后者在Java9中被废弃了。 有些对象的创建代价很高，如果要重复使用，最好做缓存。例如，使用正则表达式确定字符串是否为有效的罗马数字 // Performance can be greatly improved! static boolean isRomanNumeral(String s) &#123; return s.matches(&quot;^(?=.)M*(C[MD]|D?C&#123;0,3&#125;)&quot; + &quot;(X[CL]|L?X&#123;0,3&#125;)(I[XV]|V?I&#123;0,3&#125;)$&quot;); &#125; 这个方法的问题在于它依赖String.matches方法。虽然 String.matches 是检查字符串是否与正则表达式匹配的最简单方法，但它不适合要求高性能的情况下重复使用，因为它在内部为正则表达式创建了一个Pattern实例，并且只使用了一次就垃圾回收了，创建一个Pattern实例代价很大，因为它需要将正则表达式编译成有限的状态机 为了提高性能，将正则表达式显示编译成Pattern实例，作为类初始化的一部分，每次调用匹配的方法时都使用这个实例 // Reusing expensive object for improved performance public class RomanNumerals &#123; private static final Pattern ROMAN = Pattern.compile(&quot;^(?=.)M*(C[MD]|D?C&#123;0,3&#125;)&quot; + &quot;(X[CL]|L?X&#123;0,3&#125;)(I[XV]|V?I&#123;0,3&#125;)$&quot;); static boolean isRomanNumeral(String s) &#123; return ROMAN.matcher(s).matches(); &#125; &#125; 当对象是不可变的时候，我们很容易会想到复用。但是有些情况不那么容易想到复用。例如适配器模式中的适配器，因为适配器中没有它适配对象的状态，所以不需要创建多个适配器。 一个具体的例子是，KeySet是Map中的适配器，使得Map不仅能提供返回键值对的方法，也能返回所有键的Set，KeySet不需要重复创建，对Map的修改会同步到KeySet实例 7. 排除过时的对象引用 Java具体垃圾回收机制，会让程序员的工作轻松很多，但是并不意味着需要考虑内存管理，考虑以下简单的堆栈实现 import java.util.Arrays; import java.util.EmptyStackException; // Can you spot the &quot;memory leak&quot;? public class Stack &#123; private Object[] elements; private int size = 0; private static final int DEFAULT_INITIAL_CAPACITY = 16; public Stack() &#123; elements = new Object[DEFAULT_INITIAL_CAPACITY]; &#125; public void push(Object e) &#123; ensureCapacity(); elements[size++] = e; &#125; public Object pop() &#123; if (size == 0) throw new EmptyStackException(); return elements[--size]; &#125; /** * Ensure space for at least one more element, roughly * doubling the capacity each time the array needs to grow. */ private void ensureCapacity() &#123; if (elements.length == size) elements = Arrays.copyOf(elements, 2 * size + 1); &#125; &#125; 这段代码有一个潜在的内存泄露问题。当栈的长度增加，再收缩时，从栈中pop的对象不会被回收，因为引用仍然还在elements中。解决方法很简单，一旦pop就置空 public Object pop() &#123; if (size == 0) throw new EmptyStackException(); Object result = elements[--size]; elements[size] = null; // Eliminate obsolete reference return result; &#125; 用null处理过时引用的另一个好处是，如果被意外引用的话会立刻抛NullPointerException 另外一个常见的内存泄漏是缓存。HashMap的key是对实际对象的强引用，不会被GC回收。WeakHashMap的key如果只有WeakHashMap本身使用，外部没有使用，那么会被GC回收 内存泄露第三种常见来源是监听器和其它回调。如果客户端注册了回调但是没有显式地取消它们，它们就会一直在内存中。确保回调被及时回收的一种方法是仅存储它们的弱引用，例如，将他它们作为键存储在WeakHashMap中 8. 避免使用终结器和清除器 如标题所说 9. 使用 try-with-resources 优于 try-finally Java库中有许多必须通过调用close方法手动关闭的资源，比如InputStream、OutputStream和java.sql.Connection。 从历史上看，try-finally语句是确保正确关闭资源的最佳方法，即便出现异常或返回 // try-finally - No longer the best way to close resources! static String firstLineOfFile(String path) throws IOException &#123; BufferedReader br = new BufferedReader(new FileReader(path)); try &#123; return br.readLine(); &#125; finally &#123; br.close(); &#125; &#125; 这种方式在资源变多时就很糟糕 // try-finally is ugly when used with more than one resource! static void copy(String src, String dst) throws IOException &#123; InputStream in = new FileInputStream(src); try &#123; OutputStream out = new FileOutputStream(dst); try &#123; byte[] buf = new byte[BUFFER_SIZE]; int n; while ((n = in.read(buf)) &gt;= 0) out.write(buf, 0, n); &#125; finally &#123; out.close(); &#125; &#125; finally &#123; in.close(); &#125; &#125; 使用 try-finally 语句关闭资源的正确代码（如前两个代码示例所示）也有一个细微的缺陷。try 块和 finally 块中的代码都能够抛出异常。例如，在 firstLineOfFile 方法中，由于底层物理设备发生故障，对 readLine 的调用可能会抛出异常，而关闭的调用也可能出于同样的原因而失败。在这种情况下，第二个异常将完全覆盖第一个异常。异常堆栈跟踪中没有第一个异常的记录，这可能会使实际系统中的调试变得非常复杂（而这可能是希望出现的第一个异常，以便诊断问题） Java7 引入 try-with-resources语句解决了这个问题。要使用这个结构，资源必须实现AutoCloseable接口，这个接口只有一个void close方法，下面是前两个例子的try-with-resources形式 // try-with-resources - the the best way to close resources! static String firstLineOfFile(String path) throws IOException &#123; try (BufferedReader br = new BufferedReader(new FileReader(path))) &#123; return br.readLine(); &#125; &#125; // try-with-resources on multiple resources - short and sweet static void copy(String src, String dst) throws IOException &#123; try (InputStream in = new FileInputStream(src);OutputStream out = new FileOutputStream(dst)) &#123; byte[] buf = new byte[BUFFER_SIZE]; int n; while ((n = in.read(buf)) &gt;= 0) out.write(buf, 0, n); &#125; &#125; try-with-resources为开发者提供了更好的异常排查方式。考虑firstLineOfFile方法，如果异常由readLine和不可见close抛出，那么后者异常会被抑制。被抑制的异常不会被抛弃，它们会被打印在堆栈中并标记被抑制。可以通过getSuppressed方法访问它们，该方法是Java7中添加到Throwable中的 像try-catch-finally一样，try-with-resources也可以写catch语句。下面是firstLineOfFile方法的一个版本，它不抛出异常，但如果无法打开文件或从中读取文件，会返回一个默认值 // try-with-resources with a catch clause static String firstLineOfFile(String path, String defaultVal) &#123; try (BufferedReader br = new BufferedReader(new FileReader(path))) &#123; return br.readLine(); &#125; catch (IOException e) &#123; return defaultVal; &#125; &#125;","categories":[{"name":"Java","slug":"Java","permalink":"https://zunpan.github.io/categories/Java/"}],"tags":[{"name":"Effective-Java","slug":"Effective-Java","permalink":"https://zunpan.github.io/tags/Effective-Java/"},{"name":"创建和销毁对象","slug":"创建和销毁对象","permalink":"https://zunpan.github.io/tags/%E5%88%9B%E5%BB%BA%E5%92%8C%E9%94%80%E6%AF%81%E5%AF%B9%E8%B1%A1/"}]},{"title":"跨域问题","slug":"跨域问题","date":"2022-06-22T09:12:37.000Z","updated":"2023-09-24T04:27:40.285Z","comments":true,"path":"2022/06/22/跨域问题/","link":"","permalink":"https://zunpan.github.io/2022/06/22/%E8%B7%A8%E5%9F%9F%E9%97%AE%E9%A2%98/","excerpt":"","text":"同源策略 同源策略是浏览器的重要安全策略。大致来说，不同域（协议+域名/ip+端口）生成的cookie只能给这个域使用 跨域出现与解决 下面的演示是在hosts文件中添加以下配置 127.0.0.1 zhufeng-test.163.com 不添加的话，把zhufeng-test.163.com换成localhost或者127.0.0.1是一样的 假如我们只有后端，它的域是http://zhufeng-test.163.com:8080，它有一个get方法是setCookie，那么访问http://zhufeng-test.163.com:8080/setCookie，此时可以在Response Cookies中找到这个cookie，对这个域下的接口访问时会自动带上这个域所有可见的cookie（springboot开启allowCredentials，前端axios需要自己开启withCredentials: true） 现在我们有前端了，前后端分离运行，前端运行在http://zhufeng-test.163.com:8081 因为后端生成的cookie的domain是 zhufeng-test.163.com，所以浏览器访问相同域名的前端也可以读取到这个cookie。 但是前端不能访问后端的其它接口，因为它们端口不同，发生了跨域，浏览器不会带上cookie 给后端配置一下跨域，.allowedOrigins(“http://zhufeng-test.163.com:8081/”)表示允许前端http://zhufeng-test.163.com:8081访问后端，不开会返回状态码200的跨域错误，.allowCredentials(true)不开时前端访问后端不会带上后端域名可见的cookie @Configuration public class CorsConfig implements WebMvcConfigurer &#123; @Override public void addCorsMappings(CorsRegistry registry) &#123; //项目中的所有接口都支持跨域 registry.addMapping(&quot;/**&quot;) // 所有地址都可以访问，也可以配置具体地址 // .allowedOrigins(&quot;http://zhufeng-test.163.com:8081/&quot;) .allowedOriginPatterns(&quot;*://*:*/&quot;) // &quot;GET&quot;, &quot;HEAD&quot;, &quot;POST&quot;, &quot;PUT&quot;, &quot;DELETE&quot;, &quot;OPTIONS&quot; .allowedMethods(&quot;*&quot;) // 允许前端携带后端域名可见的cookie .allowCredentials(true) .maxAge(3600); &#125; &#125;","categories":[{"name":"基础知识","slug":"基础知识","permalink":"https://zunpan.github.io/categories/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}],"tags":[{"name":"CORS","slug":"CORS","permalink":"https://zunpan.github.io/tags/CORS/"},{"name":"Java","slug":"Java","permalink":"https://zunpan.github.io/tags/Java/"}]},{"title":"CryptographyⅠ笔记","slug":"CryptographyⅠ笔记","date":"2022-06-04T06:51:30.000Z","updated":"2023-09-24T04:27:40.272Z","comments":true,"path":"2022/06/04/CryptographyⅠ笔记/","link":"","permalink":"https://zunpan.github.io/2022/06/04/Cryptography%E2%85%A0%E7%AC%94%E8%AE%B0/","excerpt":"","text":"斯坦福教授Dan Boneh的密码学课程《Cryptography》 课程链接：https://www.bilibili.com/video/BV1Ht411w7Re 讲义：https://www.cs.virginia.edu/~evans/courses/crypto-notes.pdf 目标：课程结束后可以推导加密机制的安全性，可以破解不安全的加密机制 绪论 密码学用途 安全通信，例如https，Bluetooth 加密磁盘文件，例如EFS 内容保护，例如DVD使用CSS 用户认证 高阶用途： 外包计算（同态加密） 安全多方计算 零知识证明 安全通信 在web服务中使用https协议进行通信，起到安全通信作用的是TLS协议，TLS协议主要包括两部分： 握手协议：使用公钥密码体制建立共享密钥 Record层：使用共享密钥传输数据，保证数据的机密性和完整性 加密磁盘文件 加密磁盘文件从哲学角度来看就是今天的Alice和明天的Alice进行安全通信 对称加密 Alice和Bob双方共享密钥k，攻击者不知道密钥k，他们使用两个算法进行通信，分别是加密算法E，解密算法D。加密算法以原信息和密钥为输入产生相应密文；解密算法正好相反，以密文和密钥为输入，输出原信息。加密解密算法是公开的，只有密钥k是保密的。我们应当使用公开的算法，因为它的安全性经过业内人士的审查。 密钥使用次数 可以根据密钥使用次数分为一次使用的密钥，多次使用的密钥 一次使用的密钥只用来加密一个信息，例如加密邮件，为每一封邮件都生成一个新的密钥。 多次使用的密钥可以用来加密多个消息，例如用同一密钥加密文件系统的许多文件，多次使用的密钥需要更多机制来确保加密系统是安全的 一些结论 任何依赖可信第三方的计算都可以不借助可信第三方完成 密码算法提出三步骤 明确描述威胁模型。例如数字签名算法中，攻击者如何攻击数字签名？伪造签名目的何在？ 提出算法构成 证明在该威胁模型下，攻击者破解了算法等同于破解了根本性难题。意思就是算法的安全性由根本性难题保证，例如RSA依赖大整数质因数分解困难问题 密码历史 对称密码历史 替换式密码 替换式密码的密钥是一张替换表 例如：凯撒密码 凯撒密码是没有密钥或者只有一个固定密钥的替换式密码 假设替换式密码的密钥只能使用26个英文字母，它的密钥空间有多大？是26!26!26!，约等于2882^{88}288，也就是说可以用88位比特表示密钥，这个密钥空间是足够大的，但是这个加密算法不安全 如何破解这个加密算法？ 使用字母频率来破解。在英文文献中，出现频次最高的字母是e，那么我们统计密文中出现频次最高的那个字母，例如c，那么这个替换式密码的密钥k中很可能包含了e-&gt;c；同样的道理，出现频次第二高的是t，第三高的是a，分别找到密文中对应的字母进行还原 使用字母配对（组合）。在英语中，最常见的二元配对有“he”，“an”，“in”，“th”。第一种方法还原出了eta，根据配对规则可以还原配对中的另一个字母。 这种攻击方式叫唯密文攻击（CT only attack），替换式密码抵御不了这种攻击 Vigener密码 Vigener密码的密钥是一个单词，加密算法是将密钥重复直到和明文一样长，与明文逐位求和模26得到密文，解密算法是对密文逐位减去密钥 如何破解Vigener密码？ 首先假设攻击者已知密钥长度（未知也不影响破解，枚举长度即可），例如密钥长度为6，那么攻击者可以将密文按照每6个字母一组进行分组。然后看每组的第一个字母，他们都是由同一个字母加密的，例如上图中k的字母C。我们同样可以使用唯密文攻击，在每组的第一个字母中找到出现频次最高的字母，它们对应的明文位为E，然后对应的密钥为为密文位-E = 密钥位。同样的方法对密钥第2位直到最后一位执行。这种破解方法的实质是多次使用唯密文攻击 轮轴机 最早的轮轴机：Hebern machine（一个轮轴） 本质上是一个替换式密码。按下A，假如A加密成了T，此时轮轴机转动一格，再次按下A，A就加密成了S。轮轴机一经提出很快就被破解了，同样是唯密文攻击。 最著名的轮轴机：Enigma（3-5轮轴） 数字时代的密码 1974：DES（keys = 2562^{56}256, block size = 64 bits) Today: AES(2001), Salsa20(2008) (and others) 离散概率 有限集合与概率分布 事件 事件是有限集合的子集 事件的并集发生的概率存在上界 随机变量 随机变量是一个函数 X：U(有限集合)-&gt;V(某个集合) 集合V是随机变量取值的地方。例如定义随机变量X:{0,1}n⟶{0,1}X:\\{0,1\\}^n\\longrightarrow \\{0,1\\}X:{0,1}n⟶{0,1};X(y)=lsb(y)∈{0,1}X(y)=lsb(y) \\in \\{0,1\\}X(y)=lsb(y)∈{0,1} 这个随机变量将n位长度的字符串集合映射成了只有0和1的两个元素的集合，具体做法是取最低位。任意一个n位长度的字符串会被映射出0或者1 均匀随机变量 随机化算法 随机化算法相比于确定的算法，在原像中加入了随机因子，所以对同一条消息进行随机化，结果一般是不同的 独立性 异或 异或就是逐位模2和 异或在密码学中非常重要，在一个有限集合上有一个随机变量Y，有一个独立的均匀随机变量X，那么Z:=Y⨁XZ:=Y\\bigoplus XZ:=Y⨁X 仍然是这个集合上的均匀随机变量 生日悖论 r指的是随机变量的值 独立同分布举例：假设U = {00,01,10,11}表示抛硬币两次的结果，令随机变量X：U→{0,1}X：U\\rightarrow\\{0,1\\}X：U→{0,1}表示第一次抛硬币的结果，Y：U→{0,1}Y：U\\rightarrow\\{0,1\\}Y：U→{0,1}表示第二次抛硬币的结果。X和Y是相互独立的且概率一样，所以这两个随机变量是独立同分布的 流密码 流密码是一种对称密码。 对称密码的严格定义：定义在K,M,C\\mathcal{K,M,C}K,M,C上的一对“有效”加解密算法(E,D)(E,D)(E,D) K\\mathcal{K}K指密钥空间，M\\mathcal{M}M指明文空间，E\\mathcal{E}E指密文空间 E:K×M→CE: \\mathcal{K\\times M\\rightarrow C}E:K×M→C D:K×C→MD:\\mathcal{K\\times C\\rightarrow M}D:K×C→M 即 ∀m∈M,k∈K:D(k,E(k,m))=m\\forall m \\in \\mathcal{M}, k \\in \\mathcal{K}: D(k, E(k, m))=m∀m∈M,k∈K:D(k,E(k,m))=m &quot;有效&quot;这个词在理论密码学家看来，时间复杂度是多项式时间内的就是有效的，真正运行时间取决于输入规模。在应用密码学家看来，可能是加解密1GB数据要在10s内就算有效。 特别注意，加密算法是随机化算法，解密算法是确定性的算法 一次性密码本 一次性密码本的明文空间、密文空间、密钥空间均相同 一次性密码本的加密算法是将明文与密钥做异或运算，解密也是将密文与密钥做异或运算，根据异或运算的结合律可以知道加解密算法是正确的 如何证明这个密码的安全性？ 信息论的创始人香农提出一个观点：不能从密文得出关于明文的任何信息 攻击者截获一段密文，如果满足下面的等式，即不同明文经过同一个密钥加密后等于这个密文的概率是相同的，那么攻击者就无法得知真正的明文，这种密码是完美保密性的 一次性密码本是完美保密性的 完美保密性只是意味着没有唯密文攻击，并不意味着在实际使用中是安全的 香农在给出完美保密性的证明后又给出了一个定理，想要完美保密性，密钥长度要大于等于明文长度，所以完美保密性的密码是不实用的 流密码（Stream ciphers） 流密码是使用的一次性密码本，它的思想是使用伪随机密钥代替随机密钥 伪随机数生成器是一个函数，它可以将s位的01比特串扩展成n位的01比特串，n&gt;&gt;s。注意，伪随机数生成器算法是不具备随机性的，具备随机性的是种子 流密码使用伪随机数生成器（PRG），将密钥当做种子，生成真正用于加密的比特串 因为密钥长度远小于明文长度，所以流密码并不是完美保密性的 流密码的安全性不同于完美保密性，它需要另外一种安全性定义，这种安全性依赖具体的伪随机数生成器 安全的伪随机数生成器必须是不可预测的。 假设伪随机数生成器是可预测的，那么存在某种算法可以根据PRG的前i位推测出后面的n-i位。在SMTP协议中，明文头是“from:” , 攻击者可以用这段已知的明文与截获的密文做异或得出PRG输出的前i位，再根据预测算法得出完整的密钥，再与密文异或得出明文 严格定义PRG是可预测的： 存在“有效”算法A，对PRG的前i位做计算，输出的值与PRG的第i+1位相同的概率大于1/2+ϵ,(ϵ≥1/230)1/2+\\epsilon, (\\epsilon \\ge 1/2^{30})1/2+ϵ,(ϵ≥1/230), ϵ\\epsilonϵ大于1/2301/2^{30}1/230时是不可忽略的 不安全的PRG例子 线性同余法的PRG 可忽略和不可忽略 针对一次性密码本和流密码的攻击 两次密码本攻击 流密码的密钥一旦使用两次就不安全 WEP还有一个问题就是它的PRG使用的是RC4，它的不可预测性是较弱的，存在有效算法可以从四万帧中恢复PRG的输出 完整性攻击 一次性密码本和流密码都只保护数据的机密性，但不保证完整性，攻击者可以修改将密文与攻击者的置换密钥做异或从而定向影响明文，这种攻击无法被检测出来 流密码实际应用 现代流密码：eStream eStream支持5种PRG，这里讲其中一种，这种PRG的输入除了种子（密钥）还有一个随机数，好处是不用每次更换密钥，因为输入还包括随机数，整体是唯一的 eStream中同时支持软硬件的流密码Salsa20 图中的 || 并不是简单的拼接。这个PRG首先构造一个64kb的字符串，里面包含k，r，i（从0开始），通过多次一一映射h，最后与原字符串进行加法（不是异或）得出输出 PRG的安全定义 PRG的输出与均匀随机变量的输出不可区分 统计测试 为了定义不可区分，首先引入统计测试，统计测试是一个算法，输入值是“随机数”，输出0表示不是真随机数，1表示是真随机数 上图的例子1，这个统计测试输出1的情况当且仅当随机数中0和1的个数差小于等于随机数的长度开根号乘以10 统计测试算法有好有坏，可能并不随机的字符串也认为是随机的。所以我们需要评估统计测试算法的好坏 我们定义一个变量叫做优势，优势是统计测试算法相对于伪随机数生成器的，优势越接近1，统计测试算法越能区分伪随机数和真随机数 PRG的密码学安全定义 PRG是安全的当且仅当不存在有效的统计算法，它的优势是不可忽略的。即所有统计算法都认为PRG的输出是真随机数 但是，我们不能构造一个PRG并证明PRG是安全的，即不存在有效的统计算法。但是我们还是有大量的PRG候选方案 我们可以证明当PRG是可预测时，PRG是不安全的 当存在一个好的预测算法和好的统计测试算法，如下图。那么统计测试算法可以以一个不可忽略的 ϵ\\epsilonϵ 分辨出伪随机数和真随机数 姚期智证明了上面命题的逆命题也成立，即当PRG是不可预测时，PRG是安全的 不可区分的更通用的定义如下 语义安全 什么是安全的密码？ 攻击者不能从密文中恢复密钥 攻击者不能从密文中恢复明文 香农认为不能从密文中获得任何关于明文的信息才是安全的密码 香农的完美保密性定义约束太强，可以用计算不可区分代替 一次性密码本的语义安全 定义语义安全的的方式是通过两个实验，攻击者发送两个明文信息，挑战者（应该是被挑战者）随机选取密钥，做两次实验，第一次实验加密第一个信息，第二次实验加密第二个信息，攻击者判断密文对应的明文是哪个 上图定义了语义安全的优势，等于实验0中攻击者输出1的概率和实验1中输出1的概率之差的绝对值。简单理解一下，假如攻击者不能区分两次实验，那么实验0和实验1的输出1的概率是一样的，那么攻击者的优势为0，不能区分两次实验，意味着加密算法是语义安全的；如果攻击者能区分实验0和实验1，那么概率差不可忽略，攻击者有一定的优势区分两次实验 事实上，一次性密码本不仅是语义安全的，而且是完美保密性的 安全的PRG可以构成语义安全的流密码 我们做两次实验证明流密码的语义安全，第一次使用伪随机数生成器，第二次使用真随机数 分组密码 分组密码也属于对称密码，将明文分解成固定大小的分组，使用密钥加密成同样大小的密文 分组密码的加密过程是将密钥扩展成多个，使用轮函数多次计算分组得出密文 PRPs和PRFs K表示密钥空间，X表示明文空间，Y表示密文空间 给定密钥后，伪随机置换的加密函数是一个一一映射，也就意味着存在解密函数。伪随机置换和分组密码十分相似，有时候会混用术语 安全的PRFs Funs[X,Y]表示所有从明文空间到密文空间的真随机函数的集合，易知这个集合大小等于明文空间大小的密文空间大小次，而伪随机函数的集合大小由密钥决定，一个密钥决定了一个伪随机函数，一个安全的PRF与真随机函数不可区分 上图的G问题在于x=0时，输出固定了，攻击者可以通过x=0时的输出是否为0来判断他在和真随机函数交互还是伪随机函数，因为x=0，输出为0的概率实在太低了，等于密文空间的倒数 可以使用安全的PRF来构造安全的PRG DES DES的轮（回合）函数使用的是Feistel网络，核心思想是每个分组2n bits，右边n bits原封不动变成下一层分组左边n bits，左边n bits经过伪随机函数转换再和右边n bits异或变成下一层右边n bits 易知这个网络是可逆的，注意，不要求伪随机函数是可逆的 定理：如果伪随机函数使用的密钥是相互独立的，那么Feistel网络是一个安全的PRP 回合函数f由F根据回合密钥推导出来，回合密钥由主密钥推导得来，IP和IP逆是伪随机置换，和DES安全性无关，仅仅是标准要求 线性函数：函数可以表示成矩阵乘以入参 如果所有的置换盒子都是线性的，那么整个DES就是线性的，因为只有DES算法中只有置换盒子可能是非线性的，其它就是异或、位移等线性运算。如果是线性DES，那么存在一个矩阵B，DES可以写成B乘以一个包含明文和回合密钥的向量 针对DES的攻击 密钥穷举攻击 给定一些明文密文对，找到一个密钥使得明文密文配对，这个密文极大概率是唯一的 为了抵抗穷举攻击，衍生出了3DES 2DES存在安全性问题 针对分组密码的攻击 旁道攻击 通过测试加解密的时间或者功耗来推测密钥 错误攻击 通过外部手段影响加解密硬件，比如提高时钟频率、加热芯片，使得加密的最后一回合发生错误，根据错误信息可以推测出密钥 这两种攻击需要先窃取到硬件，比如上图的IC卡 线性和差分攻击 同样是需要给定明文密文对，推测密钥，比穷举攻击效率更高 加密算法中使用了线性函数导致下面等式以一个不可忽略的概率成立 图中的MAJ表示majority 量子攻击 AES AES基于代换置换网络构建，和Feistel最大的区别在于，在这个网络的每一回合函数会影响每一位bit 使用PRGs构造分组密码 分组密码实质是PRP，首先考虑能不能使用PRG构造PRF 使用安全的PRG可以构造一个安全的PRF，但是并不实用 有了安全的PRF，我们可以使用Luby-Rackoff定理转换成PRP，因此可以使用PRG来构造分组密码，但是不如AES启发性PRF实用 使用分组密码 很显然，真随机置换空间和伪随机置换空间大小一样，所以它是安全的PRP 这个PRP不是安全的PRF，因为明文空间太小了 使用一次性密钥的分组密码 ECB的问题在于相同的明文会加密成相同的密文，攻击者可能不知道明文内容，但也会从中学到明文的一些信息 攻击者来挑战算法，本来不应该知道两张加密图片的区别，但是ECB将头发加密成了很多1，头发又重复出现，这样密文就会出现很多1，攻击者就能根据这个区别分辨两张图片 ECB被用来加密长于一个分组的消息时不是语义安全的 安全的电子密码本是为每个分组生成一个随机密钥进行加密，类似于AES的密钥扩展 使用密钥多次利用的分组密码 密钥多次利用的分组密码的选择明文攻击就是进行多次的语义安全实验（CPA安全） 确定的加密对于选择明文攻击不可能是语义安全的，所以多次使用一个密钥加密时，相同的明文，应该产生不同的输出，有两种方法。 第一种：随机化算法 第二种：基于新鲜值的加密 新鲜值不必随机但是不能重复 随机化的新鲜值和上面的随机化算法是一样的 基于新鲜值的加密的选择明文攻击下的安全性 密钥多次利用的运行方式（CBC） CBC：密码分组链接模式 L是加密的明文长度，单位是分组。q是在CPA攻击下，攻击者获得的密文数，现实意义是使用某个密钥加密的明文数量 密钥多次利用的运行方式（CTR） CTR：计数器模式 CTR不使用分组密码，使用PRF足够 信息完整性 本节先考虑信息完整性，不考虑机密性。信息完整可以保证公开信息没有被篡改，例如广告投放商不在乎广告的机密性，但在乎广告有没有被篡改 MACs CRC是循环冗余校验算法，为检测信息中的随机发生的错误而设计，并非针对恶意错误 安全的MACs MAC可以帮助抵御数据篡改，但是无法抵御认证消息的交换 构造安全的MAC 可以用安全的PRF来构造安全的MAC 但是，PRF的输出不能太短，不然攻击者能以一个不可忽略的概率猜出消息认证码 不等式右边 1/|Y| 是因为攻击者可以猜 PRF的输出不能太短，同时为了增大输入空间，需要有一些新的构造，将小输入空间的PRF转换成大输入空间的PRF 如果PRF是安全的，那么截断PRF的输出，依然是安全的，当然，如果要用PRF构造MAC，不能截到太短 CBC-MAC和NMAC n是底层PRF的分组大小 L是分组大小 这两种构造的最后一步都至关重要，没有最后一步，攻击者可以实施存在性伪造（扩展攻击）。 第二种构造的原因很简单，攻击者询问m的函数结果，将结果与w分别作为函数的密钥和明文（F公开），计算函数结果 第一种构造的原因 MAC padding 当数据的长度不是分组长度的倍数时，需要填充数据 全部补0会有出现存在性伪造 填充函数必须是一一映射的 ISO的这种填充方法，不管是不是分组长度的倍数都要进行填充 并行的MAC CBC-MAC和NMAC将一个处理短信息的PRF转换成一个处理长信息的PRF，这两种算法是串行的 P是某个有限域上的乘法 一次性MAC HMAC(略) 抗碰撞章节细说 抗碰撞 抗碰撞在信息完整性中扮演着重要角色。我们说MAC系统是安全的，如果它在选择信息攻击下，是不可被存在性伪造的。前面4种MAC构造是通过PRF或随机数来构造的，现在通过抗碰撞的哈希函数来构造MAC 抗碰撞：没有有效算法A，能以一个不可忽略的概率找到hash函数的碰撞值 可以用抗碰撞哈希函数和处理短信息的安全MAC组合成处理长信息的安全MAC 只用抗碰撞哈希函数也可以构造安全的MAC，而且不像之前的MAC需要密钥，但是需要一个只读空间用来存信息的hash值。这种方式非常流行 针对抗碰撞哈希函数的通用攻击（生日攻击） 针对分组密码的通用攻击是穷举攻击，抵御穷举攻击的方法是增大密钥空间；为了抵御生日攻击，哈希函数的输出也必须大于某个下界 下面这种攻击算法通常几轮就能找到碰撞值 生日悖论的证明 下面的证明在非均匀分布时，n的下界更低 通用攻击 使用Merkle-Damgard机制组建抗碰撞的哈希函数 下面的IV是永远固定的，写在代码和标准里的值，填充函数在信息长度是分组长度倍数的时候也会填充一个哑分组 这种机制流行的原因是只要小hash函数（即上面的压缩函数）是抗碰撞的，那么大hash函数也是抗碰撞的 构建抗碰撞的压缩函数 使用分组密码来构建压缩函数，将信息作为密钥。SHA函数都使用了Davies-Mayer压缩函数 另外一类压缩函数是由数论里的困难问题构建的，这类压缩函数的抗碰撞性规约于数论难题，也就是说破解了压缩函数的抗碰撞性也就破解了数论难题。但是这类压缩函数很少使用，因为分组密码更快 SHA256 使用了Merkle-Damgard机制和Davies-Mayer压缩函数，底层分组密码用的是SHACAL-2 HMAC 针对MAC验证时的计时攻击 认证加密 到目前为止,我们了解了机密性和信息完整性, 我们将构建同时满足这两种性质的密码 机密性: 在选择明文攻击下满足语义安全 机密性只能抵抗攻击者窃听而不能抵抗篡改数据包 客户端加密数据后通过网络发送给服务器, 服务器解密后分发至对应端口 如果没有保证完整性, 那么攻击者可以拦截客户端的数据包, 把端口改成自己能控制的服务器的端口 可以抵抗CPA攻击的CBC分组密码不能抵抗篡改数据包, 攻击者可以简单地修改IV从而修改加密后的端口 攻击者甚至不用进入服务器, 直接利用网络来攻击。在CTR模式下，攻击者截获数据包，将加密的校验和与t异或，将加密的数据与s异或。CTR模式的特点是对密文异或，解密后等于对明文异或。攻击者重复很多次攻击，直到得到足够数量的合法的t、s对，从而恢复数据D（插值法？） 这种攻击叫做选择密文攻击。攻击者提交他选择的密文，是由他想解密的密文所推出的，然后看服务器响应，攻击者可以从中学到明文的一些信息。重复这个操作，用许多不同t、s值，攻击者可以还原明文 选择明文攻击下的安全不能保证主动攻击（前面两种攻击）下的安全 如果要保证信息完整性但不需要机密性，使用MAC 如果同时保证信息完整性和机密性，使用认证加密模式 认证加密定义 目标：提供选择明文攻击下的语义安全和密文完整性 密文完整性：攻击者不能造出合法的密文 CBC是选择明文攻击（CPA）下的安全密码，它的解密算法从不输出bottom符号，所以它不能直接被用作认证加密 认证加密不能抵抗重放攻击 选择密文攻击下的安全 攻击者的能力是既能选择明文攻击也能选择密文攻击，也就是说既能拿到想要的明文的加密结果，也能拿到想要的密文的解密结果 攻击者选择的密文不能是CPA的返回 CBC密码不是选择密文安全的，因为在选择密文时，通过对IV异或修改CPA的返回，同时由于CBC的特性，CCA的返回时对明文的异或，这样攻击者可以以1的优势赢下语义安全实验 如果密码能够提供认证加密，那么它就是选择密文安全的 所以认证加密能够选择密文攻击，但是不能防止重放攻击和旁道攻击 认证加密直到2000年才被正式提出，在这之前，已经有CPA安全的密码和安全的MAC，当时的工程师想将两者组合，但不是所有的组合可以作为认证加密 SSH的MAC签名算法的输出会泄漏明文中的一些位； SSL的加密和MAC算法之间会有一些不好的互动导致选择密文攻击。IPsec无论怎么组合CPA安全的密码和安全的MAC都可以作为认证加密。SSL的特点是MAC-than-ENC，IPsec的特点是ENC-than-MAC 这几种认证加密都支持AEAD（认证加密与关联数据，例如ip报文的报文头是关联数据不加密，报文体用加密，整个报文需要认证）。MAC对整个报文使用，加密只对报文体使用 aad是需要认证、但不需要加密的相关数据，data是需要认证和加密的数据 直接从PRP构造认证加密 OCB比前面几种认证加密快的多，但没有被广泛使用因为各种各样的专利 认证加密的例子 针对认证加密的攻击 零碎 本章讲对称密码的一些零碎 密钥推导 KDF：key drivation function 源密钥由硬件随机数生成器生成或者密钥交换协议生成 CTX：参数上下文，每个进程的ctx不同 当源密钥服从均匀分布，我们使用PRF来作为密钥推导函数（其实就是用PRF作为伪随机数生成器） 当源密钥不服从均匀分布，那么伪随机函数的输出看起来就不随机了，源密钥不服从均匀分布的原因可能是密钥交换协议的密钥空间的子集是均匀分布，或者伪随机数生成器有偏差 构建KDF的机制 先提取再扩展 因为源密钥可能不是均匀的，我们使用一个提取器和一个随机选择的固定的但可以不保密的盐值将源密钥转换为服从均匀分布的密钥k，然后再使用PRF扩展密钥 HMAC既用于PRF进行扩展，又用于提取器 基于密码的KDF PBKDF通过多次迭代哈希函数推导密钥 确定性加密 确定性加密总是把给定明文映射到同一个密文。 为什么需要确定性加密？假设有个加密数据库和服务器，服务器用k1加密索引，用k2加密数据，如果加密是确定的，服务器请求数据时可以直接使用加密后的索引作为查询条件请求数据 确定性加密有致命缺点就是不能抵御选择明文攻击，攻击者看到相同的密文就知道他们的明文是相同的 解决方法是不要用同一个密钥加密同一个消息两次，要么密钥从一个很大空间随机选择，要么明文就是唯一的，比如说用户id 确定性加密的CPA安全 在标准的选择明文攻击实验基础上，Chal不会给相同的m0加密，不会给相同的m1加密。注意上面的图40不是标准的确定性加密的CPA实验，因为攻击者两次查询的m0都是同一个 一个常见的错误是，固定IV的CBC不是确定性CPA安全的，0n1n0^n1^n0n1n表示消息有两个分组，第一个分组全0，第二个分组全1 固定IV的CTR也是不安全的 可以抵御确定性CPA的确定性加密 确定性加密是需要的，但是不能抵御选择明文攻击，因为攻击者看到两个相同的密文就知道了对应的明文是一样的。我们对确定性加密降低选择明文攻击的能力，加密者不使用一个密钥多次加密同样的明文，这样叫确定性CPA。 构造1：合成的IV（SIV） CPA安全的密码会有一个随机值，我们用PRF生成这个随机值 SIV天然提供密文完整性，不需要MAC就能作为DAE（确定性认证加密），例如SIV-CTR 当需要确定性加密，特别是明文很长时，适合用SIV，如果明文很短，比如说少于16个字节，可以用构造2 构造2：仅仅使用一个PRP 实验0，攻击者看到q个随机值，实验1中，攻击者也看到q个随机值，两次实验的结果的概率分布是一样的，攻击者无法区分。这种构造不能保证密文完整性。同时只能加密16个字节 我们先考虑如何将PRP扩展成一个大的PRP EME有两个密钥K，L，L是由K推出的。先为每个分组用L推导出一个密码本作异或，然后用PRP加密得到PPP，将所有PPP异或得到MP，再用PRP加密MP得到MC。然后计算MP异或MC，得到另外一个密钥M用于推导更多密码本，分别对PPP异或得到CCC，然后把所有这些CCC异或得到CCCO，再用PRP加密再异或密码本 现在考虑增加完整性 微调加密（Tweakable encryption） 先以硬盘加密问题引入微调加密， 硬盘扇区大小是固定的，明文和密文空间必须一致，我们最多可以使用确定性加密，因为随机性加密需要额外空间来放随机数，完整性需要额外空间放认证码 定理：如果确定性CPA安全的密码的明文空间和密文空间一样，那么这个密码一定是个PRP 这个微调分组密码的安全实验与常规的分组密码安全实验区别在于，在常规分组密码中，攻击者只能与一个置换进行互动，目标是分辨自己在和伪随机置换交互还是在和一个真随机置换交互。而在微调分组密码的安全实验中，攻击者与|T|个随机置换交互，目标是区分这|T|个随机置换是真是伪 保格式加密（Format Preserving encryption） pos机刷卡时，我们希望卡号只在终端和银行可见，但是中间又有些服务商也想得到&quot;卡号&quot;，我们可以用将卡号加密成卡号格式的密文。 我们截断使用PRF，明文后面补0（例如AES就补到128位），密文截断，然后带入Luby-Rackoff构造PRP 密钥交换 现在我们知道两个用户可以通过共享一个密钥来保护通信数据，问题是，这两个用户如何产生共享密钥，这个问题将把我们带入公钥密码的世界。我们先看一些玩具性质的密钥交换协议。 能否设计出可以抵御窃听和主动攻击的没有可信第三方的密钥交换协议？可以的，这就是公钥密钥的出发点 不需要TTP的密钥交换 首先考虑攻击者只能窃听不能篡改消息，能不能只使用对称密码体系的算法来实现不需要TTP的密钥交换？ 可以的，首先给出puzzle定义：需要花一些功夫解决的问题。例如已经给出AES密钥的前96位，明文固定，那么枚举2322^{32}232个可能的后32位密钥可以找到能正确解密 Alice准备2322^{32}232个puzzle，全部发给Bob，Bob选择一个然后开始枚举，只要解密出的原文开头包含&quot;Puzzle&quot;, 对应的k作为共享密钥，x发送给Alice，Alice就知道Bob选择了哪个 这个协议不实用但是有一个很好的想法，参与者花费线性的时间，而攻击者必须花费平方的时间，当攻击者想破解这个协议，有一个“平方鸿沟”横亘在参与者与攻击者的工作之间。 只用对称密码体系，我们不能建立一个更大的“鸿沟” 我们需要具备非常特殊性质的函数，为了构建这些函数，我们必须依赖某些代数 Diffie-Hellman协议 这是第一个实用的密钥交换机制。 同样，我们考虑攻击者只能窃听不能篡改。我们尝试建立参与者与攻击者之间的指数级鸿沟 Diffie-Hellman协议开创了密码学的新纪元，现在不仅仅是关于开发分组密码，而且是关于设计基于代数的协议 下面有张表是不同密钥长度的分组密码安全性等价于对应模数的DH函数安全性，如果用椭圆曲线，模数可以更小 上面的协议当存在主动攻击（中间人攻击）时就不安全 事实上，上面的协议也可以改成无交互的 一个开放的问题，两个参与方的密钥交换使用DH即可，三个参与方的密钥交换使用Joux提出的方法，四个及以上还没有有效方法 公钥加密 这是另外一种密钥交换的方法 在公钥加密中，没有必要赋予攻击者实施选择明文攻击的能力。因为在对称密钥系统中，攻击者必须请求他选择的明文的加密，而在公钥系统中，攻击者拥有公钥，所以他可以自己加密任何他想加密的明文，他不需要Chal的帮助来计算他选择的明文的加密。因此在公钥的设定中，选择明文攻击是与生俱来的，没有理由给攻击者多余的能力去实施选择明文攻击（公钥加密也是随机性的，每次密文不同） 可以抵抗窃听但也不能抵抗中间人攻击 数论简介 扩展欧几里得算法是已知最有效的求元素模逆的方法（也给了我们求模线性方程的方法） 注意这里是n，不是N，n=logN 费马小定理和欧拉定理 费马小定理给了我们另一个计算模质数逆的方法，但是与扩展欧几里得算法有两个不足，首先它只能用在质数模上，其次算法效率更低 我们可以用费马小定理以极大概率生成一个随机质数（期望是几百次迭代），这是一个简单但不是最好的方法 欧拉证明了Zp∗Z_p^*Zp∗​是一个循环群(p是素数)，也就是∃g∈(Zp)∗\\exist g\\in (Z_p)^*∃g∈(Zp​)∗ 使得 $ {1,g,g2,g3,…,g{p-2}}=(Z_p)*$ 有限子群的阶必然整除有限群的阶 欧拉定理，费马小定理的直接推广，适用于合数 模高次方程 0也是二次剩余，所以有(p-1)/2+1 《信息安全数学基础》P146，证明当p是形如4k+3的素数时，解的形式如下。这里Dan讲了p不是这种形式的素数时仍然是有有效的随机算法来求解 当模数是合数时且指数大于1时，同余式的解并不好找 一些算法 分组用32位表示是为了乘法不溢出 指数运算非常慢 模运算的一些难题 质数模的难题 合数模的难题 Z(2)(n)Z_{(2)}(n)Z(2)​(n)表示两个位数相同的质数乘积的集合 基于陷门置换的公钥加密 公钥密码两个作用，一是会话建立（即对称密钥交换），二是非交互式应用 选择密文攻击（CCA）下的安全，有时可以缩写成选择密文攻击下的不可区分性（IND-CCA） 当攻击者可以篡改密文时，将以优势1赢下这个CCA游戏 构建CCA安全的公钥加密系统 陷门函数特点是单向的，只有私钥持有人才能做逆向计算 单向陷门函数只加密一个随机值，随机值用来生成对称密钥，单向陷门函数的私钥持有人可以恢复随机值进而恢复密钥 不能直接用单向陷门函数加解密明文，因为算法是确定的，就不可能是语义安全的，也会存在许多类型的攻击 构建一个陷门函数 本节构建一个经典的陷门函数叫做RSA 随机选取ZNZ_NZN​中的随机元素，这个元素很可能也在ZN∗Z_N^*ZN∗​中，即该元素很可能是可逆的 （虽然x大概率是可逆的，但是如果不可逆怎么办？） 单向陷门函数是安全的，对称密码可以提供认证加密，H是random oracle，即H是某个从ZNZ_NZN​映射到密钥空间的随机函数。那么这个公钥系统就可以抵抗选择密文攻击 不要直接用RSA来加密明文！！！因为RSA是确定性的函数，因此不可能是语义安全的 （直接用RSA加密密钥会被破解，但是ISO标准是用RSA加密密钥的哈希函数原项，破解出原项再hash一下就得到了密钥，这不是一样的吗？） 对称密钥的空间大小远远小于RSA的明文空间 PKCS1 ISO标准不是RSA在实际中的应用。实际使用是将对称密钥扩展然后用RSA加密 解决方法是，服务器解密后发现开头不是02，就认为明文只是个随机值而不是包含密钥的明文，继续协议就会发现密钥不一致从而结束会话（最常用的PKCS1） RSA的安全性 如果已经知道N的因式分解，那么可以用中国剩余定理求解x 计算e次根一定要因式分解吗？如果没有其它方法就说明了一个reduction（规约）： 任何有效的计算模N的e次根的算法都是有效的因式分解算法 实际应用中的RSA 最小的公钥e是3，可以但推荐还是65537。 RSA-CRT（带中国剩余定理的RSA）。RSA的加密很快但是解密很慢 RSA数学上是正确的，但是如果没有较好实现，会出现各种旁道攻击 防火墙刚启动时种子数量少导致伪随机数生成器重复生成p，导致网络上许多设备的p相同 ElGamal 前一节讲了基于陷门置换函数的公钥加密系统，这节讲基于Diffle-Hellman协议的公钥加密系统 ElGamal如果不做预计算，加密会比解密慢，但是因为g是固定的，意味着加密可以做预计算，当内存足够时加密是比解密快的。但是内存不够，不能预计算时，RSA更快，因为只做一次指数运算 ElGamal安全性 这个计算Diffle-Hellman假设对于分析ElGamal系统的安全性并不理想 我们引入更强的哈希Diffle-Hellman假设，更强假设的意思是，攻击者的能力更强，但是我们提出的某个论断仍然成立 语义安全是不够的，我们真正想要的是选择密文安全。 为了证明选择密文安全，我们引入一个更强的假设叫做交互Diffle-Hellman假设 交互Diffle-Hellman假设是CCA安全的，现在问题是在CDH假设上能否实现CCA安全，没有random oracle能否实现CCA安全 有更好安全性分析的ElGamal变种 我们想在CDH假设上实现CCA安全，有两种办法，第一种是使用双线性群，这种群CDH和IDH是等价的；第二种是修改ElGamal系统 第二种方法有一个ElGamal的变种满足CDH假设上的CCA安全 如果没有random oracle，CCA安全还成立吗？ ElGamal和RSA两种公钥系统共同遵循的原理 这里没有形式化给出单向函数的定义，因为要证明单向函数是否存在，也就是要证明P不等于NP（若P=NP，则公钥密码学将有根基危机） RSA有乘法性质和陷门，陷门意味着有私钥就可以逆向计算 总结：公钥加密依赖具有同态性质的单向函数和陷门 课程总结","categories":[{"name":"密码学","slug":"密码学","permalink":"https://zunpan.github.io/categories/%E5%AF%86%E7%A0%81%E5%AD%A6/"}],"tags":[{"name":"密码学","slug":"密码学","permalink":"https://zunpan.github.io/tags/%E5%AF%86%E7%A0%81%E5%AD%A6/"},{"name":"Cryptography","slug":"Cryptography","permalink":"https://zunpan.github.io/tags/Cryptography/"},{"name":"Dan Boneh","slug":"Dan-Boneh","permalink":"https://zunpan.github.io/tags/Dan-Boneh/"}]},{"title":"2022.05.23 某区块链公司面试记录","slug":"2022-5-23-某区块链公司面试记录","date":"2022-05-23T11:47:58.000Z","updated":"2023-09-24T04:27:40.271Z","comments":true,"path":"2022/05/23/2022-5-23-某区块链公司面试记录/","link":"","permalink":"https://zunpan.github.io/2022/05/23/2022-5-23-%E6%9F%90%E5%8C%BA%E5%9D%97%E9%93%BE%E5%85%AC%E5%8F%B8%E9%9D%A2%E8%AF%95%E8%AE%B0%E5%BD%95/","excerpt":"","text":"投的后端开发实习生岗，一天面了两次技术面 一面 针对简历项目提问，为什么要paillier？为什么要秘密分享？ mysql的主从备份，主从节点的事务需不需要分开执行？从节点事务执行失败，主节点如何回滚？binlog的同步和事务在主从节点的执行，两者按时间顺序如何排列？ 手写代码：求字典序第k大的数 二面 区块链共识算法有哪些？每个算法容错数量是多少？双花问题？默克尔树？默克尔证明？ Java基本数据类型和引用类型的区别 为什么每个基本数据类型都有包装类，包装类有什么用？ 针对简历提问，秘密分享时间复杂度多少？paillier用在哪里？ 手写代码：多线程卖票，要求每个线程同时工作，不能超卖 两个大文件，文件内容是字符串集合，内存略大于一个文件，如何对两个文件进行字符串去重？如何优化时间复杂度？","categories":[{"name":"杂项","slug":"杂项","permalink":"https://zunpan.github.io/categories/%E6%9D%82%E9%A1%B9/"}],"tags":[{"name":"面试","slug":"面试","permalink":"https://zunpan.github.io/tags/%E9%9D%A2%E8%AF%95/"}]},{"title":"基于区块链的安全电子选举系统","slug":"基于区块链的安全电子选举系统","date":"2022-05-13T13:10:09.000Z","updated":"2023-09-24T04:27:40.282Z","comments":true,"path":"2022/05/13/基于区块链的安全电子选举系统/","link":"","permalink":"https://zunpan.github.io/2022/05/13/%E5%9F%BA%E4%BA%8E%E5%8C%BA%E5%9D%97%E9%93%BE%E7%9A%84%E5%AE%89%E5%85%A8%E7%94%B5%E5%AD%90%E9%80%89%E4%B8%BE%E7%B3%BB%E7%BB%9F/","excerpt":"","text":"背景：浙江大学软件学院实训课题，本组选择的课题是关于区块链和隐私计算的融合应用场景，具体方向是电子选举 需求：投票人不暴露身份隐私的情况下完成投票，计票机构在选票合法的情况下完成计票 技术方案总结 项目总体情况 核心功能性能测试 测试环境：x86_64，Intel® Xeon® Gold 6133 CPU @ 2.50GHz，Linux 5.4.0-96-generic 测试结果：大规模选举时延较高，适用于小规模选举活动，但因为选举本身的非实时性，也可用于更大规模 亮点 在隐匿链上选票来自谁的情况下选民只需要通过零知识证明证明自己的合法选民身份。 即使是对一个合法证明的微小改变，都无法通过链上以及链下的零知识证明验证，攻击者难以冒充合法选民。 缺陷 监管机构（后端）无法证明没有存储选民的隐私信息，例如该选民的选票号，私有盐。这里存在漏洞可以使得作恶的监管机构只需执行两次解密算法就可以获得选民的选票内容 零知识证明以匿名的形式证明了选民身份，选民通过秘密ID和监管方产生的PK生成证明并公布。秘密输入应当是选民秘密持有的，因此生成证明的过程最好是在本地做。 当前选民需要通过可信的后端API生成证明，无法便捷地通过WASM在本地生成(原计划下libsnark难以迁移，且曲线计算迁移到wasm慢几十倍)，因此使用起来较为不便。 系统展示","categories":[{"name":"区块链","slug":"区块链","permalink":"https://zunpan.github.io/categories/%E5%8C%BA%E5%9D%97%E9%93%BE/"}],"tags":[{"name":"密码学","slug":"密码学","permalink":"https://zunpan.github.io/tags/%E5%AF%86%E7%A0%81%E5%AD%A6/"},{"name":"区块链","slug":"区块链","permalink":"https://zunpan.github.io/tags/%E5%8C%BA%E5%9D%97%E9%93%BE/"},{"name":"电子选举","slug":"电子选举","permalink":"https://zunpan.github.io/tags/%E7%94%B5%E5%AD%90%E9%80%89%E4%B8%BE/"},{"name":"隐私计算","slug":"隐私计算","permalink":"https://zunpan.github.io/tags/%E9%9A%90%E7%A7%81%E8%AE%A1%E7%AE%97/"}]},{"title":"MIT-Missing-Semester学习笔记","slug":"MIT-Missing-Semester学习笔记","date":"2022-05-07T11:46:29.000Z","updated":"2023-09-24T04:27:40.278Z","comments":true,"path":"2022/05/07/MIT-Missing-Semester学习笔记/","link":"","permalink":"https://zunpan.github.io/2022/05/07/MIT-Missing-Semester%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","excerpt":"","text":"Course overview + the shell Shell Tools and Scripting TLDR TLDR可以代替man命令，查找命令说明书，TLDR可以给出详细的例子 find ripgrep 可以代替grep查找文件内容符合条件的文件 Ctrl+R 可以进入用子串查找历史命令的模式 xargs 命令，它可以使用标准输入中的内容作为参数。 例如 ls | xargs rm 会删除当前目录中的所有文件。 Vim 略 Data Wrangling 数据处理常用工具有grep 另外还有一些常用的数据处理工具 sed，文件（输入流）处理工具 sort，排序工具 awk，作用于文件的多功能编程语言 Command-line Enviroment Job Control（作业控制） 结束一个正在执行的程序可以按ctrl+c，实质是终端向程序发送了一个SIGINT的signal SIGINT可以在程序中捕获处理，因此ctrl+c也就是SIGINT信号有时候没用。 SIGQUIT信号可以结束程序并不会被捕获，按ctrl+\\ SIGSTOP信号可以暂停程序到后台，按ctrl+z。命令行输入jobs查看所有程序运行状态，使用fg %程序id 或 bg %程序id 将程序从暂停状态转到前端执行或后端执行 尽管程序可以在后端执行，但是一旦终端关闭，作为子进程的程序会收到一个SIGHUP信号导致被挂起，可以使用nohup 程序 &amp;来使得程序在后端执行并忽略SIGHUP信号 Terminal Multiplexers（终端复用器） Aliases（别名） 直接在命令行定义别名，关闭终端后就没了，需要在.bashrc或者.zshrc这样的终端启动文件中做持久化 Dotfiles（点文件或者配置文件） 许多配置文件以’.‘开头，所以叫dotfile，默认情况下ls看不到dotfile 配置文件最好用版本控制统一管理，然后将原来的配置文件路径软链接到版本控制下的配置文件路径，github上有许多dotfile仓库 Remote Machines（远程机器） 使用ssh可以登录远程终端 foo是用户名，bar.mit.edu是域名，也可以直接是ip地址 也可以直接使用ssh执行命令，只要在上面的命令后面加上需要执行的命令即可 Version Control(Git) 略 Debugging and Profiling Debugging Printf debugging and Logging “The most effective debugging tool is still careful thought, coupled with judiciously placed print statements” — Brian Kernighan, Unix for Beginners. 在程序中使用logging而不用printf的好处在于 日志可以输出到文件、sockets、甚至是远程服务器而不一定是标准输出 日志支持几种输出级别（例如INFO，DEBUG，WARN，ERROR） 对于新出现的问题，日志有足够多的信息来排查 tips：输出可以按级别用不同颜色表示，例如ERROR用红色 echo -e &quot;\\e[38;2;255;0;0mThis is red\\e[0m&quot;会打印红色的“This is red”到终端上 Third Party logs 许多第三方程序会将日志写到系统的某一处，一般是/var/log. NGINX服务器会将日志放在/var/log/nginx下. 许多linux系统使用systemd, 一个系统守护进程来控制许多事情例如某些服务的开启和运行, systemd将日志放在/var/log/journal下, 可以使用journalctl查看 Debuggers 略 Specialized Tools Linux下可以使用strace查看程序系统调用情况 tcpdump和Wireshark是网络包分析器 web开发中，Chrome和Firefox的开发者工具十分方便 Profiling（分析） 学习分析和检测工具可以帮助理解程序中那一部分花费最多时间或资源以便优化这部分 Timing 三种不同的运行时间 真实时间：程序开始到结束的时间，包括阻塞时间、等待IO、网络 用户态时间：CPU执行程序中用户态代码的时间 系统态时间：CPU执行程序中核心态代码的时间 正确的程序执行时间=用户态时间+系统态时间 time命令可以测试程序的三种时间 Metaprogramming 这节主要讲系统构建工具make、持续集成等 略 Security and Cryptography Entropy（熵） 熵度量了不确定性并可以用于决定密码的强度 熵的单位是比特，对于一个均匀分布的离散随机变量，熵等于log2(所有可能的个数，即n)log_2(所有可能的个数，即n)log2​(所有可能的个数，即n) 扔一次硬币的熵是1比特。掷一次六面骰子的熵大约为2.58比特。一般我们认为攻击者了解密码的模型（最小长度，最大长度，可能包含的字符种类等），但是不了解某个密码是如何选择的 https://xkcd.com/936/ 例子里面，“correcthorsebatterystaple”这个密码比“Tr0ub4dor&amp;3”更安全，因为前者熵更大，大约40比特的熵足以对抗在线穷举攻击（受限于网络速度和应用认证机制）；而对于离线穷举攻击（主要受限于计算速度），一般需要更强的密码（比如80比特） Potpourri 略","categories":[{"name":"Linux","slug":"Linux","permalink":"https://zunpan.github.io/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://zunpan.github.io/tags/Linux/"},{"name":"Shell","slug":"Shell","permalink":"https://zunpan.github.io/tags/Shell/"}]},{"title":"Maven问题记录","slug":"Maven问题记录","date":"2022-05-07T11:01:06.000Z","updated":"2023-09-24T04:27:40.279Z","comments":true,"path":"2022/05/07/Maven问题记录/","link":"","permalink":"https://zunpan.github.io/2022/05/07/Maven%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/","excerpt":"","text":"多模块项目依赖错误 多模块项目直接对根模块进行install，Maven会根据依赖自动判断install顺序。install完成后需要点击Maven界面的Reload All Maven Projects 父模块的pom文件一定得写&lt;packaging&gt;pom&lt;/packaging&gt; 因为默认打包方式是jar，&lt;xs:element name=&quot;packaging&quot; minOccurs=&quot;0&quot; type=&quot;xs:string&quot; default=&quot;jar&quot;&gt; 有些公司内部仓库的包只有pom文件没有jar，这种是下载不下来的 只有jar包下载到了本地，maven才能找到依赖，所以这就是为什么多模块项目，被依赖的模块要先install到本地，这样依赖它的模块才能找到这个模块","categories":[{"name":"Maven","slug":"Maven","permalink":"https://zunpan.github.io/categories/Maven/"}],"tags":[{"name":"Maven","slug":"Maven","permalink":"https://zunpan.github.io/tags/Maven/"}]},{"title":"Maven学习笔记","slug":"Maven学习笔记","date":"2022-05-07T10:47:28.000Z","updated":"2023-09-24T04:27:40.279Z","comments":true,"path":"2022/05/07/Maven学习笔记/","link":"","permalink":"https://zunpan.github.io/2022/05/07/Maven%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","excerpt":"","text":"学习资料：Maven官方文档 最常用的两种打包方法 clean，package（如果报错，很可能就是jar依赖的问题） clean，install Maven生命周期 Maven有三种内置的构建生命周期：default，clean，site。default生命周期处理项目部署，clean生命周期处理项目清理，site生命周期处理项目web站点的建立。每个生命周期包含一系列阶段 default生命周期阶段（非完整） validate：验证，验证工程是否正确，所需的信息是否完整。 compile：编译源码，编译生成class文件,编译命令，只编译选定的目标，不管之前是否已经编译过，会在你的项目路径下生成一个target目录，在该目录中包含一个classes文件夹，里面全是生成的class文件及resources下的文件。 test：单元测试 package：打包，将工程文件打包为指定的格式，例如JAR，WAR等。这个命令会在你的项目路径下一个target目录，并且拥有compile命令的功能进行编译，同时会在target目录下生成项目的jar/war文件。如果a项目依赖于b项目，打包b项目时，只会打包到b项目下target下，编译a项目时就会报错，因为找不到所依赖的b项目，说明a项目在本地仓库是没有找到它所依赖的b项目，这时就用到install命令了 integration-test：将jar包部署到环境中进行单元测试 verify：核实，检查package是否有效、符合质量标准。 install：将包安装至本地仓库，以作为其它项目的dependency。该命令包含了package命令功能，不但会在项目路径下生成class文件和jar包，同时会在你的本地maven仓库生成jar文件，供其他项目使用（如果没有设置过maven本地仓库，一般在用户/.m2目录下。如果a项目依赖于b项目，那么install b项目时，会在本地仓库同时生成pom文件和jar文件，解决了上面打包package出错的问题）。 deploy：复制到远程仓库。 clean生命周期阶段（非完整） clean：清理，在进行真正的构建之前进行一些清理工作，移除所有上一次构建生成的文件。执行该命令会删除项目路径下的target文件，但是不会删除本地的maven仓库已经安装的jar文件。 site生命周期阶段（非完整） site：站点，生成项目的站点文档 Phases are actually mapped to underlying goals. The specific goals executed per phase is dependant upon the packaging type of the project. For example, package executes jar:jar if the project type is a JAR, and war:war if the project type is - you guessed it - a WAR. An interesting thing to note is that phases and goals may be executed in sequence. mvn clean dependency:copy-dependencies package 这条命令执行了mvn的clean阶段，dependency插件的copy-dependencies目标，package阶段（package处于default生命周期，Maven会先顺序执行validate、compile、test） 阶段由插件目标构成 一个插件目标代表一个具体的任务（比阶段更细），它可以被绑定到0个或多个阶段。没有绑定到阶段也能直接执行，目标和阶段执行顺序取决于调用的顺序 For example, consider the command below. The clean and package arguments are build phases, while the dependency:copy-dependencies is a goal (of a plugin). mvn clean dependency:copy-dependencies package 如果一个目标被绑定到一个或多个阶段，目标将在阶段中被调用。如果阶段没有绑定任何目标，那阶段就不会执行。如果阶段绑定一个或多个目标，执行这个阶段会执行所有目标，目标执行的顺序在POM中定义 build和compile的区别 点击Build Project，idea 使用自己的构建工具进行编译（编译器默认使用Javac），相当于maven的compile，点击Build Artifacts，相当于maven的Package 插件和goal a plugin is a collection of goals with a general common purpose. For example the jboss-maven-plugin, whose purpose is “deal with various jboss items”. 总结 mvn clean package 依次执行了clean、resources、compile、testResources、testCompile、test、jar(打包)等７个阶段。 mvn clean install 依次执行了clean、resources、compile、testResources、testCompile、test、jar(打包)、install等8个阶段。 mvn clean deploy 依次执行了clean、resources、compile、testResources、testCompile、test、jar(打包)、install、deploy等９个阶段。 注意：maven默认的生命周期中，不管是package还是install，都不会把第三方依赖打包到一个jar包中，需要自己配置插件，比如maven-assembly-plugin。若是springboot项目，有spring-boot-maven-plugin插件，执行mvn package后，先走一遍maven自身生命周期到package，然后springboot打包插件里面提供了一个repackage的goal，这样mvn package在插件的指导下先完成标准的打包流程再完成插件定义的流程","categories":[{"name":"Maven","slug":"Maven","permalink":"https://zunpan.github.io/categories/Maven/"}],"tags":[{"name":"Maven","slug":"Maven","permalink":"https://zunpan.github.io/tags/Maven/"}]},{"title":"LeetCode刷题记录(cpp)","slug":"LeetCode刷题记录-cpp","date":"2022-05-07T09:00:09.000Z","updated":"2023-09-24T04:27:40.278Z","comments":true,"path":"2022/05/07/LeetCode刷题记录-cpp/","link":"","permalink":"https://zunpan.github.io/2022/05/07/LeetCode%E5%88%B7%E9%A2%98%E8%AE%B0%E5%BD%95-cpp/","excerpt":"","text":"stringstream用法 需要include&lt;sstream&gt;，任何输入输出都会被转换成字符串 istringstream类用于执行C风格的串流的输入操作 ostringstream类用于执行C风格的串流的输出操作 stringstream类同时支持C风格的串流的输入输出操作 #include &lt;iostream&gt; #include &lt;vector&gt; #include &lt;string&gt; #include &lt;sstream&gt; using namespace std; // 例子1: 基本用法 int main() &#123; string s; stringstream ss; int n = 11, m = 88; // 将11放入流中 ss &lt;&lt; n; // 从流中提取数据到s中，自动类型转换，无需关心类型问题 ss &gt;&gt; s; // 11 cout &lt;&lt; s &lt;&lt; endl; s += &quot;23&quot;; // 1123 cout &lt;&lt; s &lt;&lt; endl; // 清空，以便下一次使用 ss.clear(); ss.str(&quot;&quot;); ss &lt;&lt; s; ss &gt;&gt; n; // 1123 cout &lt;&lt; n &lt;&lt; endl; return 0; &#125; // 例2：按空格分隔字符串 int main(int argc, char const *argv[]) &#123; stringstream ss(&quot;1 2 3&quot;); string s; while (ss &gt;&gt; s) &#123; cout &lt;&lt; s &lt;&lt; endl; &#125; return 0; &#125; // 例3：按分隔符分隔字符串 const vector&lt;string&gt; split(const string &amp;str, const char &amp;delimiter) &#123; vector&lt;string&gt; result; stringstream ss(str); // 等于ss.str(str); string tok; while (getline(ss, tok, delimiter)) // 从ss流中按delimiter为分隔符读取数据存储到tok中，直到流结束 &#123; result.push_back(tok); &#125; return result; &#125; 默认值 string类型的默认值是&quot;&quot; int类型的默认值是0 substr函数用法 string s(&quot;123&quot;); string a = s.substr(0,5);// 获得字符串s中从第0位开始的长度为5的字符串，若 pos+n&gt;s.length()，只会截取到末尾 string.length()或者size()与变量作比较的bug string.length()是unsigned型的不能与负数作比较，否则在机器数的表示上俩者都会以unsigned的编码形式比较，那就大概率结果就不对了,所以建议平时编程养成好习惯，类型不一样的数据比较或运算时一定要留一个心眼，不能直接拿来就比。用的时候可以在前面写上强转 链表题技巧 多用定义变量，不容易被逻辑绕晕，比如修改指针前先用first、second、third先保存下来 head有可能发生改动时，先增加一个假head，返回的时候返回head-&gt;next，避免修改头结点导致多出一堆逻辑 二叉树题技巧 先考虑递归（前中后序遍历）还是迭代（层序遍历） 递归要先考虑函数是否有返回值，比如说判断是否是平衡二叉树，当前递归函数中要根据左右子树的返回值做判断，所以递归函数要返回值 然后考虑是前，还是中，还是后。如果要先处理左右子树那就是后，特别地，回溯也是后，参考https://programmercarl.com/0236.二叉树的最近公共祖先.html 最后再考虑递归终止条件，一般是root==nullptr就返回，也有特殊情况，比如叶子节点就返回 层序遍历一般用队列。如果要一层一层的访问，在出队的时候要先记录当前层的节点数，根据节点数出队 数字和字符串互转 数字转字符串：to_string(number) 字符串转数字：stoi(intStr) , stol(longStr), stof(floatStr), stod(doubleStr) 数组初始化 初始化一个n*n的二维数组，用0填充 vector&lt;vector&lt;int&gt;&gt;v(n,vector&lt;int&gt;(n,0)); 或者 v.resize(n); for(int k=0;k&lt;n;k++)&#123; v[k].resize(n); &#125; 或者用memset int v[n][n]; memset(v,0,sizeof(int)*n*n); DFS和BFS适用条件 dfs适合找解的存在和所有解，bfs适合找最优解。例如dfs可以解决可达性，bfs可以解决无权图最短路径（有权使用dijkstra）。dfs需要回溯时要注意，以N皇后和解数独为例，如果当前位置的合法性依赖之前放置的位置，那么回溯时要将位置清除（N皇后），如果当前位置的合法性不依赖之前的位置，而是依赖别的辅助空间，那么回溯时只需要清除辅助空间，放置的位置可以不清除（解数独） 图的连通性 考虑dfs和并查集 自定义排序 // sort函数第三个参数是lambda表达式，[x] 表示捕获外部的变量x sort(arr.begin(),arr.end(),[x](const int&amp; a,const int&amp; b)&#123; return abs(a-x)==abs(b-x)?a&lt;b:abs(a-x)&lt;abs(b-x); &#125;); 二分技巧 标准二分查找 int search(vector&lt;int&gt;&amp; nums, int target) &#123; int left = 0; int right = nums.size() - 1; while (left &lt;= right) &#123; int mid = left + ((right - left) &gt;&gt; 1); if (nums[mid] == target) return mid; else if (nums[mid] &gt; target) &#123; right = mid - 1; &#125; else &#123; left = mid + 1; &#125; &#125; return -1; &#125; 二分查找第一个满足XXX的位置 int left = 0, right = nums.size()- 1, ans = -1; // &lt;= 可以判断区间长度为1时，即left==right时是否有解；若二分条件必须在区间大于等于2的情况下判断，则去掉=，参考leetcode162 while (left &lt;= right) &#123; int middle = left + (right - left) / 2; if (满足XXX) &#123; ans = middle; right = middle - 1; &#125; else &#123; left = middle + 1; &#125; &#125; return ans; 二分查找最后一个满足XXX的位置 int left = 0, right = nums.size()- 1, ans = -1; // &lt;= 可以判断区间长度为1时，即left==right时是否有解；若二分条件必须在区间大于等于2的情况下判断，则去掉=，参考leetcode162 while (left &lt;= right) &#123; int middle = left + (right - left) / 2; if (满足XXX) &#123; ans = middle; left = middle + 1; &#125; else &#123; right = middle - 1; &#125; &#125; return ans; STL的二分查找 lower_bound(begin,end,num): 从数组的begin位置到end-1位置二分查找第一个大于或等于num的数字，找到返回该数字的迭代器，不存在则返回end。通过返回的迭代器减去begin可以得到数字的下标。 upper_bound(begin,end,num): 从数组的begin位置到end-1位置二分查找第一个大于num 数字，找到返回该数字的迭代器，不存在则返回end。通过返回的迭代器减去begin可以得到数字的下标。 // 用在vector上 vector&lt;int&gt; nums1&#123;1,10,4,4,2,7&#125;; sort(nums1.begin(),nums1.end()); int pos1 = lower_bound(nums1.begin(),nums1.end(),9)-nums1.begin(); // 用在数组上 int nums2[6] = &#123;1, 10, 4, 4, 2, 7&#125;; sort(nums2,nums2+6); int pos2 = lower_bound(nums2,nums2+6,9)-nums2; // 返回vector中最靠近5的第一个小于或等于5的数 vector&lt;int&gt; nums1&#123;1,10,4,4,2,7&#125;; sort(nums1.rbegin(),nums1.rend()); int pos3 = lower_bound(nums1.begin(), nums1.end(), 5,greater&lt;int&gt;())-nums1.begin(); 优先队列 // 升序序列 priority_queue&lt;int, vector&lt;int&gt;, greater&lt;int&gt;&gt;q; // 降序序列(默认，等同于priority_queue&lt;int&gt;) priority_queue&lt;int, vector&lt;int&gt;, less&lt;int&gt;&gt;q; // 自定义类型排序 struct student &#123; student(int age)&#123; this-&gt;age = age; &#125; int age; &#125;; struct cmp&#123; bool operator() (student&amp; s1,student&amp; s2)&#123; return s1.age&lt;s2.age; &#125; &#125;; int main(int argc, char const *argv[]) &#123; vector&lt;int&gt; ages&#123;3,4,1,2&#125;; priority_queue&lt;student,vector&lt;student&gt;,cmp&gt;a; for(int age:ages)&#123; student s(age); q.push(s); &#125; while(q.size()&gt;0)&#123; student head = q.top();q.pop(); cout&lt;&lt;head.age&lt;&lt;endl; &#125; &#125; 位运算符技巧 按位与运算符（&amp;） 参加运算的两个数据，按二进制位进行“与”运算 运算规则：0&amp;0=0；0&amp;1=0；1&amp;0=0；1&amp;1=1； 即：两位同时为1，结果才为1，否则为0 例如：3&amp;5 即 0000 0011 &amp; 0000 0101 = 0000 0001 因此，3&amp;5=1 ”与运算“的特殊用途 清零。如果想将一个单元清零，即全部二进制位归0，只要与一个各位都为0的数值相与，结果为0 取一个数的指定二进制位。如果想取某个数的后四位，那么可以计算 该数&amp;0000 1111 按位或运算符（|） 参加运算的两个数据，按二进制位进行“或”运算 运算规则：0|0=0；0|1=1；1|0=1；1|1=1； 即：参加运算的两个数据只要有一个位1，其值为1 例如：3|5 即 0000 0011 | 0000 0101 = 0000 0111， 因此，3|5的值为7 “或运算”特殊用途 常用来对一个数的某些位置置为1。如果想对某个数的后四位置为1，那么可以计算 该数 | 0000 1111 异或运算符（^） 参加运算的两个数据，按二进制位进行“异或”运算 运算规则：0 ^ 0=0；0 ^ 1=1； 1 ^ 0=1； 1 ^ 1=0； 即：参加运算的两个数据，如果两个相应位值不同，则该位位1，否则为0（模2和） “异或运算”特殊用途 翻转一个数的指定位。如果将对某个数的后四位翻转， 那么可以计算 该数 ^ 0000 1111 与0异或，保留原值 取反运算符（~） 参加运算的一个数据，按二进制位进行“取反”运算 运算规则：~1 = 0；~0 = 1； 即：对一个二进制数按位取反。 “取反运算”特殊用途 使一个数的最低位为0，可以计算 该数 &amp; ~1 左移运算符（&lt;&lt;） 将一个数据的各二进制位全部左移若干位（左边的二进制位丢弃，右边补0） 例如：a = a &lt;&lt; 2 将a的二进制位左移2位，右补0。若左移时舍弃的高位不包含1，则每左移一位，相当于该数乘以2 右移运算符（&gt;&gt;) 将一个数的各二进制位全部右移若干位，正数左补0，负数左补1，右边丢弃。 例如： a = a &gt;&gt; 2 将a 二进制位右移2位，操作数每右移一位，相当于该数除以2 &gt;&gt; 运算符把 expression1 的所有位向右移 expression2 指定的位数。expression1 的符号位被用来填充右移后左边空出来的位。向右移出的位被丢弃。 例如-14（1111 0010）右移两位等于-4（11111100） 无符号右移运算符（&gt;&gt;&gt;) &gt;&gt;&gt; 运算符把 expression1 的各个位向右移 expression2 指定的位数。右移后左边空出的位用零来填充。移出右边的位被丢弃。 例如 -14 （11111111 11111111 11111111 11110010），向右移两位后等于1073741820（00111111 11111111 11111111 11111100） 不同长度的数据进行位运算 如果两个不同长度的数据进行位运算时，系统会将二者按右端对齐，然后进行位运算 以“与”运算为例说明如下：我们知道在C语言中long型占4个字节，int型占2个字节，如果一个long型数据与一个int型数据进行“与”运算，右端对齐后，左边不足的位依下面三种情况补足 如果整形数据为正数，左边补16个0 如果整形数据为负数，左边补16个1 如果整形数据为无符号数，左边也补16个0 数组题技巧 考虑双指针，双向前缀和，单调队列 快排 void myquickSort(vector&lt;int&gt;&amp; nums,int low,int high)&#123; // mid不能是下标 int mid = nums[(low+high)/2]; int i = low,j = high; do&#123; while(nums[i]&lt;mid) i++; while(nums[j]&gt;mid) j--; if(i&lt;=j) swap(nums[i++],nums[j--]); &#125;while(i&lt;=j); if(low&lt;j) myquickSort(nums,low,j); if(i&lt;high) myquickSort(nums,i,high); &#125; 背包问题 常见的背包问题有三种 组合排列问题 组合总和 Ⅳ 目标和 零钱兑换 II True、False问题 单词拆分 分割等和子集 最大最小问题 一盒零 零钱兑换 组合排列问题公式 dp[i] += dp[i-num] True、False问题公式 dp[i] = dp[i] or dp[i-num] 最大最小问题公式 dp[i] = min(dp[i],dp[i-num]+1)或者dp[i] = max(dp[i],dp[i-num]+1) 以上三组公式是解决对应问题的核心公式 背包问题分析步骤 分析是否为背包问题 是以上三种背包问题中的哪一种 是0-1背包问题还是完全背包问题。也就是题目给的nums数组中的元素是否可以重复使用 如果是组合排列问题，组合问题外循环物品，内循环背包；排列相反 背包问题特征 背包问题具有的特征：给定一个target，target可以是数字也可以是字符串，再给定一个数组nums，nums中装的可能是数组，也可能是字符串，问：能否使用nums中的元素做各种排列组合得到target 背包问题遍历 如果是0-1背包，即数组中的元素不可重复使用，nums放在外循环，target在内循环，且内循环倒序 for(int i=0;i&lt;nums.size();i++)&#123; for(int j=target;j&gt;=nums[i];j--)&#123; // dp公式 &#125; &#125; 如果是完全背包，即数组中的元素可重复使用，nums放在外循环，target在内循环，且内循环正序 for(int i=0;i&lt;nums.size();i++)&#123; for(int j=nums[i];j&lt;=target;j++)&#123; // dp公式 &#125; &#125; 如果是排列问题，即需要考虑元素之间的顺序，target放在外循环，nums放在内循环 for(int i=0;i&lt;=target;i++)&#123; for(int j=0;j&lt;nums.size();j++)&#123; if(i&gt;nums[j])&#123; // dp公式 &#125; &#125; &#125; 单调栈 通常是一维数组，要寻找任一元素的右边或者左边第一个比自己大或者小的元素的位置，此时我们就要想到可以用单调栈了","categories":[{"name":"算法","slug":"算法","permalink":"https://zunpan.github.io/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"LeetCode","slug":"LeetCode","permalink":"https://zunpan.github.io/tags/LeetCode/"},{"name":"cpp","slug":"cpp","permalink":"https://zunpan.github.io/tags/cpp/"},{"name":"algorithm","slug":"algorithm","permalink":"https://zunpan.github.io/tags/algorithm/"}]},{"title":"Git学习笔记","slug":"Git学习笔记","date":"2022-05-06T13:47:10.000Z","updated":"2023-09-24T04:27:40.277Z","comments":true,"path":"2022/05/06/Git学习笔记/","link":"","permalink":"https://zunpan.github.io/2022/05/06/Git%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","excerpt":"","text":"新建git项目 在一个目录下执行 git init 克隆git项目 在一个目录下执行 git clone [ssh/https]address 管理修改 将文件修改添加到暂存区 git add filename 将暂存区的修改提交到本地仓库 git commit -m &#x27;commitMessage&#x27; [filename] 每次修改完要先add到暂存区，然后commit才能把你修改的东西提交的本地仓库 提交信息规范 commit message格式 &lt;type&gt;(&lt;scope&gt;): &lt;subject&gt; type(必须) 用于说明git commit的类别，只允许使用下面的标识。 feat：新功能（feature）。 fix/to：修复bug，可以是QA发现的BUG，也可以是研发自己发现的BUG。 fix：产生diff并自动修复此问题。适合于一次提交直接修复问题 to：只产生diff不自动修复此问题。适合于多次提交。最终修复问题提交时使用fix docs：文档（documentation）。 style：格式（不影响代码运行的变动）。 refactor：重构（即不是新增功能，也不是修改bug的代码变动）。 perf：优化相关，比如提升性能、体验。 test：增加测试。 chore：构建过程或辅助工具的变动。 revert：回滚到上一个版本。 merge：代码合并。 sync：同步主线或分支的Bug。 scope(可选) scope用于说明 commit 影响的范围，比如数据层、控制层、视图层等等，视项目不同而不同。 例如在Angular，可以是location，browser，compile，compile，rootScope， ngHref，ngClick，ngView等。如果你的修改影响了不止一个scope，你可以使用*代替。 subject(必须) subject是commit目的的简短描述，不超过50个字符。 建议使用中文（感觉中国人用中文描述问题能更清楚一些）。 结尾不加句号或其他标点符号。 根据以上规范git commit message将是如下的格式： fix(DAO):用户查询缺少username属性 feat(Controller):用户查询接口开发 查看工作区和暂存区的不同 git diff [filename] 查看暂存区和版本库的不同 git diff --cached [filename] 删除修改 在文件已经加入版本库的情况下 如果文件修改后没有add进暂存区，我们可以用 git checkout -- filename 取消工作区的修改，恢复到和版本库一摸一样的状态 如果文件修改后已经add进暂存区了，我们可以用 git reset HEAD filename 取消暂存区的修改，恢复到工作区有修改的状态，这是查看git status 会看到 要撤销工作区的修改就用上面的checkout 如果文件修改后已经commit到了本地仓库，那么我们只能使用版本回退了 git reset --hard [commitID] commitID可以通过git log 或者 git reflog查看。git log 查看当前版本的所有commit，git reflog查看所有commit，所以回退到了老版本后想回到新版本后得先git reflog 查看新版本的commitID 在文件没有加入版本库的时候 这种情况一般是新生成的文件，工作区可以任意编辑，因为版本库中没有，所以不需要checkout（checkout在没有add的时候恢复至版本库状态） 如果add进了暂存区，此时和上面一样，可以使用 git resset HEAD filename 恢复至工作区有修改的状态 删除文件 如果文件已经处于tracked状态（已经被add了）,当你要删除文件的时候，可以采用命令： rm filename 这个时候（也就是说这个时候只执行了rm test.txt）有两种情况 第一种情况:的确要把test.txt删掉，那么可以执行 git rm test.txt 或者 git add test.txt git commit -m &quot;remove test.txt&quot; 然后文件就被删掉了，这种情况想恢复只能用版本回退了，但是之前的版本也没有提交这个文件，那这个就永远消失了 第二种情况:删错文件了，不应该删test.txt，注意这时只执行了rm test.txt，还没有提交，所以可以执行 git checkout test.txt 将文件恢复。 并不是说执行完git commit -m &quot;remove test.txt&quot;后还能用checkout恢复，commit之后版本库里的文件也没了，自然没办法用checkout恢复，而是要用其他的办法（版本回退） 远程仓库 检测是否能ssh登录远程仓库 ssh -T git@github.com 添加远程仓库 git remote add origin git@github.com:username/reponame.git 查看远程仓库 git remote 抓取/合并远程仓库 git fetch origin git merge origin/master 推送到远程仓库 git push -u origin master -u 表示把本地的master分支和远程仓库origin的master分支关联 后面就可以用 git push origin master 删除远程仓库关联 git remote rm origin 分支管理 新建并切换分支 git checkout -b dev 和下面作用一样 git branch dev git checkout dev 查看分支 *表示当前所在分支 分支操作 和上面的文件修改一个意思，只不过上面默认是在master分支操作 合并 在dev分支下执行文件修改，add、commit操作不会影响到master分支，dev开发完成之后可以切换到master分支，执行合并操作 git merge dev 删除分支 git branch -d dev 注意：在任何一条分支上做的修改必须要commit到本地仓库在能算作在dev分支上的一个版本，否则切换到别的分支，别的分支仍然能看到修改没有被添加到暂存区或者没有提交到本地仓库，别的分支可以继续处理这个修改，有一个办法是git stash保留当前分支上的工作内容，这样切到别的分支上git status就没有要处理的修改了 解决冲突 Git用&lt;&lt;&lt;&lt;&lt;&lt;&lt;，=======，&gt;&gt;&gt;&gt;&gt;&gt;&gt;标记出不同分支的内容 &lt;&lt;&lt;&lt;和=====之间是HEAD（当前分支）的内容，=====和&gt;&gt;&gt;&gt;&gt;之间是feature1分支的内容 下图是手动修改后的冲突文件 查看分支合并情况 git log --graph bug分支 保留现场 git stash 然后切到有bug的分支上，新建一个分支，修复好后merge到出现bug的分支上，然后查看保存的工作现场 git stash list 恢复现场有两种办法： git stash pop pop会同时恢复现场和删除保存的工作现场 也可以指定恢复现场 git stash apply stash@&#123;0&#125; 然后删除分支 git stash drop stash@&#123;0&#125; 复制提交 比如上面bug分支提交了一个修改之后merge到了master分支，我们正在开发的dev分支也有这个bug，我们可以把bug分支提交的修改复制到dev分支 git cherry-pick commitID commitID通过git log查看 推送分支 git push origin branchName 多人协作 查看远程库信息，使用git remote -v； 本地新建的分支如果不推送到远程，对其他人就是不可见的； 从本地推送分支，使用git push origin branch-name，如果推送失败，先用git pull抓取远程的新提交； 在本地创建和远程分支对应的分支，使用git checkout -b branch-name origin/branch-name，本地和远程分支的名称最好一致； 建立本地分支和远程分支的关联，使用git branch --set-upstream branch-name origin/branch-name； 从远程抓取分支，使用git pull，如果有冲突，要先处理冲突。 代理 设置代理 git config --global http.proxy socks5://127.0.0.1:7890 git config --global https.proxy socks5://127.0.0.1:7890 后面的端口是你的梯子监听的端口（clash默认7890） 如果是在wsl2中使用，请看WSL2.md中的代理配置 https代理存在一个局限，那就是没有办法做身份验证，每次拉取私库或者推送代码时，都需要输入github的账号和密码，非常痛苦。 设置ssh代理前，请确保你已经设置ssh key。可以参考在 github 上添加 SSH key 完成设置更进一步是设置ssh代理。只需要配置一个config就可以了。 # Linux、MacOS vi ~/.ssh/config # Windows 到C:\\Users\\your_user_name\\.ssh目录下，新建一个config文件（无后缀名） 将下面内容加到config文件中即可 对于windows用户，代理会用到connect.exe，你如果安装了Git都会自带connect.exe，如我的路径为C:\\APP\\Git\\mingw64\\bin\\connect #Windows用户，注意替换你的端口号和connect.exe的路径 ProxyCommand &quot;C:\\APP\\Git\\mingw64\\bin\\connect&quot; -S 127.0.0.1:51837 -a none %h %p #MacOS用户用下方这条命令，注意替换你的端口号 #ProxyCommand nc -v -x 127.0.0.1:51837 %h %p Host github.com User git Port 22 Hostname github.com # 注意修改路径为你的路径 IdentityFile &quot;C:\\Users\\Your_User_Name\\.ssh\\id_rsa&quot; TCPKeepAlive yes Host ssh.github.com User git Port 443 Hostname ssh.github.com # 注意修改路径为你的路径 IdentityFile &quot;C:\\Users\\Your_User_Name\\.ssh\\id_rsa&quot; TCPKeepAlive yes 保存后文件后测试方法如下，返回successful之类的就成功了 # 测试是否设置成功 ssh -T git@github.com 取消代理 git config --global --unset http.proxy git config --global --unset https.proxy github搜索命令 仓库搜索 in:name example 名字中有“example” in:readme example readme中有“example” in:description example 描述中有“example” stars:&gt;1000 star&gt;1000 forks:&gt;1000 fork&gt;1000 pushed:&gt;2019-09-01 2019年9月1日后有更新的 language:java 用Java编写的项目 https://docs.github.com/en/search-github/searching-on-github/searching-for-repositories issue搜索 java in:title,body,comments 搜索标题中内容中评论中包含java的issue https://docs.github.com/en/search-github/searching-on-github/searching-issues-and-pull-requests 零碎 已经提交的文件取消追踪 先修改.gitignore git rm -r --cached . git add . git commit -m 'update .gitignore'","categories":[{"name":"Git","slug":"Git","permalink":"https://zunpan.github.io/categories/Git/"}],"tags":[{"name":"Git","slug":"Git","permalink":"https://zunpan.github.io/tags/Git/"}]},{"title":"truffle部署合约到远程私有链","slug":"truffle部署合约到远程私有链","date":"2020-02-10T13:04:14.000Z","updated":"2023-09-24T04:27:40.281Z","comments":true,"path":"2020/02/10/truffle部署合约到远程私有链/","link":"","permalink":"https://zunpan.github.io/2020/02/10/truffle%E9%83%A8%E7%BD%B2%E5%90%88%E7%BA%A6%E5%88%B0%E8%BF%9C%E7%A8%8B%E7%A7%81%E6%9C%89%E9%93%BE/","excerpt":"","text":"首先安装了truffle和truffle-hdwallet-provider npm install truffle -g npm install truffle-hdwallet-provider 安装truffle卡住的同学请在truffle后面加上@5.0.1指定版本 修改truffle配置文件 var HDWalletProvider = require(&quot;truffle-hdwallet-provider&quot;); // 导入模块 var mnemonic = &quot;你的助记词&quot;; //MetaMask的助记词。 module.exports = &#123; networks: &#123; development: &#123; network_id: &quot;*&quot;, // Any network (default: none) provider: function () &#123; // mnemonic表示MetaMask的助记词。 &quot;ropsten.infura.io/v3/33...&quot;表示Infura上的项目id return new HDWalletProvider(mnemonic, &quot;http://ip:port&quot;, 0); // 1表示第二个账户(从0开始) &#125; &#125;, &#125;, // Set default mocha options here, use special reporters etc. mocha: &#123; // timeout: 100000 &#125;, // Configure your compilers compilers: &#123; solc: &#123; // version: &quot;0.5.0&quot;, // Fetch exact version from solc-bin (default: truffle&#x27;s version) // docker: true, // Use &quot;0.5.1&quot; you&#x27;ve installed locally with docker (default: false) // settings: &#123; // See the solidity docs for advice about optimization and evmVersion // optimizer: &#123; // enabled: false, // runs: 200 // &#125;, // evmVersion: &quot;byzantium&quot; // &#125; &#125; &#125; &#125; 上面的 new HDWalletProvider 让我非常迷，我在metamask导入了一个有余额的账户，按理说是1，但是指定1部署的时候用的不是这个账户，所以我就给0账户转了点钱，依旧指定0账户 最后在终端执行部署命令 truffle migrate 注意，不要忘记把挖矿开起来，不然合约不会部署上去","categories":[{"name":"区块链","slug":"区块链","permalink":"https://zunpan.github.io/categories/%E5%8C%BA%E5%9D%97%E9%93%BE/"}],"tags":[{"name":"MetaMask","slug":"MetaMask","permalink":"https://zunpan.github.io/tags/MetaMask/"},{"name":"truffle","slug":"truffle","permalink":"https://zunpan.github.io/tags/truffle/"}]},{"title":"MetaMask连接私有链发生的转账问题","slug":"MetaMask连接私有链发生的转账问题","date":"2020-01-18T05:26:35.000Z","updated":"2023-09-24T04:27:40.279Z","comments":true,"path":"2020/01/18/MetaMask连接私有链发生的转账问题/","link":"","permalink":"https://zunpan.github.io/2020/01/18/MetaMask%E8%BF%9E%E6%8E%A5%E7%A7%81%E6%9C%89%E9%93%BE%E5%8F%91%E7%94%9F%E7%9A%84%E8%BD%AC%E8%B4%A6%E9%97%AE%E9%A2%98/","excerpt":"","text":"我用ganache-cli启了一个以太坊网络，然后我在MetaMask连接到了这个网络，并且导入了一个账户，正常显示余额是100ETH，但是转账的时候发生了错误。 EthQuery - RPC Error - Error: [ethjs-rpc] rpc error with payload &#123;&quot;id&quot;:3715053778334,&quot;jsonrpc&quot;:&quot;2.0&quot;,&quot;params&quot;:[&quot;0xf8720485012410110082520894c8fb523ca95721bf3408f6e6ef0ed8e7c3f65488880de0b6b3a7640000808602df64fab56ea052b79e6fe25a6bd073f48d853c053d35b0d4de1e3f1e77e7e82c520bd24d9783a059bf550c3fa7fec8a93584f77b0998545b936e7e583bb53076fb2eac9664725d&quot;],&quot;method&quot;:&quot;eth_sendRawTransaction&quot;&#125; [object Object] 错误原因是ganache的chainId和MetaMask的chainId不同 解决方法： ganache-cli -i 1 -h 0.0.0.0 -p 7545 -i 指定启动的链id，我这里指定1 -h 指定监听所有ip，因为我把ganache装在了云服务器上面，且没有注册域名，没法用nginx做代理，所以只能监听所有ip，这样我才能在本机访问到 -p 指定端口，默认是8545，但是truffle部署合约时是用的7545，所以懒得改truffle的代码就可以指定ganache在7545启动 随后在metamask中添加一个自定义网络并填上相应信息，metamask建议去github上下压缩包自己添加到扩展程序中，我在chrome商店下了N回都是出错，metamask添加网络步骤如下 网络名称随意，url如果本机就填 http://localhost:7545，如果是服务器填上对应ip，关键是ChainID一定要和ganache-cli 启动时一致，最后保存即可 注意：如果你的MetaMask已经连到了你本来没有指定chainId的网络了，那你再指定chainId启动网络，MetaMask转账可能仍然报错，这时候可以把MetaMask删掉重新加入扩展程序 转账成功的样子","categories":[{"name":"区块链","slug":"区块链","permalink":"https://zunpan.github.io/categories/%E5%8C%BA%E5%9D%97%E9%93%BE/"}],"tags":[{"name":"MetaMask","slug":"MetaMask","permalink":"https://zunpan.github.io/tags/MetaMask/"},{"name":"转账","slug":"转账","permalink":"https://zunpan.github.io/tags/%E8%BD%AC%E8%B4%A6/"},{"name":"ganache","slug":"ganache","permalink":"https://zunpan.github.io/tags/ganache/"}]},{"title":"以太坊","slug":"以太坊","date":"2020-01-07T22:53:31.000Z","updated":"2023-09-24T04:27:40.281Z","comments":true,"path":"2020/01/08/以太坊/","link":"","permalink":"https://zunpan.github.io/2020/01/08/%E4%BB%A5%E5%A4%AA%E5%9D%8A/","excerpt":"","text":"以太坊概述 比特币和以太坊是两种最重要的加密货币，比特币被称为区块链1.0，以太坊被成为区块链2.0 以太坊在系统设计上，针对比特币运行过程中出现的一些问题进行了改进，比如说出块时间调整至15s，这是基于ghost协议的共识机制；另外一个改进是挖矿使用的mining puzzle，比特币的mining puzzle是计算密集型的，比拼的是计算hash的算力，这样会造成挖矿设备的专业化，以太坊设计的是memory hard mining puzzle ，目的是在一定程度上限制ASIC芯片的使用（ASIC resistance），将来以太坊还有一些革命性的改变，用权益证明（proof of stake）代替工作量证明（proof of work）。除此之外，以太坊还加入了重要功能——对智能合约的支持（smart contract）。 什么是智能合约？ 我们知道比特币是去中心化的货币（decentralized currency），而以太坊是去中心化的合约（decentralized contract）。货币本来由政府发行，货币的价值是建立在政府公信力的基础上，政府通过司法手段来维护货币系统的正常运行。比特币的出现用技术手段把政府的职能取代了。去中心化的合约也是类似的意思，现实社会中，合约的有效性也是应该通过政府维护的，以太坊也用技术手段取代了司法手段，如果合同中的内容可以通过代码实现，代码就可以放到区块链上，通过区块链的不可篡改性来保证代码的正确运行，当然，不是所有的合同都能用编程语言实现，也不是说所有的合同条款都可以量化，但是逻辑比较简单清晰的是可以写成智能合约的。 去中心化的合约的好处？ 去中心化货币的一个应用场景是跨国转账，比如人民币→小币种，手续麻烦且手续费贵，如果用比特币就很方便。智能合约也有类似的应用场景，假如合同方来自世界各地，没有一个统一的司法管辖权，比较难用司法手段维护合约的有效性，这时候可以将合约内容以代码形式写进区块链里保证合约强制执行 以太坊账户 比特币是基于交易的账本，这种模式下并没有显式记录账户中有多少钱。 以太坊采用的是基于账户的模式，这种模式和银行账户比较类似，系统中要显示记录账户中有多少以太币，转账的时候只要余额够就可以转账，不用说明币的来源也不用找零，这样可以防御double spending attack。以太坊虽然不用说明币的来源，但是也不能篡改账户余额，因为账户余额在所有全节点的状态数中维护，必须所有全节点认为你的账户余额发生改变，你的账户余额才发生改变。虽然能防御double spending attack，但也可能会受到replay attack（重放攻击），转账的时候收款人是有恶意的，他可能会将这次交易再次发布，导致付款人转了两次。double spending attack 和 replay attack 是相对的，double spending 是花钱的人不诚实，replay attack 是收钱的人不诚实。以太坊解决replay attack的手段是加一个nonce（交易数），交易数代表这个账户建立以来的交易次数，由全节点维护，这个交易数会在交易过程成写进交易信息一起签名保护，如果交易的交易数比全节点中维护的付款人的交易数刚好大1，说明是正常交易，如果交易被重放的话，两者会相等，属于不合法交易。 以太坊中有两类账户，一类是外部账户（externally owned account），外部账户类似于比特币的账户，就是一对公私钥，掌握了私钥就掌握了这个账户。外部账户有两个状态，一个是balance，一个是nonce。第二类账户是合约账户（smart contract account），合约账户不是通过公私钥对控制，除了balance和nonce，还有code，storage。 以太坊状态树 以太坊采用的是基于账户的模式，系统中显式维护每个账户的余额 用什么数据结构来实现这种模式？ 我们要完成的是账户地址到账户状态的映射，addr→state，以太坊用的账户地址是160bits，一般表示成40个十六进制数，状态是外部账户的状态和合约账户的状态，包括余额，交易数，对于合约账户，还包括代码和存储。 第一种方案，用hash表+merkle tree。从直观上看，映射就是一个key-value对，很自然的想法是用一个hash表来实现。系统中的全节点维护一个hash表，每次有一个新的账户插入到hash表里，查询账户的余额就查hash表，如果不考虑hash碰撞，查询速度可以在常数级别，更新也很方便。这种方案的问题是，以太坊要防止所有账户的state被篡改，就要像比特币一样构建一个merkle tree，里面的交易变成账户状态，如果每次发布一个合法交易，某个账户状态发生改变，那么每个全节点都要重新构造merkle tree，代价太大。 第二种方案，不用hash表，直接用merkle tree，修改的时候直接改merkle tree。这个方法的问题在于merkle tree没有提供高效的查找和更新的方法，另外不同节点构造的merkle tree 叶子节点顺序不同也会导致merkle root 不同 ，所以得用sorted merkle tree ，但是sorted merkle tree 也有问题，如果新创建的用户地址在hash值在中间，那么插入merkle tree之后，merkle tree几乎重构。 有一种数据结构叫trie（字典树或前缀树），从retrieval（检索）中来，下面是一个一些单词组织成一个trie的例子 上图中，单词根据每一位的不同进行分裂，比如第二列有e和o就有两个分叉，第三列有在第二列是e的基础上只有n，所以只有一个分叉，单词有可能在非叶子节点结束。这个结构有一些特点。 第一个特点，在trie中，每个节点的分支数目取决于key中每个元素的取值范围，这个例子中，每个都是小写英文单词，所以每个节点的分叉数目最多27（26个小写字母+1个结束标志位）个，结束标志位表示到这个地方，这个单词就结束了。在以太坊中，地址是40位十六进制数，分叉数目有时候也叫branching factor 是17。 第二个特点，trie的查找效率取决于key的长度，键值越长，查找需要访问内存的次数就越多，以太坊中，键值都是40位。 第三个特点，如果用hash表存储key-value对，有可能出现碰撞，trie不会出现碰撞，只要地址不同，最后一定是不同分支 第四个特点，mekle tree不排序的话插入的账户位置不一样导致树的结构不一样，trie不论插入顺序如何，插入内容一致，最后的树就是一样的，这个对于以太坊非常有用 第五个特点，每次发布交易的时候，系统中大部分账户不变，只有少部分账户的状态需要更新，trie的局部更新性很好，只需要访问对应分支（注意，上图只画出了key）就可以找到value进行修改。 但是trie有一个缺点，trie的存储比较浪费，像上图有些节点只有一个子节点。如果能把这些节点合并，就可以减少存储的开销，也提高了查找的效率。 还有一种数据结构叫patricia tree或patricia trie（压缩前缀树），用上图例子进行路径压缩的结果如下 直观上看，树的高度减少了，存储更密集了。但是，如果新插入一个单词，原来压缩的路径可能需要扩展开来，假设上图加入geometry，就不能压缩成EN节点了。路径压缩有时候效果明显，有时候不明显。树中插入的键值分布如果比较稀疏情况下，路径压缩效果明显。比如假如上图的每个英文单词都很长，但是一共没有几个单词（misunderstanding、decentralized、disintermediation(去中心商，意思是让系统中的价值提供者和消费者直接交互)），这个时候插入到trie中，就会变成下图 如果用了压缩树，就会变成 因此键值分布比较稀疏的时候，路径压缩效果较好。而在以太坊中，键值是地址，160位，总的地址空间有2160位，非常大。以太坊的账户数和2160相比微乎其微，所以键值分布非常稀疏 第三种方案 先提一下MPT（merkle patricia tree）。MPT和PT的区别就是连接节点之间的指针用的是hash指针，最后会保留一个关于状态树的merkle root，它的作用之一也是防止账户状态被篡改，作用之二是merkle proof证明账户余额，将账户状态所在分支发给轻节点即可证明，作用之三是证明账户不存在，如果账户存在，把应当存在的账户的所在分支发给轻节点验证，验证失败则不存在。以太坊的状态树用的就是MMPT（modified MPT），下图是一个例子。 右上角有4个账户，为了简单起见，地址都非常短，就是上面的keys，账户状态只显示余额，就是上面的values。树中的节点分为三种，Extension Node，如果树的某一部位进行了压缩，就会出现一个Extension Node。因为4个地址的前两位开头都是a7，所以根节点就是Extension Node。下一层出现了分叉，所以出现了一个Branch Node。1的后面只有一个1355，所以它就是一个Leaf Node。7的后面有两个地址都是d3，所以压缩，再往下是3和9就分开来了，所以是一个Branch Node,再下面就是Leaf Node了。最右边的f后面就只有一个9365，所以是一个Leaf Node。 每次发布一个新的区块的时候，状态树中有一些节点的值会发生变化，这些改变不是原地修改，而是新建分支，原来的分支被保存。 上图是两个相邻的区块，State Root是状态树的根hash值，虽然每一个区块只有一个State Root，但是两棵树的大部分节点是共享的，右边的树主要指向左边这棵树的节点，只有发生改变的节点需要新建分支。上图例子中是合约账户（包括nonce，balance，codehash，storage root的那个节点）发生变化，每一个合约账户的存储都是一棵小的MPT，上图交易次数nonce发生变化，balance发生变化，代码不变，所以codehash指向原来的code，存储变了，但是存储树中的大部分节点也是没有改变，唯一的改变的29变成了45，所以新建了一个分支。所以系统中要维护的不只是一颗MPT，而是每次出现一个区块，都要新建一个MPT，只不过这些状态树中大部分节点是共享的，只有少数发生变化的节点要新建分支。为什么要保留历史状态？系统当中有时候会出现分叉，临时性分叉非常普遍，假设出现一个分叉，两个节点同时获得记账权，如果上面一个节点胜出，下面的节点可以roll back（回滚对账户状态的修改）然后顺着上面的节点所在分支继续挖，这个和比特币不太一样，比特币交易类型比较简单，有的时候可以反向操作推断出前一个状态，比如说转账交易，A给B转了10个比特币，回滚只需要给A加10个比特币，B减去10个比特币，但是以太坊中不行，因为以太坊中有智能合约，智能合约执行完成后再推算之间的状态是不可能的，所以要想支持回滚就要记录历史状态。 以太坊中代码的数据结构! 上图是block header结构 ParentHash：前一个区块块头的hash UncleHash：叔叔区块的hash，可能比Parent大好几辈 Coinbase：挖出区块的矿工地址 Root：状态树的根hash TxHash：交易树的根hash，类似于比特币中的merkle root ReceiptHash：收据树的根hash Bloom：布隆过滤器，和收据树相关，提供高效的查询符合某种条件的交易的执行结果 Difficulty：挖矿难度 GasLimit和GasUsed和汽油费相关，智能合约消耗汽油费，类似于比特币中的交易费 Time：区块产生时间 MixDigest和Nonce和挖矿过程相关 上图是区块结构，header是指向block header的指针，uncles是指向叔叔区块的指针，而且是数组，transactions是交易列表 上图是区块在网上发布的真实结构，其实就是区块结构的前三项。 我们知道状态树保存的是key-value pairs，key就是地址，value是账户状态，账户状态要经过序列化过程才能保存进状态树中，序列化用的是RLP（Recursive Length Prefix），特点是简单，只支持nested array of bytes ，意思是字节数组可以嵌套，以太坊中所有数据类型最后都要变成nested array of bytes， 以太坊的交易树和收据树 每次发布的区块中，交易会组织成一棵交易树，也是一棵merkle tree，和比特币中情况类似；每个交易执行完之后会形成一个收据，记录交易的相关信息，交易树和收据树上的节点是一一对应的，增加收据树是考虑到以太坊的智能合约执行过程比较复杂，通过增加收据树的结构有利于快速查询执行结果。从数据结构上看，交易树和收据树都是MPT，和比特币有所区别，比特币的交易树就是普通的merkle tree ，MPT也是一种merkle tree，但是和比特币中用的不是完全一样。对于状态树来说，查找账户状态所用的key是地址，对于交易树和收据树来说，查找的键值就是交易在区块中的序号，交易的排列顺序由发布区块的节点决定。这三棵树有一个重要的区别，就是交易树和收据树都是只把当前发布的区块中的交易组织起来，而状态树是把系统中所有账户的状态都组织起来，不管账户和当前区块中的交易有没有关系。从数据结构上来说，多个区块的状态树是共享节点的，每次新发布的区块时，只有区块中的的交易改变了账户状态的那些节点需要新建分支，其它节点都沿用原来状态树上的节点。相比之下不同区块的交易树和收据树都是独立的。 交易树和收据树的作用 交易树一个用途是merkle proof ，像比特币中用来证明某个交易被打包到某个区块里。收据树也是类似的，证明某个交易的执行结果，也可以在收据树里提供一个merkle proof。除此之外，以太坊还支持更复杂的查询操作，比如查询过去十天当中和某个智能合约有关的交易，这个查询方法之一是，把过去十天产生的所有区块中交易都扫描一遍，看看哪些是和这个智能合约相关的，这种方法复杂度比较高，且对轻节点不友好。以太坊中的查询是引入了bloom filter（布隆过滤器），这个数据结构支持比较高效的查找某个元素是不是在一个比较大的集合里，bloom filter给一个大的集合计算出一个很紧凑的摘要，比如说一个128位的向量，向量初始都是0，通过hash函数，把集合中的每个元素映射到向量中的某个位置，元素的映射位置都置为1，所有元素处理完后向量就是一个摘要，这个摘要比原来的集合小很多。这个过滤器的作用是，我们想查询一个元素，但集合太大我们不能保存，这时候对该元素取hash值，发现映射到向量中0的位置，说明这个元素不在集合里，但是映射到向量中1的位置，也不能说明元素在集合里，因为可能会出现hash碰撞。所以用bloom filter时，可能会出现false positive，但是不会出现false negative，意思是有可能出现误报，但是不会出现漏报，在里面一定说在里面，不在里面可能也会说在里面。bloom filter有各种各样的变种，比如说像解决hash碰撞，有的bloom filter用的不是一个hash函数，而是一组，每个hash函数独立的把元素映射到向量中的某个位置，用一组hash函数的好处是，一般不可能一组hash函数都出现碰撞。bloom filter的一个局限性不支持删除操作，因为存在hash碰撞，使得不同元素映射到向量同一个位置，如果删掉一个元素，使对应位置上的1变成0，那么和它发生碰撞的元素也被删除了，所以简单的bloom filter 不支持删除操作，可以将0和1改成计数器，记录有多少元素映射过来，而且还要考虑计数器是否会overflow，但是这样就复杂的多，和当初设计的理念就违背了，所以一般用bloom filter就不支持删除操作。以太坊中bloom filter 的作用是，每个交易执行完成后会形成一个收据，收据里面就包含了一个bloom filter ，记录这个交易的类型、地址等其它信息，发布的区块在块头里也有一个总的bloom filter ，这个总的bloom filter 是区块里所有交易的bloom filter 的并集，所以说想查询过去十天当中和某个智能合约有关的交易，先查哪个区块的块头的bloom filter里有我要的交易的类型，如果块头的bloom filter里面没有，那么这个区块里面就没有我们想要的，如果块头的bloom filter 里有，我们再去查找区块里面包含的交易所对应的收据树里面对应的bloom filter，但是可能会出现误报，如果有的话，我们再找到相对应的交易进行确认，好处是通过bloom filter能快速过滤大量无关区块，很多区块看块头的bloom filter就知道没有我们想要的交易，剩下的少数候选区块再仔细查看。轻节点只有块头信息，根据块头就能过滤掉很多信息，剩下有可能是想要的区块，问全节点要具体信息。 以太坊的运行过程可以看作交易驱动的状态机（transaction-driven state machine），状态机的状态指状态树中的那些账户状态，交易指交易树中那些交易，通过执行这些交易，使得系统从当前状态转移到下一个状态。比特币也可以认为是交易驱动的状态机，比特币中的状态是UTXO。这两个状态机有一个共同特点是状态转移都是确定性的，对一组给定的交易能够确定性的驱动系统转移到下一个状态，因为所有的节点都要执行同样的交易，所以状态转移必须是确定性的。 以太坊挖矿算法 对于基于工作量证明的区块链系统来说，挖矿是保障安全的重要手段。为了抵制矿机，以太坊设计了一中memory hard mining puzzle，以太坊用了两个数据集，一个是16M的cache，一个是1G的dataset叫做DAG，DAG是从cache中生成，这样设计的目的是便于轻节点验证，轻节点只需要保存16M的cache即可，只有矿工才需要保存1G的大数据集。基本思想是先用一个种子节点经过一些运算得到数组的第一个元素，然后对元素依次取hash得到后面的元素，这样得到的是一个填充了伪随机数的数组，就是一个cache，然后大数据集里面的每一个元素根据cache里的元素，依次读取256次取hash生成，求解puzzle的时候用的是大数据集，按照伪随机的顺序从大数据集中读取128个数，一开始，根据区块的块头算出一个初始的hash，根据hash映射到大数据集中的某个位置，把该数读取出来，然后进行运算得到下一个数得位置，每次读取的时候除了计算出这个元素的位置之外，还要把相邻的元素读取出来，进行64次循环，每次取出2个数，得到128个数，最后算出一个hash值，和挖矿难度的目标阈值比较一下，如果不合适就将block header里面的nonce替换一下重复上面过程 权益证明 比特币和以太坊目前用的都是基于工作量的证明，这种共识机制受到普遍的批评就是浪费电。 矿工挖矿是出于出块奖励，算力越大，出块奖励平均下来就越大，算力取决于设备的多少，也就是资金的投入，资金投入越多，奖励也越丰厚。那么我们可不可以不挖矿，直接比拼资金，奖励按资金比分配？这就是权益证明的思想，权益证明有时候也叫virtual mining。 采用权益证明的交易货币，一般会在正式发行之前预留一些货币给开发者，也会出售一部分货币来换取开发加密货币所需要的资金，将来按照权益证明的共识机制，每个人按照持有货币的数量进行投票，这种方法和工作量证明相比有一些优点，一个是不需要挖矿了，减少能耗；二是挖矿的算力从现实世界来，攻击者只要足够富裕，买大量矿机就可以发动攻击，对于小币种是致命打击，权益证明是按持有的货币数量进行投票，类似股票分红，如果某人想发动攻击，他需要先获得货币总量的51%才能发动攻击，也就是说发动攻击的资源必须从加密货币的系统中来，这样就系统形成了一个闭环，无论攻击者在系统外有多少资源，都不会对系统造成直接的影响，如果一定要发动攻击就要买大量的币，造成币的大涨，而开发者和早期矿工就可以从中获利。权益证明和工作量证明不是互斥的，有些加密货币采用的是混合模型，仍然要挖矿，但是挖矿难度和持有多少币是相关的，币越多难度越低，但是这样简单设计有一个问题就是富人挖矿越来越简单。有些两者混用的加密货币系统会将用于降低挖矿难度的币锁定一段时间，下次再挖一个区块的时候，不能用锁定的币降低难度，过几个区块才能使用，这种叫proof of deposit。 权益证明有许多问题，早期的权益证明有一个问题是两边下注 上图出现分叉，如果挖矿的话，我们会沿着上面这条链继续挖，但是下面的链也有可能成为最长合法链，只要下面这个分支连续挖出好几个区块。但是矿工不会两边都挖，因为算力会分散。如果不挖矿，用权益证明的话，两边都可以下注，如果上面那条链成为最长合法链，下面分支锁定的币对上面分支没有影响的，所以这种情况叫nothing at stake。 以太坊准备采用的权益证明协议叫做Casper the Friendly Finality Gadget（FFG），它在过渡阶段也是要和工作量证明混合使用，为工作量证明提供finality，finality 是最终状态，包含在finality中的交易不会被取消，单纯基于工作量证明是有可能被回滚的，Casper协议引入validator，要想成为validator，必须投入一定数量的以太币作为保证金，这个保证金会被锁定，validator推动系统达成共识，投票决定哪条链是最长合法链，投票权重取决于保证金的大小。挖矿的时候（混用状态下）每挖出一百个区块，就作为一个epoch，然后要决定它能不能成为finality要进行投票，投票进行两轮，类似于数据库的two-phrase commit，一个是prepare message，一个是commit message，Casper规定每一轮投票都要获得2/3以上的投票才能通过。实际当中不区分投票阶段，epoch也减少至50个，每个epoch只用一轮投票就行，这轮投票对于上一个epoch来说是commit message，对于下一个epoch来说是prepare message，要连续两个epoch都得到2/3的投票才算有效。 上图是早期的Casper协议，100个区块构成一个epoch，每个epoch要投两轮，都要获得2/3的票 上图是实际的Casper，这轮投票对于上一个epoch来说是commit message，对于下一个epoch来说是prepare message，要连续两个epoch都得到2/3的投票才算通过。 验证者验证的好处是如果验证者履行职责，那么可以获得相应的奖励，就像矿工挖矿能获得出块奖励一样，验证者验证也可以得到奖励，相反，如果验证者有不良行为，要受到相应处罚，比如验证者不作为，导致系统迟迟达不成共识，这样要扣掉验证者的部分保证金，如果验证者乱作为，给两个有冲突的分叉都投票，这种情况要没收全部的保证金。没收的保证金会销毁，相当于减少了以太币的总量。每个验证者有一定的任期，任期满了之后要经过一定时间的等待期，等待期是为了让其它节点可以检举验证者的不良行为，等待期过了没有受到惩罚那么验证者可以取回保证金以及一定的奖励，这就是casper协议的过程。 这里有一个问题，包含在finality的交易是不是一定不会被回滚，假设有某个恶意节点发动攻击，如果他只是一个矿工，那么他是不能推翻已经达成的finality，因为finality是验证者投票投出来的。如果有大量的验证者两边下注，给前后两个有冲突的finality都下注，casper协议规定每轮投票要2/3的支持才算通过，所以至少有1/3的验证者是两条分叉都投票了。","categories":[{"name":"区块链","slug":"区块链","permalink":"https://zunpan.github.io/categories/%E5%8C%BA%E5%9D%97%E9%93%BE/"}],"tags":[{"name":"区块链技术与应用","slug":"区块链技术与应用","permalink":"https://zunpan.github.io/tags/%E5%8C%BA%E5%9D%97%E9%93%BE%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8/"},{"name":"以太坊","slug":"以太坊","permalink":"https://zunpan.github.io/tags/%E4%BB%A5%E5%A4%AA%E5%9D%8A/"}]},{"title":"比特币的匿名性","slug":"比特币的匿名性","date":"2020-01-07T11:59:08.000Z","updated":"2023-09-24T04:27:40.283Z","comments":true,"path":"2020/01/07/比特币的匿名性/","link":"","permalink":"https://zunpan.github.io/2020/01/07/%E6%AF%94%E7%89%B9%E5%B8%81%E7%9A%84%E5%8C%BF%E5%90%8D%E6%80%A7/","excerpt":"","text":"比特币的匿名性更像是一种化名、网名。 和现金相比，匿名性不如现金，现金是完全匿名，上面没有任何人的信息，所以非法交易会用大量现金。 和银行相比，匿名性好于银行，因为银行开户是实名制，而比特币不需要。但如果银行开户允许使用化名，那么银行的匿名性要更好，因为比特币的账本是公开的，而银行的账本是受到控制的，银行工作人员可以查到所有人账号，但是普通老百姓是查不到别人的账号的。 破坏匿名性的情况 多个账户 一个人可能有很多账户，这些账户可能会被关联。 比如交易过程中，inputs里面有两个输入，一个地址输入4比特币，一个地址输入5比特币，outputs有两个输出，一个地址输出6比特币，一个地址输出3比特币。因为像买东西这种场景，大概率是会出现零钱的，3比特币就是零钱。因为交易的两个输入受你的控制，说明这是你的账户，两个地址就被关联了。输出的第二个地址也会被关联上，因为如果买东西只需要花3个比特币，那么输入只需要一个地址即可，所以分析出来3个比特币流向的地址也是你的账户。 与现实世界发生联系 比特币和实体货币发生联系的时候都有可能泄露身份，比如买比特币的时候，可以去交易所，此时就会留下你的交易记录。又比如用比特币支付。 如何提高匿名性 洋葱路由 首先保证网络层的匿名性，普遍的做法是多路径转发，TOR（洋葱路由）就是这个原理，消息在网络上传输要经过许多中间节点，每个节点只知道上一个节点是谁，而不知道谁发出的，只要路径上有一个节点是诚实的，他就会把发消息的人信息隐藏掉，后面的节点就不知道发消息是谁了。 混币 在应用层上，一种做法是coin mixing ，借助这类服务提供商，将你的币和别人的币混在一起，这时候你去取币，取的就不是原来的地址；还有一种做法是应用提供的天然mixing，比如在线钱包，大家都往里面投了币，然后取得时候就不一定是用原来的地址；还有一种手段是通过比特币交易所，你在交易所里面托管了比特币，然后经过一段时间的投资，比特币→美元→以太坊→莱特币→比特币，这时候你取到的比特币可能就不是原来的地址了 零知识证明 零知识证明是指一方（证明者）向另一方（验证者）证明一个陈述是正确的，而无需透露除该陈述是正确的外的任何信息。 一个有争议的例子：我想向别人证明这个账户是我的，也就是我持有它的私钥，但是我不能直接透露出私钥，我可以产生一个用私钥签的名，别人可以通过公钥验证签名，这个例子中，我是证明者，验证者是别人，陈述是我持有它的私钥，但是我没有透露私钥以外的信息。但这是有争议的，因为我没有透露私钥，但还是透露了用私钥产生的签名。 零知识证明的数学基础是同态隐藏 第一点说明不会发生碰撞，第二点说明隐藏性，针对第三点举例","categories":[{"name":"区块链","slug":"区块链","permalink":"https://zunpan.github.io/categories/%E5%8C%BA%E5%9D%97%E9%93%BE/"}],"tags":[{"name":"区块链技术与应用","slug":"区块链技术与应用","permalink":"https://zunpan.github.io/tags/%E5%8C%BA%E5%9D%97%E9%93%BE%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8/"},{"name":"比特币","slug":"比特币","permalink":"https://zunpan.github.io/tags/%E6%AF%94%E7%89%B9%E5%B8%81/"}]},{"title":"比特币的分叉","slug":"比特币的分叉","date":"2020-01-07T08:07:17.000Z","updated":"2023-09-24T04:27:40.283Z","comments":true,"path":"2020/01/07/比特币的分叉/","link":"","permalink":"https://zunpan.github.io/2020/01/07/%E6%AF%94%E7%89%B9%E5%B8%81%E7%9A%84%E5%88%86%E5%8F%89/","excerpt":"","text":"分叉是指一条链变成了两条链。 原因 分叉可能是多种原因造成的 一种原因是两个节点差不多同时挖到了区块，这个时候两个节点都可以发布区块，就出现了一个临时性的分叉，这种分叉叫state fork。分叉攻击也属于state fork ，也是属于对比特币区块的当前状态产生了分歧，只不过分叉攻击的分歧是故意造成的，所以分叉攻击造成的分叉也叫deliberate fork 。 除此之外，比特币协议发生了改变也会造成分叉，要修改协议需要软件升级，在一个去中心化的系统里，升级软件的时候没有办法保证所有节点同时都升级软件，我们假设大部分节点升级了软件，少部分节点因为种种原因没有升级，这种情况导致的分叉叫做protocd fork。根据对协议内容修改的不同，又可以进一步分成硬分叉（hard fork）和软分叉（soft fork）。 硬分叉 如果对比特币协议增加新的特性，那些没有升级软件的节点不认可新特性，这个时候就会产生硬分叉，一个例子是比特币的区块大小限制。假如有人发布了软件更新，将区块大小限制从1M→4M，假设大多数节点都更新了，少数节点没有更新，这里的大多数不是按账户数目来算，而是按算力来算，即系统中有大多数hash算力的节点都更新了软件。新节点能挖出最大是4M的区块，但是旧节点不认可大小超过1M的区块，所以旧节点会沿着小区块一直挖下去，新节点大小区块都接受，但旧节点不接受大区块，两者没有达成共识，这样就会出现一个永远的分叉。 软分叉 如果对比特币协议加一些限制，原来合法的交易或者区块在新的协议可能不合法就会引起软分叉。假如有人发布更新将区块大小变成0.5M，大部分节点更新了软件，少部分没有更新。这时候新节点沿着一条链挖小区块，不认可大区块，旧节点沿着一条链挖大区块，同时旧节点也认可小区块，因为新节点算力更强，所以更快形成一条新旧节点都认可的最长合法链，但是新节点不会在旧节点产出的大区块后面继续挖，而是会继续分叉挖小区块，所以旧节点挖出的大区块最后都没用了，不得不更新软件。 软硬分叉的区别就是 掌握大多数算力的新节点认可旧节点挖出的区块，但旧节点不认可新节点挖出的区块，那么就是硬分叉； 掌握大多数算力的新节点不认可旧节点挖出的区块，但是旧节点认可新节点挖出的区块，那么就是软分叉。 实际当中出现软分叉的情况之一是给某些目前协议中没有规定的域增加新的含义，赋予新的规则，如coinbase域，将前8字节作为extra nonce，但是剩下的字节都没有被使用。有人提出剩下的字节用来存UTXO的根hash值，那么新节点不认可旧节点挖出的区块，而旧节点认可新节点，因为coinbase写啥都无所谓，这样就会出现软分叉。 比特币历史上有名的软分叉例子是P2SH，这个功能最初没有，是后来通过软分叉加进来的。支付的时候不是付给一个public key hash（也就是输出脚本不是给出收款人公钥hash），而是付给一个redeem script hash（赎回脚本的hash），具体流程参考前文。对于旧节点来说，他不知道这个P2SH这个特性，他只会验证赎回脚本是否正确，新节点才会做第二阶段的验证，所以旧节点认为合法的交易，新节点可能认为非法，新节点认为合法的交易旧节点也认可","categories":[{"name":"区块链","slug":"区块链","permalink":"https://zunpan.github.io/categories/%E5%8C%BA%E5%9D%97%E9%93%BE/"}],"tags":[{"name":"区块链技术与应用","slug":"区块链技术与应用","permalink":"https://zunpan.github.io/tags/%E5%8C%BA%E5%9D%97%E9%93%BE%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8/"},{"name":"比特币","slug":"比特币","permalink":"https://zunpan.github.io/tags/%E6%AF%94%E7%89%B9%E5%B8%81/"}]},{"title":"比特币脚本","slug":"比特币的脚本","date":"2020-01-07T06:31:45.000Z","updated":"2023-09-24T04:27:40.284Z","comments":true,"path":"2020/01/07/比特币的脚本/","link":"","permalink":"https://zunpan.github.io/2020/01/07/%E6%AF%94%E7%89%B9%E5%B8%81%E7%9A%84%E8%84%9A%E6%9C%AC/","excerpt":"","text":"交易实例 这个交易的下面一个输出已经被花掉了，并且这个交易得到了23个confirmation，回滚的可能性很小。 输入脚本包含两个操作，分别把两个很长的数压入栈中，比特币使用的脚本语言非常简单，唯一能访问的内存空间只有堆栈，不像通用编程语言C++之类，有全局变量、局部变量、动态分配的内存空间等，它这里只有一个栈，所以叫基于栈的语言。输出脚本有两行，分别对应上面的两个输出，每个输出有自己单独的一段脚本。 交易具体内容 首先看一下交易的宏观信息 txid：transaction id，交易id hash：交易的hash version：比特币协议版本号 size：交易大小 locktime：用来设定交易的生效时间，0表示立即生效，非0值表示过一段时间生效，比如说等10个区块之后才能被写入区块链中 vin：输入部分 vout：输出部分 blockhash：这个交易所在区块的hash值 confirmations：确认数 time：交易产生时间 blocktime：区块产生时间 一个交易可能有多个输入，所以vin是个数组结构，上图只包含一个输入，每个输入都要说明输入花的币来自之前哪个交易的输出，所以前两行表示来源 txid：之前交易的hash vout：之前这个交易的第几个输出 scriptSig：输入脚本 一个交易可能也有多个输出，所以vout也是一个数组结构，上图只包含两个输出 value：输出金额，单位是比特币 n：这个交易的第几个输出 scriptPubKey：输出脚本 asm：输出脚本的内容 reqSigs：需要多少个签名 type：输出类型，上图都是公钥的hash address：输出地址 使用脚本验证交易合法性 验证这个交易的合法性就要将B→C的输入脚本和A→B的输出脚本拼接在一起执行。注意后一个交易的输入脚本在前，前一个交易的输出脚本在后，在早期的比特币系统中，这两个脚本是拼接在一起，从头到尾执行一遍，后来出于安全因素考虑，这两个脚本改为分别执行，首先执行输入脚本，如果没有出错，就执行输出脚本，如果能顺利执行，最后栈顶的结果为非0值，也就是true，这个交易就是合法的。如果执行过程中有任何错误，这个交易就是非法的。如果交易有多个输入，那么每个输入脚本都要和所对应的输出脚本进行配对验证，全都验证通过，这个交易才是合法的。 输入和输出脚本的最简单形式就是P2PK，输出脚本直接给出收款人的公钥（PUSHDATA），CHECKSIG是检查签名的操作，输入脚本直接给出用私钥对输入脚本所在交易的签名。 此处为了方便演示将输入脚本和输出脚本拼接在了一起，实际上是分开的。该脚本执行首先将输入脚本提供的签名压入栈中，然后将输出脚本提供的公钥压入栈，然后将栈顶两个元素弹出，用公钥检查签名是否正确，如果正确返回true 上图是P2PK实例，上面这个交易的输入脚本就是把签名压入栈，下面的交易是上面交易的币的来源，输出有两行，第一行将公钥压入栈，第二行是验证。 输入和输出脚本的第二种形式是P2PKH，这种形式与上一种形式的区别在于输出脚本没有直接给出收款人的公钥，给出的是公钥的hash，公钥在输入脚本给出，输入脚本既要给出签名，也要给出公钥，输出脚本其它操作是为了验证签名的正确性。这种形式是最常用的。 上面两行来自输入脚本，后面来自输出脚本，还是从上往下执行，前两条语句将签名和公钥压入栈，DUP 表示将栈顶元素复制一遍，HASH160表示将栈顶元素弹出，取hash之后再压入栈，此时栈顶变成公钥的hash值，此时栈的情况如下图 EQUALVERIFY是弹出两个栈顶元素，比较是否相等，相等就会消失 最后 CHECKSIG 和上一种形式一致，弹出两个元素，用公钥检查签名的正确性，正确最后栈中只会留下一个true P2PKH是最常用的形式 有一种最复杂的脚本形式P2SH，这种的形式的输出脚本给出的不是收款人的公钥hash，而是收款人提供的脚本的hash，叫redeemScriptHash（赎回脚本）。将来花这个钱的时候输入脚本要给出赎回脚本的具体内容，同时要给出让赎回脚本正常运行的签名 P2SH为什么这么复杂？P2SH在最初的比特币版本中并没有，后来通过软分叉的形式加进去的。常见的应用场景是对多重签名的支持。比特币系统中，一个输出可能要有多个签名才能把钱取出，比如某个公司的账户，可能要求5个合伙人中，任意3个合伙人的签名，才能把钱从公司账户中取走，这样为私钥的泄露提供了安全的保证，比如某些合伙人私钥泄露出去了，那么问题也不大，因为还需要另外两人的签名，才能取出钱来，同时也为私钥的丢失提供了冗余，5个合伙人中，即使有2人忘掉了私钥，剩下3人仍然可以把钱取出，然后转到某一个安全的账户。这个功能是通过CHECKMULTISIG实现的 输出脚本里给出N个公钥，同时给出一个域值M，输入脚本只要提供N个公钥中对应的签名中任意M个合法签名就可以通过验证。输入脚本的第一行红X表示压入一个多余的元素，CHECKMULTSIG有一个bug是会多弹出一个元素，因为去中心化的特性，现在不能通过软件升级改正。给出的M个签名的相对顺序要和N个公钥中的相对顺序一致才可以。 这个过程并没有用到P2SH，而是用的原生的CHECKMULTISIG。这样实现有许多不方便的地方，比如网上购物，某个电商平台用多重签名，要求5个合伙人中任意3个合伙人的签名才能把钱取出来，这就要求消费者在支付的时候要给出5个合伙人的公钥，同时还要给出N和M的值，这里N是5，M是3。消费者只能从平台获知这些信息，转账非常麻烦，这时候就可以采用P2SH实现多重签名 用P2SH实现多重签名可以将输出脚本的复杂度转移到赎回脚本里，输出脚本只要给出赎回脚本的hash，赎回脚本要给出N个公钥和N和M的值，赎回脚本在输入脚本提供，也就是说由收款人提供。 现在的多重签名一般都采用这种P2SH形式 最后一种脚本形式比较特殊，这种格式的输出脚本开头是RETURN操作，后面可以跟任意内容，RETURN操作的作用是无条件返回错误，所以包含这个操作的脚本永远不可能通过验证，执行到RETURN语句就出错终止。那么output的比特币岂不是永远花不出去了？确实花不出去，这种脚本的作用是证明销毁比特币，一种应用场景是有些小的币种要求销毁一定比特币才能换取这个币种，这种小币种叫AltCoin（Alternative Coin）。另外一个应用场景是往区块链里写一些内容，因为区块链是个不可篡改的账本，有人就利用这个特性，往里面添加一些永远需要保存的内容，比如前文提到的digital commitment，证明在某个时间知道某个事情，比如将知识产权的内容取hash写入RETURN后面，后面的内容反正永远不会执行，写什么都无所谓，而且放进去的是hash值，不会占太多地方也不会泄露内容，将来出现产权纠纷，可以将具体内容公布出去，证明你在某个时间已经知道某个知识了。 这个交易的输入是铸币交易，第一个输出是出块奖励和交易费，第二个输出的金额是0，输出脚本就是上面那种特殊的形式，开头是RETURN，后面是一些看起来乱七八糟的东西，这个输出的目的就是往区块链里面写一些东西 这是个普通的转账交易，输出脚本也是以RETURN开头。这个交易的输入是0.05个比特币，输出金额是0，说明输入金额全部用来支付交易费了，这个交易并没有销毁任何比特币，只不过把输入的比特币作为交易费转给挖到矿的矿工了，这种形式的好处是矿工看到这种脚本的时候知道它里面的输出永远不可能兑现，所以没有必要保存在UTXO里面，这样对全节点比较友好。","categories":[{"name":"区块链","slug":"区块链","permalink":"https://zunpan.github.io/categories/%E5%8C%BA%E5%9D%97%E9%93%BE/"}],"tags":[{"name":"区块链技术与应用","slug":"区块链技术与应用","permalink":"https://zunpan.github.io/tags/%E5%8C%BA%E5%9D%97%E9%93%BE%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8/"},{"name":"比特币","slug":"比特币","permalink":"https://zunpan.github.io/tags/%E6%AF%94%E7%89%B9%E5%B8%81/"}]},{"title":"比特币挖矿难度","slug":"比特币挖矿难度","date":"2020-01-06T10:36:06.000Z","updated":"2023-09-24T04:27:40.283Z","comments":true,"path":"2020/01/06/比特币挖矿难度/","link":"","permalink":"https://zunpan.github.io/2020/01/06/%E6%AF%94%E7%89%B9%E5%B8%81%E6%8C%96%E7%9F%BF%E9%9A%BE%E5%BA%A6/","excerpt":"","text":"挖矿就是不断尝试block header里面的nonce，使得整个block header 的hash值小于等于给定的目标阈值 H（blockheader）&lt;=targetH（block header）&lt;= targetH（blockheader）&lt;=target target越小，挖矿难度越大，调整挖矿难度就是调整目标空间在整个输出空间中所占的比例，比特币用的hash算法是SHA-256，产生的hash值是256位，输出空间是2256 ,调整目标空间在输出空间的比例通俗说就是hash值前面多少个0，合法的区块要求算出来的hash前面至少70个0，这是一种通俗的说法，不是特别准确，因为目标阈值并不是说前面都是0，从某一位开始后面都是1，严格来说，上面的公式才是对的。 挖矿难度和目标阈值是成反比的 difficulty_1_target表示挖矿难度等于1的时候所对应的目标阈值，挖矿难度最小就是1，这个时候对应的目标阈值是个非常大的数。 为什么要调整挖矿难度 系统总算力不断增强，挖矿难度保持不变，出块时间就会越来越短。出块时间越来越短会有什么问题，比如说不到1s出一个区块，这个区块在网络上传输可能需要几十s，别的节点在没有收到这个区块时会继续沿着已有的区块链往下继续扩展，如果有两个节点差不多同时都收到这个区块，差不多同时都发布一个区块，这个时候会出现一个分叉。出块时间越来越短，分叉就会越来越频繁，越来越多。分叉过多对于系统达成共识是没有好处的，而且危害了系统的安全性。 比特币协议假设大部分算力是掌握在诚实的矿工手里，系统的总算力越强，安全性就越好，如果恶意节点掌握了51%的算力，那么它就可以做任何事情（51% attack）。假设现在出现个分叉，系统中的总算力就被分散了，节点根据在网络中位置的不同，可能会选择不同的分叉继续扩展，而有恶意的节点可以集中算力就扩展它的分叉，也就是上图的A→A，这样可以很快使得这条分叉成为最长合法链，因为好人的算力被分散了，这个时候可能都不需要51%的算力，10%的算力可能就够了，所以出块时间不是越短越好。那比特币协议设计的10分钟是不是就是最优的呢，这不一定，比如说8分钟，5分钟行不行，应该也行，这个只是说出块时间要有一个合适的波动范围。有人觉得比特币的10分钟出块间隔太长了，对于一个支付系统来说，支付要等这么长时间才能得到确认，这个有点太长了。以太坊的出块时间就降低到了15s（同样需要加大难度保持出块时间稳定），出块时间大幅下降后，以太坊就要设计一个新的控制协议，叫ghost。在这个协议当中，这些分叉产生的叫orphan block，不能简单丢弃，而是要给它一些奖励，这叫做uncle reward。 如何调整挖矿难度 比特币协议中规定每隔2016个区块，要重新调整一下目标阈值，这个大概是每两个星期调整一下，调整按照以下公式： expected time 就是两个星期，actual time 是系统中最近产生2016个区块实际花费的时间。实际代码中target限制不会一次增大4倍以上，也不会一次减小到1/4以下。 那怎么让所有的矿工同时调整难度呢？ 计算target的代码是写在比特币系统的代码里，每挖到2016个会自动进行调整，但是比特币代码是开源的，如果有恶意的节点就是不改target怎么办？因为block header里面有target的编码域（nBits），而没有target域，因为target有256位，直接存要32个字节，nBits只要4个字节，可以认为nBits是target的压缩编码，如果有恶意的节点不改target，检查区块的合法性就通不过，因为每个节点都要独立验证发布的区块的合法性，检查的内容包括nBits域设置的对不对。以太坊也要定期调整挖矿难度，但它不是隔几个区块进行调整，而是每个新出的区块都有可能进行调整，而且调整的方法也比比特币的复杂得多。","categories":[{"name":"区块链","slug":"区块链","permalink":"https://zunpan.github.io/categories/%E5%8C%BA%E5%9D%97%E9%93%BE/"}],"tags":[{"name":"区块链技术与应用","slug":"区块链技术与应用","permalink":"https://zunpan.github.io/tags/%E5%8C%BA%E5%9D%97%E9%93%BE%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8/"},{"name":"比特币","slug":"比特币","permalink":"https://zunpan.github.io/tags/%E6%AF%94%E7%89%B9%E5%B8%81/"}]},{"title":"比特币网络","slug":"比特币网络","date":"2020-01-06T08:05:03.000Z","updated":"2023-09-24T04:27:40.285Z","comments":true,"path":"2020/01/06/比特币网络/","link":"","permalink":"https://zunpan.github.io/2020/01/06/%E6%AF%94%E7%89%B9%E5%B8%81%E7%BD%91%E7%BB%9C/","excerpt":"","text":"用户把交易发到比特币网络上，节点收到交易后把他们打包到区块里，然后把区块发到比特币网络上。那么新发布的交易和区块在网络上是如何传输的？ 比特币网络的工作原理 比特币工作在应用层（application layer），他的底层是一个P2P的Overlay network 比特币的P2P网络非常简单，所有节点都是对等的，不像有些P2P有所谓的超级节点（Super Node）或者主节点（Master Node）。你要加入这个网络，首先至少得知道一个种子节点（Seed Node），然后你和种子节点联系，它会告诉你它所知道的网络中的其它节点。节点之间是通过TCP通信的，这样有利于穿透防火墙，然后你离开的时候不需要通知其它节点，退出应用程序即可，别的节点没有听到你的消息，过一段时间就会把你删掉。 比特币网络的设计原则 简单、鲁棒而不是高效。每个节点维护一个邻居节点的集合，消息传播在网络中采取的是flooding方式，节点第一次听到某个消息的时候，把它传播给所有的邻居节点，同时记录一下这个消息我已经收到过，下次再收到这个消息就不再转发。邻居节点的选择是随机的，没有考虑底层的拓扑结构，比如加利福尼亚的节点选的邻居节点可能在阿根廷，这样设计的好处是增强鲁棒性，但是牺牲的是效率，你向身边的人转账和美国的人转账速度是差不多的。比特币系统中，每个节点要维护一个等待上链的交易的集合，第一次听到某个交易的时候，把这个交易加入集合，并且转发这个交易给邻居节点，以后再收到这个交易就不转发，这样避免交易在网络中无效传播，转发的前提是交易是合法的。这里有一个risk condition（风险状况），有可能两个有冲突的交易同时被广播到网络上，比如有一个交易是A→B，另外一个交易是A→C，它们用的是同一个输出，每个节点根据在网络中的位置的不同，有的可能先收到前者，有的可能先收到后者，收到后加入等待上链的交易的集合，下次收到另外一个交易的时候就认定是非法的，就不管了。集合中的交易如果被写到区块链中就要被删掉，比如说有个节点听到新发布的区块里面包含了A→B这个交易，这个交易在自己的等待上链的交易的集合中，就会被集合删掉，如果它听到的新发布的区块是A→C这个交易，也会把A→B给删掉，因为它是非法的。新发布的区块在网络中传播的方式和新发布的交易类似，每个节点除了要检查区块的内容的合法性，还要检查是不是在最长合法链上，越是大的区块在网络上传播的速度越慢，比特币协议对区块大小有一个1M的限制。 比特币网络的传播是属于best effort，一个交易被发布到比特币网络上，不一定所有节点都能收到，而且不同节点收到这个交易的顺序也不一定一样，网络传播存在延迟，可能会很长，而且有的节点可能不会按比特币协议的要求进行转发，比如有的该转发不转发，导致别的节点收不到合法交易；有的转发不该转发的，像不合法交易，这个是面临的实际问题。","categories":[{"name":"区块链","slug":"区块链","permalink":"https://zunpan.github.io/categories/%E5%8C%BA%E5%9D%97%E9%93%BE/"}],"tags":[{"name":"区块链技术与应用","slug":"区块链技术与应用","permalink":"https://zunpan.github.io/tags/%E5%8C%BA%E5%9D%97%E9%93%BE%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8/"},{"name":"比特币","slug":"比特币","permalink":"https://zunpan.github.io/tags/%E6%AF%94%E7%89%B9%E5%B8%81/"}]},{"title":"比特币实现","slug":"比特币实现","date":"2020-01-06T07:01:40.000Z","updated":"2023-09-24T04:27:40.282Z","comments":true,"path":"2020/01/06/比特币实现/","link":"","permalink":"https://zunpan.github.io/2020/01/06/%E6%AF%94%E7%89%B9%E5%B8%81%E5%AE%9E%E7%8E%B0/","excerpt":"","text":"UTXO 区块链是去中心化的账本，比特币采用的是基于交易的账本模式（transaction-based ledger）。每个区块里记录的是交易信息，有转账交易，铸币交易，但是系统当中并没有哪个地方显式的记录账户中有多少钱，这得需要通过交易记录推算。比特币系统的全节点要维护一个UTXO:Unspent Transaction Output 数据结构。区块链上有很多交易，有些交易的输出已经被花掉了，有些没有被花掉，所有交易中输出还没有花掉的组成一个集合就是UTXO。注意，一个交易可能有多个输出，所以会出现一个交易中有的输出在UTXO里面，有的不在。UTXO集合当中每个元素要给出产出这个输出的交易的hash值以及他在这个交易里是第几个输出，这两个信息就可以定位到UTXO中的输出。 UTXO的作用 为了检测double spending。没有花掉的币必须在UTXO中。全节点要在内存中维护UTXO，以便快速检测double spending 。每个交易会消耗掉一些输出，同时会产生新的输出。如果某人收到比特币的转账交易后，这个钱始终都不花，那么这个信息就要永久存储在UTXO中。 每个交易可以有多个输入和多个输出，所有输入的金额加起来要等于所有输出的金额，这个叫做total inputs =total outputs 。这意味着交易可能不止一个签名。有些交易的total inputs和total outputs有略微的差异，差额会作为交易费给获得记账权发布区块的节点。节点争夺记账权的动力之一是出块奖励，但是光有出块奖励是不够的，获得记账权的节点如果只打包自己的交易记录怎么办？所以，动力之二是就是交易费，就是前面提到的差额。比特币当中的交易费通常很小0.00几，也有些简单的交易是没有交易费的。目前矿工挖矿主要还是为了得到出块奖励。随着出块奖励的减少，以后的交易费会成为争夺记账权的主要动力。 除了比特币这种transaction-based ledger外，与之对应的还有基于账户的模式（account-based ledger），以太坊用的就是这种交易模式，在这种模式当中，系统要显示记录每个账户有多少币，这个和现实体验非常接近。比特币这种交易模式隐私保护性较好，但是也有代价，比如转账交易要说明币的来源，这就是没有账户的代价。以太坊系统就不存在这种问题。 比特币例子 Number Of Transactions : 表示这个区块包含了686个交易 Output Total：总的输出是4220.46616378个比特币 Transaction Fees：总的交易费 Height：区块序号 TimeStamp：产出的时间戳 Difficulty：挖矿的难度，每隔2016个区块会进行调整，保持出块时间在每10分钟产出一个 Nonce：挖矿的随机数 Block Reward：出块奖励 Hash：区块块头的hash Previous Block：前一个区块的块头hash Hash 和 Previous Block 前面都有一串0，这不是偶然的，所谓的挖矿就是不断试nounce，使得整个block header的hash小于等于给定的目标阈值，这个目标阈值表示成16进制就是前面有一长串的0，所以凡是符合难度要求的区块，它的块头的hash值都带一串0 Merkle Root：merkle tree的根hash值 注意，nounce的类型是32位的无符号整数，我们知道挖矿的时候要调整nonce，但是这个nonce最多只有232次方个取值，按照比特币现在的挖矿难度，就算把这232 个的取值都遍历一遍，很可能仍然找不到符合难度要求的。因为近年来挖矿的人不断增多，挖矿难度不断增大，单纯调正nonce是很可能找不到符合难度要求的，所以还要调整block header其他的域，下图是block header 里面的各个域的描述 version：当前使用的比特币协议的版本号，这个无法更改 previous block header hash：前一个区块的块头hash，也是无法修改 merkle root hash：merkle tree 的根hash值，可以修改 time：区块产生时间，有一定的调整余地，比特币系统并不要求非常精确的时间，可以对时间在一定范围内调整 nBits：挖矿时候用的目标阈值编码后的版本，4个字节，只能按照协议的要求定期调正，不能更改 nonce：我们能改的nonce 为什么merkle root hash可以改？ 每个发布的区块有一个铸币交易，这是比特币系统中产生新的比特币的唯一方式，这个交易没有输入（也就是上面的no input），因为币是凭空造出来的，它有一个CoinBase域，它可以写入任何东西，没有人管。 上图是一个小型区块链的示意图，左下角是coinbase transaction，我们改变这个交易的CoinBase域之后，这个交易的hash就发生了变化，这个变化会沿着merkle tree 不断向上传播，最终会导致block header 里的Merkle Root发生变化，所以我们可以把这个域当作extra nonce，块头的4字节nonce不够用，这里有很多字节可以用，比如把CoinBase域的前8个字节当作extra nonce来用，这样搜索空间一下子增大到了296次方。真正挖矿是有两层循环的，外层循环是调整CoinBase域的extra nonce，算出block header里的Merkle Root之后，内层循环再调整nonce。 上图是一个普通转账交易的记录，这个交易有两个输入，两个输出，左上方虽然写的是Output，其实对这个交易来说是输入，这里写的Output意思是说他们花掉的是之前哪个交易的Output。右边两个输出还没有被花掉，处于Unspent状态，会保存在UTXO里面 Inputs and Outputs中 Total Input：输入的总金额 Total Output：输出的总金额 Fees：前面两者的差值，这笔交易的交易费 Input Scripts和Output Scripts 输入和输出都是用脚本的形式指定，比特币系统中验证交易的合法性就是把Input Scripts 和Output Scripts配对后执行，注意上图中的Input Scripts和Output Scripts不是一对，上图的Input Scripts要和提供币的来源的交易的Output Scripts进行配对，如果输入脚本和输出脚本拼接在一起能顺利执行，那么这个交易就是合法的。 注意，上图中的所有的tx在计算时就是是一个Merkle Root 挖矿的过程的概率分析 挖矿的过程就是不断尝试各种nonce来求解puzzle，每次尝试nonce 都可以看作一个Bernoulli trial：a random experiment with binary outcome（伯努利试验：一个二元结果的随机试验）。伯努利实验的经典案例是抛硬币，如果硬币不均匀，每次实验，正面的概率就是p，反面的概率是1-p。对于挖矿来说，这两个概率相差甚远，每次尝试一个nonce，成功的概率是微乎其微的，大概率不行的，如果我们做大量的伯努利实验，每个实验都是随机的，那么这些伯努利实验就构成了一个Bernoulli process：a sequence of independent Bernoulli trial（伯努利过程：一系列独立伯努利试验）。Bernoulli process的一个特性是无记忆性（memoryless），意思是说做大量实验，前面的实验结果对后面的结果没有影响，挖矿过程就是不断尝试nonce，这种情况下Bernoulli process可以用Poisson process来近似，实验次数很多，每次成功的概率很小就可以用Poisson process来近似。我们关心的是出块时间，也就是系统里产生下一个区块的时间，这个在概率上可以推导出来，服从指数分布（exponential distribution） 注意，上图的出块时间是整个系统的出块时间，并不是矿工的出块时间，整个系统的平均出块时间是10分钟，平均时间是比特币协议规定的，通过定期调整挖矿难度，使得平均出块时间维持在10分钟左右，具体到每一个矿工，出块时间取决于矿工的算力，算力大，概率就大，出块时间就短。指数分布也是无记忆的，概率密度曲线的特点有：从任何一个地方把它截断，剩下曲线的形状仍然和原来一样，仍然服从指数分布，这就是无记忆的性质，假如过去了10分钟，仍然没有人找到区块，那么接下来还要等多久呢？还是平均下来10分钟，这个和直觉不太一致。这个概率分析告诉我们将来还要挖多少时间和过去挖了多少时间是没有关系的，仍然是服从指数分布，平均还要10分钟，这个性质也叫progress free。 progress free 或者 memoryless有什么作用？ 设想一下，如果有某个puzzle不满足这个性质会出现什么情况，比如说过去做的工作越多，接下来尝试nonce的时候成功的概率越大，相当于抛硬币的时候每次结果不是随机的，过去抛了好多次，都是反面朝上，下次再抛硬币的时候，正面朝上的概率会增加。如果有某一个加密货币设计出这样的puzzle，会有什么结果？算力强的矿工会有不成比例的优势，因为算力强的矿工过去的工作量更大。什么是不成比例的优势？比如系统中有两个矿工，一个的算力是另一个的10倍，理想状况下，算力强的矿工能挖到矿的概率应该是另一个矿工的10倍，这才算公平，因为算力强的矿工能尝试的nonce是另一个的10倍，这就是我们说的progress free 或者 memoryless所保证的，如果不是这样的话，算力强的矿工获得记账权的概率就会超过10倍，因为它过去尝试了更多的不成功nonce，那么下次成功的概率就会增大，这就是不成比例的优势。所以progress free 或者 memoryless保证了挖矿公平性。 比特币的总量 我们知道出块奖励是系统产生比特币的唯一途径，而这个奖励是每隔4年减半的，这样产生的比特币数量构成了一个几何序列（geometric series）。一开始的21W个区块能产生21W 50的比特币，接下来的区块能够产生21W 25，再往下就是21W12.5…… 总量就是21W 50 *（1+1/2+1/4+……）约等于2100W 不像寻找某个符合条件的质数这类数学难题，比特币求解puzzle除了比拼算力外并没有实际意义，比特币越来越难挖到是因为出块奖励被人为减少，比特币的稀缺性是人为造成的。虽然挖矿求解的puzzle本身没有实际意义，但是挖矿过程对于维护比特币系统的安全性是至关重要的，也叫做BitCoin is secured by mining。对于一个去中心化的没有membership控制的系统来说，挖矿提供了一种凭借算力投票的有效手段，只要大部分算力掌握在诚实的节点手里，系统的安全性就能得到保证，所以挖矿这一过程，表面上没有意义，但是这个机制对于维护系统安全性是非常有效的。 我们知道出块奖励每隔4年减半，是不是说挖矿的动力也会越来越小呢，从过去几年情况来看，恰恰相反，挖矿竞争越来越激烈，因为比特币的价格是飙升的。随着出块奖励越来越少，交易费也是越来越多的。 比特币的安全性 假设大部分算力是掌握在诚实的矿工手里，我们能得到什么样的安全保证？能不能保证写入区块的交易都是合法的？不能，挖矿只是概率上的保证，只能说有一个比较大的概率，下一个区块是诚实的矿工发布的，但是不能保证记账权不会落到有恶意的节点手里。比如，好的矿工占90%算力，坏的矿工占10%的算力，平均下来，10%的情况下，记账权会落到有恶意的节点手里。 恶意的节点掌握记账权 偷币 第一个问题，他能不能偷币?能不能把别人账上的钱转给自己？ 不能，因为他不能伪造别人的签名，交易不合法。但是如果他强行把不合法的交易写进区块链里会出现什么情况？诚实的节点都不接受这个区块，他们会沿着上一个合法的区块继续添加区块，形成最长合法链，导致恶意节点写进去的区块作废，这样，恶意节点不仅没有偷到钱，还把出块奖励陪了。 双花 第二个问题，他能不能把已经花过的币再花一遍？ 比如说M节点发布一个转账交易给A，现在他获得了记账权，又把钱转给自己，如果直接连在M→A的后面肯定是不合法的，因为很明显的double spending，凡是诚实的节点都不会接受这个区块，他要想发布交易就一定要插在M→A前面一个区块后面，也就是前面文章提到的分叉攻击。注意，区块插在什么位置，是要在刚开始挖矿就决定的，因为设置的block header要填上前一个区块的hash，所以M节点想插到这个位置，一开始就要把这个区块设置成前一个区块，而不是等获得了记账权再说。这种情况下会出现两个等长合法链，取决于其他节点沿着哪条链继续往下扩展，最后有一个会胜出，另一个就作废了。这种攻击有什么用？假如M是消费者，A是商家，M买了东西然后用比特币支付，M又发起一个交易，把钱转给自己，然后把下面的交易扩展成最长合法链，这样上面的区块就作废了，这种攻击的作用就是既买的了商品，又把钱收回来了，达到double spending attack的目的。 怎么防范这个问题？如果M→A这个交易不在最后一个区块，而是后面跟了几个区块，那么这种攻击的难度就会大大增加，要想回滚M→A这个交易还是得在他前面一个区块后面插入新区块，然后想办法让新区块所在分支成为最长合法链，这个难度非常大，因为诚实的节点不会沿着新区块往后发展，因为他不是最长合法链。这种情况相当于两条链在赛跑，如果大部分算力是掌握在诚实节点手里，这种攻击成功的可能性很小。所以一种防范的手段是多等几个区块或者叫确认（confirmation）。M→A这个交易刚刚写进区块的时候，称为one-confirmation，以此类图，它后面的第三个区块叫做three-confirmation。比特币协议规定，缺省情况下要等6个confirmation，到了six-confirmation时，才认为前面的交易是不可篡改的，也就是要等待10分钟（平均出块时间）*6 = 1小时。 我们知道区块链也被叫做不可篡改的分布式账本（irrevocable ledger），是不是说凡是写入区块链的内容就永远改不了呢？经过前面的分析我们可以知道，这种不可篡改性只是一种概率上的保证，刚刚写入区块链的内容，相对来说，还是比较容易被改掉的，经过一段等待时间之后，或者后面跟着好几个确认之后，这种被篡改的概率会指数级别下降。其实还有一种zero confirmation，这个意思是说，转账交易已经发布出去但是下一个区块还没有挖出来。拿电商购物的例子来说，相当于支付的时候发布一个转账交易，电商运行一个全节点或者委托某个全节点，监听区块链上的交易，收到转账交易后要验证交易的合法性，但是不用等到交易写到区块链里。这听起来风险很大，其实zero confirmation 在实际当中应用还是很普遍的，第一个原因是比特币协议缺省的设置是节点接受最先听到交易，两个交易有冲突，先听到哪个就接受哪个，所以zero confirmation处，M→A交易被节点收到，M→M交易较大概率不被诚实节点接受；第二个原因是很多购物网站从支付成功到发货，是有一定时间间隔的，天然有一定处理时间，比如说你要买个东西，在网上支付成功后，电商第二天才会发货，期间发现转账交易没有被写到最长合法链上，那么电商就可以取消发货。 故意不把某些合法交易写进区块链 第三个问题是他能不能故意不把某些合法交易写进区块链里？ 比特币协议没有规定获得记账权的节点一定要把某些交易写进区块链里，但是出现这种情况问题也不大，因为总会有诚实的节点愿意发布这些交易。区块链正常情况下也会出现合法的交易没有被写进去的情况，可能就是这段时间交易数目太多了，比特币协议规定每个区块的大小是有限制的，最多不能超过1M，所以交易放不下了就得等到下一个区块。","categories":[{"name":"区块链","slug":"区块链","permalink":"https://zunpan.github.io/categories/%E5%8C%BA%E5%9D%97%E9%93%BE/"}],"tags":[{"name":"区块链技术与应用","slug":"区块链技术与应用","permalink":"https://zunpan.github.io/tags/%E5%8C%BA%E5%9D%97%E9%93%BE%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8/"},{"name":"比特币","slug":"比特币","permalink":"https://zunpan.github.io/tags/%E6%AF%94%E7%89%B9%E5%B8%81/"}]},{"title":"比特币协议","slug":"比特币协议","date":"2019-12-27T07:16:33.000Z","updated":"2023-09-24T04:27:40.282Z","comments":true,"path":"2019/12/27/比特币协议/","link":"","permalink":"https://zunpan.github.io/2019/12/27/%E6%AF%94%E7%89%B9%E5%B8%81%E5%8D%8F%E8%AE%AE/","excerpt":"","text":"不用区块链的数字货币的问题 假设央行不用区块链发行数字货币，央行可以对每个货币用私钥签名，然后交易的时候，交易双方可以用公钥去验证这个签名的正确性，正确即是央行发行的货币，可以交易，否则就是假货币。 此方案存在的问题是：货币的真伪性可以得到保证，但是这个数量无法控制。假如你现在有一个央行发行的货币，你虽然不能伪造但是可以复制，从而发生一钱多用的情况（花两次攻击，英文名：double spending attack） 改进方案：央行除了要对货币签名，还要给每个货币打上一个编号，然后维护一个数据库，这个编号的货币现在在谁手上。交易的时候，双方先用公钥验证真伪，然后准备收钱的一方访问一下央行数据库，这个货币现在是不是在你手上，在的话就可以继续交易，否则这个货币你是已经用过了，不能再用了。这样就解决了double spending attack问题。 改进方案存在的问题：这是中心化的方案，央行数据库压力很大。比特币系统解决的就是这种问题。 去中心化货币系统要解决的两个问题 数字货币的发行，谁有权发？什么时候发？发多少？ 怎么验证交易的有效性？怎么防止double spending attack问题？ 比特币的方案 问题1的解决方案 共识算法 问题2的解决方案 有一个用户通过共识算法获得了发行货币的权力，我们管他叫铸币权，他发行了10个比特币给了A，把这交易信息写入区块链中。然后A把比特币给了B和C，一人5个，这个交易需要A的签名，证明是A同意的，同时这个交易还要说明花掉的比特币是从哪来的，也就是下图的第二个交易要说明比特币从哪来的，这里是从铸币交易输出来的。比特币交易系统中都包含了输入和输出两部分，输入部分要说明币的来源，输出部分要给出收款人的公钥的hash，比如A给B转钱，要说明B的公钥的hash是什么。然后B再把钱转给C，2个和D，3个，同样要签名，以及说明币的来源。这时候C想转给E7个币，币的来源就有两个了。见下图 注意：上图有两种hash指针，一种就是前面一篇文章提到的连接区块的hash指针，把区块串起来构成一个链表，还有第二种指针，是为了说明币的来源，这样可以防范double spending attack：假如上图B还想给F转比特币，单纯验证签名，貌似合法，但是币从哪来呢？还是要从上图的第二个区块来，别的节点收到这个交易后，会进行查询，从这个交易一直回溯到币的来源，回溯过程中，发现B已经把币全给C和D了，所以交易不合法，这个交易不会写入区块链中。 A和B交易需要的信息 需要有A的签名和B的地址，比特币系统中收款方的地址是通过公钥推算出来的，比如B的地址是B的公钥取hash再进行一些转过生成的，这个地址相当于银行账号，A要给B转钱，需要知道B的银行账号。那么A怎么才能知道B的地址呢？比特币系统没有提供查询某个人的地址的方法，这得需要第三方的形式，比如说电商平台支持比特币支付，那么商家要把账户地址告诉电商平台，然后消费者就知道要给谁转账了。 另外需要知道A的地址，A要B的地址是为了知道给谁转账，B（其实是所有节点）要A的地址是为了①谁给B转的账②验证是否是A的签名（A的私钥签名，其他节点知道A的地址后就可以公钥验证）。 问题来了？怎么才能知道A的公钥呢？ A的公钥是交易自己给出的，交易有输入和输出，输入不仅要说明币的来源还要说明A的公钥。 那么问题又来了，交易自己决定输入公钥，那不是有冒名顶替的风险吗？ 前面提到输入要给出币的来源和付款人的hash，而输出要给出收款人的公钥的hash，那么上面A到B的交易的币是哪里来的呢？是前面铸币交易来的，来的同时要带上付款人的公钥的hash也就是前面铸币交易的收款人的公钥的hash。也就是说第二个交易的输入的公钥要和第一个交易的输出的公钥要一致。 在比特币系统当中，验证过程是通过脚本实现的，每个交易的输入是一段脚本，包括公钥，也是在脚本指定的，每个输出也是一段脚本，验证是否合法，就是把当前输入的脚本和币的来源的那个交易的输出的脚本拼在一起，如果能正常执行，那么就是合法的。 注意：上图对区块进行了简化，每个区块只包含了一个交易，实际上，一个区块可以有很多交易，所有交易构成了一个merkle tree。每个区块分为块头和块身两部分。 块头包含这个区块宏观的一些信息，比如说用的是比特币版本的哪个协议（version），还有区块链当中指向前一个区块的hash指针（hash of previous block header），还有整个merkle tree 的根hash值（Merkle root hash），还有两个域和挖矿相关的，一个是挖矿的难度目标阈值（target），一个是随机数nonce 比特币中的共识协议 一个节点一票 假设系统中大部分节点是诚实的，那么就投票，比如说某一个节点提出一个候选区块，根据收到的交易信息，判断哪些交易是合法的，然后把这些交易按顺序打包到候选区块里。候选区块发布给所有节点，每个节点收到区块后，检查一下，这里面的交易是不是合法的，如果都是合法的，投赞成票，否则投反对票。最后得票超过半数，候选区块写写入区块链中。 存在的问题 恶意节点不断提出有问题的区块，时间都浪费在了投票上面，区块链无法发展 没法强迫节点投票。如果都不投票，那么区块链就陷入了瘫痪。 效率上的问题。投票等多久决定于网络延迟 任何基于投票的方案都要先决定谁有投票权（membership），这是区块链最大的问题。如果区块链的成员有严格定义，比如说有一种区块链叫联盟链（hyperledger fabric），只有符合条件的大公司才能加入，这种方案就可行。但是加入比特币系统是很容易的。 根据算力投票 比特币系统是根据计算力投票的。每个节点都可以在本地组装出一个候选区块，把它认为是合法的交易放在这个区块里，然后尝试各种nonce值，即计算H（block header）&lt;=target，nonce是个4 bytes的数。如果某个节点找到了nonce，就获得了记账权，就是往比特币去中心化的账本中写入下一个区块的权力，只有获得了记账权的节点才有权力发布下一个区块。其他节点收到区块后验证区块的合法性，比如block header里面的各个域是否正确，它里面的nBits域（时间上是target的一个编码），检查一下nBits设置的是不是符合比特币系统规定的难度要求；然后检查一下nonce是不是使得整个H（block header）小于等于target。然后验证一下block body里面的交易是不是合法的，第一要有合法签名，第二没有用过。 假设一个区块经过检查是合法的，那么有没有可能节点不接受它呢？ 恶意节点不接受合法区块 假设一个获得记账权的节点发布一个合法的区块,如下图,插在了区块链的中间。这个区块是完全合法的,那么我们应不应该接受它? 首先说明这个合法区块为什么插在了中间？因为每一个区块都带了一个指向前面一个区块的hash指针（hash of prev block header）。那么插在这里有什么问题？见下图 假如上方这个区块有一个交易是A把钱给了B，而我们新插入的这个区块里有个交易是A把钱给了自己。这里要说明这种情况不会发生double spending。我们判断是不是发生了double spending 是判断该区块所在分支有没有发生，上方的区块和新插入的区块已经是不同分支了，所以不是double spending。这种情况相当于把A→B这个交易给回滚了。 那么我们到底要不要接受这个新区块呢?不接受，因为它不在最长合法链上（longest valid chain） 比特币协议中规定接受的区块应该是在扩展最长合法链。上图这个例子是分叉攻击的例子（forking attack），通过往区块链中插入一个区块，来回滚已经发生的交易。 但是比特币系统在正常情况下也可能出现分叉。比如有两个节点同时获得了记账权，发布区块，这个时候会出现下图这种情况 有两条最长合法链。那该接受哪一个？比特币协议中，缺省情况下，每个节点接受最早收到的那一个，由于网络延迟，有些节点可能接受1，有些节点可能接受2。什么叫接受？如果节点在1后面继续扩展，说明节点接受了1。所以如果出现系统中两个节点差不多同时发布区块的情况，这种等长的临时性的分叉会维持一段时间，直到一个分叉胜出。上图例子中，假设1区块抢先找到了下一个区块，那么上面的分叉就成了最长合法链，下面的分叉就成了orphan block，orphan block 铸币交易产生的比特币都会失效。为什么要争夺记账权？获得记账权的节点有一定权力，它可以决定哪些交易可以写到区块里，但是设计协议的时候不应该让它成为争夺记账权的主要动力，因为我们希望凡是合法的交易都应该被写到区块链里，而不是取得记账权的节点决定是否写入。比特币系统提供的出块奖励（block reward）成为争夺记账权的动力，比特币协议规定获得记账权的节点在发布的区块里可以有一个特殊的交易，就是前面提到的铸币交易（coinbase transaction），可以发布一定数量的比特币（初始50个，每21万个区块之后减半）。这样就解决了前文提到的去中心化货币系统要解决的第一个问题。铸币交易是比特币系统中发行比特币的唯一方法，其他所有交易都是比特币的转移。 所以比特币系统要取得的共识到底是什么？分布式hash表要取得的共识是hash表的内容，比特币系统中要取得的共识是去中心化账本里的内容。谁来决定写这个账本？取得记账权的节点写账本，所以比特币系统中的共识机制是根据算力（每秒钟能试多少个nonce，也成为hash rate）来投票的，hash rate越高，得到记账权和得到比特币的概率就越高。那么这种共识机制是如何避免女巫攻击的，因为不管创建多少账户，节点的hash rate 都不会改变，所以获得记账权和比特币的概率不会提高。","categories":[{"name":"区块链","slug":"区块链","permalink":"https://zunpan.github.io/categories/%E5%8C%BA%E5%9D%97%E9%93%BE/"}],"tags":[{"name":"区块链技术与应用","slug":"区块链技术与应用","permalink":"https://zunpan.github.io/tags/%E5%8C%BA%E5%9D%97%E9%93%BE%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8/"},{"name":"比特币","slug":"比特币","permalink":"https://zunpan.github.io/tags/%E6%AF%94%E7%89%B9%E5%B8%81/"}]},{"title":"比特币的数据结构","slug":"比特币的数据结构","date":"2019-12-26T11:43:07.000Z","updated":"2023-09-24T04:27:40.284Z","comments":true,"path":"2019/12/26/比特币的数据结构/","link":"","permalink":"https://zunpan.github.io/2019/12/26/%E6%AF%94%E7%89%B9%E5%B8%81%E7%9A%84%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/","excerpt":"","text":"比特币有两种数据结构，一个是区块链，一个是Merkle Tree 区块链 传统意义上的指针，保存了所指向结构体在内存中的起始位置，而hash指针除了存地址之外，还要保存这个结构体的hash值，一般用H表示这hash指针，好处是不光知道这个结构体的地址，还可以知道这个结构体有没有发生改变。 比特币中最基本的数据结构就是区块链。区块链是什么，就是一个一个区块组成的链表，区块链和普通的链表有什么区别？一个区别就是hash指针代替了普通的指针。 每个区块包含一个前一个区块的hash指针，这个指针是对前一个区块整个信息进行hash出来的，包括前一个区块的hash指针，通过这种数据结构可以实现tamper-evident log（防篡改）。因为，一旦改了某个区块的信息，那么它后面的区块的hash指针就要变化，后面的区块的hash指针一变，后面的后面的hash指针也要跟着变，所以这个数据结构的好处是，只要我们保存最后一个hash指针，那么区块链上任意一个区块发生改变，我们都可以知道 Merkle Tree 比特币的另外一个结构是Merkle Tree，和二叉树类似，不过父节点保存的是左右孩子节点的hash指针 比特币当中是用hash指针将各个区块连接在一起，每个区块的交易是组织成一个Merkle tree 的形式，上图底下的数据块其实就是一个个交易。每个区块分为两部分，块头和块身。block header包含root hash，但是没有交易的具体内容，block body 有交易内容。 Merkle Proff 比特币中的节点分为两类，一类是全节点，一类是轻节点，全节点保存整个区块的内容（即有block header和block body），轻节点只保存block header。轻节点不包含block body，但是交易信息是在block body里面的，那全节点如何向轻节点证明交易确实发生了呢？ 假设某个轻节点想要知道黄色这个交易是不是包含在了这个merkle tree里面了，轻节点没有这颗merkle tree，只有一个根hash值，轻节点向某个全节点发出请求，请求一个能够证明黄色交易被包含在这个merkle tree里面的merkle proof ，全节点收到请求之后，只要把途中标为红色的这三个hash值发给这个轻节点就行，有了这三个hash值，轻节点可以在本地计算出图中标为绿色的hash值，首先算出黄色交易的hash值，即图中最下面的绿色的hash值，然后和右边的红色hash值组合计算出上一个绿色的hash值，同理可以算出整个树的根hash值，轻节点把这个hash值和block header里面的比较，就可以知道这个交易是不是在这个merkle tree里面。 这个是用merkle proof证明某个交易在merkle tree里面，那么如何证明某个交易不在merkle tree里面呢？ 一种方法是把整个merkle tree发给别人，别人验证正确后，发现没有要找的交易，即交易不在merkle tree里面。 另外一种方法是对交易根据hash排个序，然后把要找的交易也hash一下，然后二分查找，如果找到了，merkle proof证明它确实在里面，不存在证明失败；如果没找到，对相邻的两个交易进行merkle proof，如果证明成功，说明它俩是紧邻的，也即我们要找的交易不在里面，因为如果在里面，它俩就不是紧邻的了，不存在证明成功。","categories":[{"name":"区块链","slug":"区块链","permalink":"https://zunpan.github.io/categories/%E5%8C%BA%E5%9D%97%E9%93%BE/"}],"tags":[{"name":"区块链技术与应用","slug":"区块链技术与应用","permalink":"https://zunpan.github.io/tags/%E5%8C%BA%E5%9D%97%E9%93%BE%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8/"},{"name":"比特币","slug":"比特币","permalink":"https://zunpan.github.io/tags/%E6%AF%94%E7%89%B9%E5%B8%81/"}]},{"title":"比特币的密码学原理","slug":"比特币的密码学原理","date":"2019-12-26T11:41:33.000Z","updated":"2023-09-24T04:27:40.284Z","comments":true,"path":"2019/12/26/比特币的密码学原理/","link":"","permalink":"https://zunpan.github.io/2019/12/26/%E6%AF%94%E7%89%B9%E5%B8%81%E7%9A%84%E5%AF%86%E7%A0%81%E5%AD%A6%E5%8E%9F%E7%90%86/","excerpt":"","text":"前言：这一系列的文章是看肖臻老师《区块链技术与应用》公开课做的笔记 比特币用到了密码学的两个功能，一个是哈希，一个是签名 哈希 函数 对于集合A中的任意一个数x，在集合B中都有唯一确定的数y与之对应 要注意： 集合A中的元素a，都能找到有且仅有一个集合B中的数与之对应；即不存在f（a）=b1，又有f（a）=b2的情况。但是，存在f（a1）=b，同时又有f（a2）=b的情况，如偶函数y=x2 哈希函数 本质是一个函数，但是特殊在输入值是任意字符串，输出值是固定长度的字符串，比如比特币用的SHA256,是将输入的字符串计算成64位十六进制数（256位二进制数） 比特币用到的哈希函数的三个特性 collision resistance（抗碰撞性） 什么是碰撞？ 对于函数f（x) ，如果存在a！=b，使得f（a）==f（b），那么就称为碰撞 什么是抗碰撞性？ 除了暴力枚举输入空间，很难找到会发生碰撞的两个输入值 抗碰撞性的作用 上传文件之前可以对文件计算一个hash值，下次下载下来，再计算一个hash值，如果两个hash值不同，则说明被文件篡改了。因为抗碰撞性使得修改后的文件的计算出来的hash值和原来的文件极大概率是不同的。 hiding property（隐藏性） 光给你一串hash值，除了暴力求解，你是很难知道加密前的内容是什么，这就叫隐藏性。但是hiding成立的前提是输入空间无限大，而且输入的分布比较均匀，各种取值的可能性是差不多的，所以暴力求解也是不可行的。 隐藏性的作用 假如有一个人说他预测股票很准，怎么验证他说的准不准呢？如果他公布预测结果，可能会有人故意和他反着来，操纵股市导致预测不准，所以预测结果不能提前公布。那么大家又是如何知道他预测准不准呢？这个时候可以先让这个预测的人将预测结果取个hash值公布出来，由于隐藏性，光是知道hash值很难知道加密前的内容，这样就不会影响股市。然后收盘之后，让这个预测的人把预测结果公布出来，由于抗碰撞性，预测的人是无法篡改预测结果的，要是改了的话就和当初公布的hash对不上，这样就起到了一个sealed envelope 的作用。但是这个例子有一个问题，就是股票的数量是非常有限的，不满足隐藏性成立的前提条件，所以实际操作会在预测结果后面加一串随机数，然后整个取hash puzzle friendly 光是看输入，很难猜出来hash值是什么样的，所以你想让hash值落在某个范围之内，那只能暴力枚举了。比如想得出hash值前面k位数都是0，怎么得出呢？没有办法，不能事先知道，只能一个一个去试，所以叫puzzle friendly ，挖矿的过程就是计算H（block header）&lt;=target。 block header是区块的信息，其中包括一个nonce（随机数），我们所要做的就是不断就试这个nonce，使得hash值落在一个target space中，所以这个挖矿的过程没有捷径，只能不断去试nonce，所以这个才可以作为工作量证明。 签名 日常生活中想要开一个银行账户，怎么开，带上证件去银行。 那比特币怎么开账户呢？很简单，只需要一对公钥和私钥，每个用户自己决定开不开户。在本地中创建一个公钥私钥对就完成了开户。 公钥和私钥来自非对称加密。 什么是对称加密？ 双方在进行信息交换的时候用同一个密钥对内容进行加密/解密，那么这个密钥就一定要在网络中传输，但是网络是有可能被窃听的，所以对称加密并不安全。 什么是非对称加密？ 双方都有一对公钥和私钥，我给你发信息时，我用你的公钥对内容进行加密，然后你用你的私钥对信息解密，同理你给我发信息时得用我的公钥对信息进行加密，然后我用我的私钥对你发来的信息解密。公钥不需要保密，但是私钥需要保密。解决了对称加密的密钥分发的安全问题。 在比特币系统中，非对称加密的真正用途其实是签名，而不是信息传输，何谓签名？比如我想发起一笔交易，别人怎么知道是我发起的而不是别人偷偷的替我发起的呢，这就需要我在发布交易的时候用我的私钥进行签名（先对交易信息进行hash），然后别人用我的公钥去验证这个签名的正确性，如果不正确，说明不是我本人发起的。 所以总的来说，非对称加密算法的作用是：公钥加密，私钥解密； 私钥签名，公钥验证。 公钥私钥对重复问题 既然公钥私钥对始终是在本地产生，那么产生重复怎么办？ 如大量生成公钥私钥对，然后去对比产生的公钥是不是和区块链上已有的某个公钥相同。如果相同，那么就可以用对应的私钥把账户上的钱转走。 这种方式理论上可行，但是实际上没法做，因为产生重复的公钥私钥对的概率非常小，可以忽略不计。到目前为止还没有能用这种方法攻击成功的先例。 a good source of randness 生成公钥私钥的过程是随机的，但要求选取一个好的随机源，否则前面的分析就不成立了（就有不足够小的可能生成重复的公钥私钥对）。 比特币中使用的签名算法不仅在生成公钥私钥对时有好的随机源，在之后每次签名的时候也要有好的随机源。如果签名时使用的随机源不好，就有可能泄露私钥。","categories":[{"name":"区块链","slug":"区块链","permalink":"https://zunpan.github.io/categories/%E5%8C%BA%E5%9D%97%E9%93%BE/"}],"tags":[{"name":"区块链技术与应用","slug":"区块链技术与应用","permalink":"https://zunpan.github.io/tags/%E5%8C%BA%E5%9D%97%E9%93%BE%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8/"},{"name":"比特币","slug":"比特币","permalink":"https://zunpan.github.io/tags/%E6%AF%94%E7%89%B9%E5%B8%81/"}]}],"categories":[{"name":"开发经验","slug":"开发经验","permalink":"https://zunpan.github.io/categories/%E5%BC%80%E5%8F%91%E7%BB%8F%E9%AA%8C/"},{"name":"ArkTS","slug":"ArkTS","permalink":"https://zunpan.github.io/categories/ArkTS/"},{"name":"Java","slug":"Java","permalink":"https://zunpan.github.io/categories/Java/"},{"name":"MySQL","slug":"MySQL","permalink":"https://zunpan.github.io/categories/MySQL/"},{"name":"杂项","slug":"杂项","permalink":"https://zunpan.github.io/categories/%E6%9D%82%E9%A1%B9/"},{"name":"基础知识","slug":"基础知识","permalink":"https://zunpan.github.io/categories/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"},{"name":"密码学","slug":"密码学","permalink":"https://zunpan.github.io/categories/%E5%AF%86%E7%A0%81%E5%AD%A6/"},{"name":"区块链","slug":"区块链","permalink":"https://zunpan.github.io/categories/%E5%8C%BA%E5%9D%97%E9%93%BE/"},{"name":"Linux","slug":"Linux","permalink":"https://zunpan.github.io/categories/Linux/"},{"name":"Maven","slug":"Maven","permalink":"https://zunpan.github.io/categories/Maven/"},{"name":"算法","slug":"算法","permalink":"https://zunpan.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"Git","slug":"Git","permalink":"https://zunpan.github.io/categories/Git/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://zunpan.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"重构","slug":"重构","permalink":"https://zunpan.github.io/tags/%E9%87%8D%E6%9E%84/"},{"name":"ArkTS","slug":"ArkTS","permalink":"https://zunpan.github.io/tags/ArkTS/"},{"name":"深入理解Java虚拟机","slug":"深入理解Java虚拟机","permalink":"https://zunpan.github.io/tags/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Java%E8%99%9A%E6%8B%9F%E6%9C%BA/"},{"name":"LCS","slug":"LCS","permalink":"https://zunpan.github.io/tags/LCS/"},{"name":"Levenshtein","slug":"Levenshtein","permalink":"https://zunpan.github.io/tags/Levenshtein/"},{"name":"编辑距离","slug":"编辑距离","permalink":"https://zunpan.github.io/tags/%E7%BC%96%E8%BE%91%E8%B7%9D%E7%A6%BB/"},{"name":"diff","slug":"diff","permalink":"https://zunpan.github.io/tags/diff/"},{"name":"merge","slug":"merge","permalink":"https://zunpan.github.io/tags/merge/"},{"name":"MySQL","slug":"MySQL","permalink":"https://zunpan.github.io/tags/MySQL/"},{"name":"数据库","slug":"数据库","permalink":"https://zunpan.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"Java并发编程实战","slug":"Java并发编程实战","permalink":"https://zunpan.github.io/tags/Java%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AE%9E%E6%88%98/"},{"name":"Java Concurrency In Practice","slug":"Java-Concurrency-In-Practice","permalink":"https://zunpan.github.io/tags/Java-Concurrency-In-Practice/"},{"name":"实习","slug":"实习","permalink":"https://zunpan.github.io/tags/%E5%AE%9E%E4%B9%A0/"},{"name":"Effective-Java","slug":"Effective-Java","permalink":"https://zunpan.github.io/tags/Effective-Java/"},{"name":"异常","slug":"异常","permalink":"https://zunpan.github.io/tags/%E5%BC%82%E5%B8%B8/"},{"name":"通用程序设计","slug":"通用程序设计","permalink":"https://zunpan.github.io/tags/%E9%80%9A%E7%94%A8%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/"},{"name":"方法","slug":"方法","permalink":"https://zunpan.github.io/tags/%E6%96%B9%E6%B3%95/"},{"name":"lambda表达式","slug":"lambda表达式","permalink":"https://zunpan.github.io/tags/lambda%E8%A1%A8%E8%BE%BE%E5%BC%8F/"},{"name":"Stream","slug":"Stream","permalink":"https://zunpan.github.io/tags/Stream/"},{"name":"枚举","slug":"枚举","permalink":"https://zunpan.github.io/tags/%E6%9E%9A%E4%B8%BE/"},{"name":"注解","slug":"注解","permalink":"https://zunpan.github.io/tags/%E6%B3%A8%E8%A7%A3/"},{"name":"泛型","slug":"泛型","permalink":"https://zunpan.github.io/tags/%E6%B3%9B%E5%9E%8B/"},{"name":"类和接口","slug":"类和接口","permalink":"https://zunpan.github.io/tags/%E7%B1%BB%E5%92%8C%E6%8E%A5%E5%8F%A3/"},{"name":"对象的通用方法","slug":"对象的通用方法","permalink":"https://zunpan.github.io/tags/%E5%AF%B9%E8%B1%A1%E7%9A%84%E9%80%9A%E7%94%A8%E6%96%B9%E6%B3%95/"},{"name":"创建和销毁对象","slug":"创建和销毁对象","permalink":"https://zunpan.github.io/tags/%E5%88%9B%E5%BB%BA%E5%92%8C%E9%94%80%E6%AF%81%E5%AF%B9%E8%B1%A1/"},{"name":"CORS","slug":"CORS","permalink":"https://zunpan.github.io/tags/CORS/"},{"name":"Java","slug":"Java","permalink":"https://zunpan.github.io/tags/Java/"},{"name":"密码学","slug":"密码学","permalink":"https://zunpan.github.io/tags/%E5%AF%86%E7%A0%81%E5%AD%A6/"},{"name":"Cryptography","slug":"Cryptography","permalink":"https://zunpan.github.io/tags/Cryptography/"},{"name":"Dan Boneh","slug":"Dan-Boneh","permalink":"https://zunpan.github.io/tags/Dan-Boneh/"},{"name":"面试","slug":"面试","permalink":"https://zunpan.github.io/tags/%E9%9D%A2%E8%AF%95/"},{"name":"区块链","slug":"区块链","permalink":"https://zunpan.github.io/tags/%E5%8C%BA%E5%9D%97%E9%93%BE/"},{"name":"电子选举","slug":"电子选举","permalink":"https://zunpan.github.io/tags/%E7%94%B5%E5%AD%90%E9%80%89%E4%B8%BE/"},{"name":"隐私计算","slug":"隐私计算","permalink":"https://zunpan.github.io/tags/%E9%9A%90%E7%A7%81%E8%AE%A1%E7%AE%97/"},{"name":"Linux","slug":"Linux","permalink":"https://zunpan.github.io/tags/Linux/"},{"name":"Shell","slug":"Shell","permalink":"https://zunpan.github.io/tags/Shell/"},{"name":"Maven","slug":"Maven","permalink":"https://zunpan.github.io/tags/Maven/"},{"name":"LeetCode","slug":"LeetCode","permalink":"https://zunpan.github.io/tags/LeetCode/"},{"name":"cpp","slug":"cpp","permalink":"https://zunpan.github.io/tags/cpp/"},{"name":"algorithm","slug":"algorithm","permalink":"https://zunpan.github.io/tags/algorithm/"},{"name":"Git","slug":"Git","permalink":"https://zunpan.github.io/tags/Git/"},{"name":"MetaMask","slug":"MetaMask","permalink":"https://zunpan.github.io/tags/MetaMask/"},{"name":"truffle","slug":"truffle","permalink":"https://zunpan.github.io/tags/truffle/"},{"name":"转账","slug":"转账","permalink":"https://zunpan.github.io/tags/%E8%BD%AC%E8%B4%A6/"},{"name":"ganache","slug":"ganache","permalink":"https://zunpan.github.io/tags/ganache/"},{"name":"区块链技术与应用","slug":"区块链技术与应用","permalink":"https://zunpan.github.io/tags/%E5%8C%BA%E5%9D%97%E9%93%BE%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8/"},{"name":"以太坊","slug":"以太坊","permalink":"https://zunpan.github.io/tags/%E4%BB%A5%E5%A4%AA%E5%9D%8A/"},{"name":"比特币","slug":"比特币","permalink":"https://zunpan.github.io/tags/%E6%AF%94%E7%89%B9%E5%B8%81/"}]}